<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>GGSTU</title>
  
  <subtitle>Good Good Study</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.ggstu.com/"/>
  <updated>2018-09-30T04:34:52.715Z</updated>
  <id>https://www.ggstu.com/</id>
  
  <author>
    <name>Wu Zhenyong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark SQL中的JSON数据源</title>
    <link href="https://www.ggstu.com/2018/09/30/Spark-SQL%E4%B8%AD%E7%9A%84JSON%E6%95%B0%E6%8D%AE%E6%BA%90/"/>
    <id>https://www.ggstu.com/2018/09/30/Spark-SQL中的JSON数据源/</id>
    <published>2018-09-29T23:59:08.000Z</published>
    <updated>2018-09-30T04:34:52.715Z</updated>
    
    <content type="html"><![CDATA[<p>Spark SQL可以自动推断JSON文件的元数据，并且加载其数据，创建一个DataFrame。<br>可以使用SparkSession.read.json()方法，针对一个元素类型为String的RDD，或者是一个JSON文件创建DataFrame。</p><p>这里使用的JSON文件，每行必须只能包含一个单独的、自包含的、有效的JSON对象。不能让一个JSON对象分散在多行。否则会报错。<br><a id="more"></a><br>例如：查询成绩在80分以上的学生的年龄信息和成绩信息<br>首先在本地路径下创建个JSON文件，scores.json，其内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;Tom&quot;, &quot;score&quot;:85&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Bob&quot;, &quot;score&quot;:75&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Alice&quot;, &quot;score&quot;:92&#125;</span><br></pre></td></tr></table></figure></p><p>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Encoders;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.RowFactory;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line">import org.apache.spark.sql.types.DataTypes;</span><br><span class="line">import org.apache.spark.sql.types.StructField;</span><br><span class="line">import org.apache.spark.sql.types.StructType;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class JSONDataSource &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkSession spark = new SparkSession</span><br><span class="line">.Builder()</span><br><span class="line">.appName(&quot;JSONDataSource&quot;)</span><br><span class="line">.master(&quot;local&quot;)</span><br><span class="line">.getOrCreate();</span><br><span class="line"></span><br><span class="line">// 使用JSON文件创建DataFrame</span><br><span class="line">Dataset&lt;Row&gt; scoresDF = spark.read().json(&quot;C://Users//asus//Desktop//scores.json&quot;);</span><br><span class="line"></span><br><span class="line">// 针对学生成绩信息的DataFrame注册为临时表，查询分数大于80分的学生的姓名和成绩</span><br><span class="line">scoresDF.createOrReplaceTempView(&quot;student_scores&quot;);</span><br><span class="line">Dataset&lt;Row&gt; highScoresDF = spark.sql(&quot;SELECT name,score FROM student_scores WHERE score &gt;= 80&quot;);</span><br><span class="line"></span><br><span class="line">// 获取分数大于80分的学生的姓名</span><br><span class="line">List&lt;String&gt; highScoresNames = highScoresDF.javaRDD().map(new Function&lt;Row, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public String call(Row row) throws Exception &#123;</span><br><span class="line">return row.getString(0);</span><br><span class="line">&#125;</span><br><span class="line">&#125;).collect();</span><br><span class="line"></span><br><span class="line">// 使用Dataset&lt;String&gt;创建DataFrame</span><br><span class="line">List&lt;String&gt; agesJSON = new ArrayList&lt;String&gt;();</span><br><span class="line">agesJSON.add(&quot;&#123;\&quot;name\&quot;:\&quot;Tom\&quot;, \&quot;age\&quot;:21&#125;&quot;);</span><br><span class="line">agesJSON.add(&quot;&#123;\&quot;name\&quot;:\&quot;Bob\&quot;, \&quot;age\&quot;:25&#125;&quot;);</span><br><span class="line">agesJSON.add(&quot;&#123;\&quot;name\&quot;:\&quot;Alice\&quot;, \&quot;age\&quot;:18&#125;&quot;);</span><br><span class="line">Dataset&lt;String&gt; agesJSONRDD = spark.createDataset(agesJSON, Encoders.STRING());</span><br><span class="line">Dataset&lt;Row&gt; agesDF = spark.read().json(agesJSONRDD);</span><br><span class="line"></span><br><span class="line">// 针对学生年龄信息的DataFrame注册为临时表，查询分数大于80分的学生的姓名和年龄</span><br><span class="line">agesDF.createOrReplaceTempView(&quot;student_ages&quot;);</span><br><span class="line">String sql = &quot;SELECT name,age FROM student_ages WHERE name IN (&quot;;</span><br><span class="line">for (int i = 0; i &lt; highScoresNames.size(); i++) &#123;</span><br><span class="line">sql += &quot;&apos;&quot; + highScoresNames.get(i) + &quot;&apos;&quot;;</span><br><span class="line">if (i &lt; highScoresNames.size() - 1) &#123;</span><br><span class="line">sql += &quot;,&quot;;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">sql += &quot;)&quot;;</span><br><span class="line">Dataset&lt;Row&gt; highScoresAgesDF = spark.sql(sql);</span><br><span class="line"></span><br><span class="line">// 将两份数据的DataFrame转换为JavaPairRDD，并执行join操作</span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;Integer, Integer&gt;&gt; highScoresStudentsRDD = highScoresDF.javaRDD()</span><br><span class="line">.mapToPair(new PairFunction&lt;Row, String, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, Integer&gt; call(Row row) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;String, Integer&gt;(row.getString(0),</span><br><span class="line">Integer.valueOf(String.valueOf(row.getLong(1))));</span><br><span class="line">&#125;</span><br><span class="line">&#125;).join(highScoresAgesDF.javaRDD().mapToPair(new PairFunction&lt;Row, String, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, Integer&gt; call(Row row) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;String, Integer&gt;(row.getString(0),</span><br><span class="line">Integer.valueOf(String.valueOf(row.getLong(1))));</span><br><span class="line">&#125;</span><br><span class="line">&#125;));</span><br><span class="line"></span><br><span class="line">// 将封装在RDD中的信息转换为JavaRDD&lt;Row&gt;的格式</span><br><span class="line">JavaRDD&lt;Row&gt; highScoresStudentsRowRDD = highScoresStudentsRDD</span><br><span class="line">.map(new Function&lt;Tuple2&lt;String, Tuple2&lt;Integer, Integer&gt;&gt;, Row&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Row call(Tuple2&lt;String, Tuple2&lt;Integer, Integer&gt;&gt; tuple) throws Exception &#123;</span><br><span class="line">return RowFactory.create(tuple._1, tuple._2._1, tuple._2._2);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">// 创建一份元数据，将JavaRDD&lt;Row&gt;转换为DataFrame</span><br><span class="line">List&lt;StructField&gt; structFields = new ArrayList&lt;StructField&gt;();</span><br><span class="line">structFields.add(DataTypes.createStructField(&quot;name&quot;, DataTypes.StringType, true));</span><br><span class="line">structFields.add(DataTypes.createStructField(&quot;score&quot;, DataTypes.IntegerType, true));</span><br><span class="line">structFields.add(DataTypes.createStructField(&quot;age&quot;, DataTypes.IntegerType, true));</span><br><span class="line">StructType structType = DataTypes.createStructType(structFields);</span><br><span class="line">Dataset&lt;Row&gt; highScoresStudentsDF = spark.createDataFrame(highScoresStudentsRowRDD, structType);</span><br><span class="line"></span><br><span class="line">// 将信息保存到一个json文件中</span><br><span class="line">highScoresStudentsDF.write().format(&quot;json&quot;).save(&quot;C://Users//asus//Desktop//high_score&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后，就可以在保存的目录上看到high_score文件夹，文件夹内容如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/101-1.png" width="90%" height="90%"><br>使用记事本打开json文件，看到如下内容，即查询出了成绩在80分以上的学生的年龄信息和成绩信息<br><img src="http://pd8lpasbc.bkt.clouddn.com/101-2.png" width="70%" height="70%"></p><p>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">import org.apache.spark.sql.types.StructType</span><br><span class="line">import org.apache.spark.sql.types.StructField</span><br><span class="line">import org.apache.spark.sql.types.StringType</span><br><span class="line">import org.apache.spark.sql.types.IntegerType</span><br><span class="line"></span><br><span class="line">object JSONDataSource &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;JSONDataSource&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .getOrCreate</span><br><span class="line"></span><br><span class="line">    // 使用JSON文件创建DataFrame</span><br><span class="line">    val scoresDF = spark.read.json(&quot;C://Users//asus//Desktop//scores.json&quot;)</span><br><span class="line"></span><br><span class="line">    // 针对学生成绩信息的DataFrame注册为临时表，查询分数大于80分的学生的姓名和成绩</span><br><span class="line">    scoresDF.createOrReplaceTempView(&quot;student_scores&quot;)</span><br><span class="line">    val highScoresDF = spark.sql(&quot;SELECT name,score FROM student_scores WHERE score &gt;= 80&quot;)</span><br><span class="line">    // 获取分数大于80分的学生的姓名</span><br><span class="line">    val highScoresNames = highScoresDF.rdd.map(row =&gt; row(0)).collect</span><br><span class="line"></span><br><span class="line">    // 使用Dataset&lt;String&gt;创建DataFrame</span><br><span class="line">    val agesJSON = Array(</span><br><span class="line">      &quot;&#123;\&quot;name\&quot;:\&quot;Tom\&quot;,\&quot;age\&quot;:21&#125;&quot;,</span><br><span class="line">      &quot;&#123;\&quot;name\&quot;:\&quot;Bob\&quot;,\&quot;age\&quot;:25&#125;&quot;,</span><br><span class="line">      &quot;&#123;\&quot;name\&quot;:\&quot;Alice\&quot;,\&quot;age\&quot;:18&#125;&quot;);</span><br><span class="line">    val agesJSONRDD = spark.sparkContext.parallelize(agesJSON)</span><br><span class="line">    val agesDF = spark.read.json(agesJSONRDD)</span><br><span class="line"></span><br><span class="line">    // 针对学生年龄信息的DataFrame注册为临时表，查询分数大于80分的学生的姓名和年龄</span><br><span class="line">    agesDF.createOrReplaceTempView(&quot;student_ages&quot;)</span><br><span class="line">    var sql = &quot;SELECT name,age FROM student_ages WHERE name IN (&quot;</span><br><span class="line">    for (i &lt;- 0 until highScoresNames.length) &#123;</span><br><span class="line">      sql += &quot;&apos;&quot; + highScoresNames(i) + &quot;&apos;&quot;</span><br><span class="line">      if (i &lt; highScoresNames.length - 1) &#123;</span><br><span class="line">        sql += &quot;,&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    sql += &quot;)&quot;</span><br><span class="line">    val highScoresAgesDF = spark.sql(sql)</span><br><span class="line"></span><br><span class="line">    // 将两份数据的DataFrame转换为JavaPairRDD，并执行join操作</span><br><span class="line">    val highScoresStudentsRDD = highScoresDF.rdd.map &#123; row =&gt; (row.getAs[String](&quot;name&quot;), row.getAs[Long](&quot;score&quot;).toString().toInt) &#125;</span><br><span class="line">      .join(highScoresAgesDF.rdd.map &#123; row =&gt; (row.getAs[String](&quot;name&quot;), row.getAs[Long](&quot;age&quot;).toString().toInt) &#125;)</span><br><span class="line"></span><br><span class="line">    // 将封装在RDD中的信息转换为JavaRDD&lt;Row&gt;的格式</span><br><span class="line">    val highScoresStudentsRowRDD = highScoresStudentsRDD.map &#123; tuple =&gt; Row(tuple._1, tuple._2._1, tuple._2._2) &#125;</span><br><span class="line"></span><br><span class="line">    // 创建一份元数据，将JavaRDD&lt;Row&gt;转换为DataFrame</span><br><span class="line">    val structType = StructType(Array(</span><br><span class="line">      StructField(&quot;name&quot;, StringType),</span><br><span class="line">      StructField(&quot;score&quot;, IntegerType),</span><br><span class="line">      StructField(&quot;age&quot;, IntegerType)))</span><br><span class="line">    val highScoresStudentsDF = spark.createDataFrame(highScoresStudentsRowRDD, structType)</span><br><span class="line"></span><br><span class="line">    // 将信息保存到一个json文件中</span><br><span class="line">    highScoresStudentsDF.write.format(&quot;json&quot;).save(&quot;C://Users//asus//Desktop//high_score&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，同样得到如上结果<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spark SQL可以自动推断JSON文件的元数据，并且加载其数据，创建一个DataFrame。&lt;br&gt;可以使用SparkSession.read.json()方法，针对一个元素类型为String的RDD，或者是一个JSON文件创建DataFrame。&lt;/p&gt;
&lt;p&gt;这里使用的JSON文件，每行必须只能包含一个单独的、自包含的、有效的JSON对象。不能让一个JSON对象分散在多行。否则会报错。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL中的Parquet数据源</title>
    <link href="https://www.ggstu.com/2018/09/29/Spark-SQL%E4%B8%AD%E7%9A%84Parquet%E6%95%B0%E6%8D%AE%E6%BA%90/"/>
    <id>https://www.ggstu.com/2018/09/29/Spark-SQL中的Parquet数据源/</id>
    <published>2018-09-29T01:14:30.000Z</published>
    <updated>2018-09-29T10:39:10.391Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>什么是Parquet文件？</b></font><br>Parquet是列式存储格式的一种文件类型。列式存储和行式存储相比有如下优势：<br>1、可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量。<br>2、压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码(如Run Length Encoding和Delta Encoding)进一步节约存储空间。<br>3、只读取需要的列，支持向量运算，能过获取更好的扫描性能。<br><a id="more"></a><br><br></p><p><font size="4"><b>以编程方式加载数据</b></font><br>这里使用spark官方包提供的示例文件users.parquet为例，查询用户数据中的用户姓名<br>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.api.java.function.MapFunction;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Encoders;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line"></span><br><span class="line">public class LoadDataProgrammatically &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkSession spark = new SparkSession.Builder().appName(&quot;LoadDataProgrammatically&quot;).master(&quot;local&quot;)</span><br><span class="line">.getOrCreate();</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; usersDF = spark.read().parquet(&quot;C://Users//asus//Desktop//users.parquet&quot;);</span><br><span class="line"></span><br><span class="line">usersDF.createOrReplaceTempView(&quot;users&quot;);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; namesDF = spark.sql(&quot;SELECT name FROM users&quot;);</span><br><span class="line"></span><br><span class="line">Dataset&lt;String&gt; namesDS = namesDF.map((MapFunction&lt;Row, String&gt;) row -&gt; &quot;Name: &quot; + row.getString(0),</span><br><span class="line">Encoders.STRING());</span><br><span class="line"></span><br><span class="line">namesDF.show();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后，得到如下结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/100-1.png" width="15%" height="15%"><br>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object LoadDataProgrammatically &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;LoadDataProgrammatically&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">      </span><br><span class="line">    val usersDF = spark.read.parquet(&quot;C://Users//asus//Desktop//users.parquet&quot;)</span><br><span class="line">    </span><br><span class="line">    usersDF.createOrReplaceTempView(&quot;users&quot;)</span><br><span class="line">    </span><br><span class="line">    val namesDF = spark.sql(&quot;SELECT name FROM users&quot;)</span><br><span class="line">    </span><br><span class="line">    import spark.implicits._</span><br><span class="line">    namesDF.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，同样得到如上结果<br><br></p><p><font size="4"><b>自动分区判断</b></font><br>表的分区是Hive等系统中常见的优化方式。在分区表中，数据通常存储在不同的目录中，分区列的值通常就包含了分区目录的目录名中。Spark SQL中的Parquet数据源，支持自动根据目录名推断出分区信息。</p><p>如果将人口数据存储在分区表中，并且使用性别和国家作为分区列，那么目录结构可能如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender=male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country=US</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country=CN</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender=female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country=US</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country=CN</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure></p><p>如果将path/to/table传入SparkSession.read.parquet或者SQLContext.read.load方法，那么Spark SQL就会自动根据目录结构提取分区信息，是gender和country。即使数据文件中只包含了两列值，name和age，但是Spark SQL返回的DataFrame调用printSchema()方法时，会打印出四个列的值：name、age、country、gender，如下所示。这就是自动分区推断的功能。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable = true)</span><br><span class="line">|-- age: long (nullable = true)</span><br><span class="line">|-- gender: string (nullable = true)</span><br><span class="line">|-- country: string (nullable = true)</span><br></pre></td></tr></table></figure></p><p>此外，分区列的数据类型也是自动推断出来的。目前Spark SQL支持自动推断出数字类型、日期、时间戳和字符串类型。<br>如果不希望Spark SQL自动推断分区列的数据类型，可以配置自动类型推断spark.sql.sources.partitionColumnTypeInference.enabled，默认为true，设置为false，就不会自动推断类型。禁止自动推断分区列的类型时，所有分区列的类型，统一默认都是String。</p><p>例如：自动推断用户数据的性别和国家<br>首先在本地目录创建个文件夹users，在此文件夹下再创建个文件夹gender=male，接着在gender=male下创建个文件夹country=CN，最后将users.parquet放到country=CN文件夹下<br>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package com.ggstu.sparksql;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line"></span><br><span class="line">public class ParquetPartition &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkSession spark = new SparkSession</span><br><span class="line">.Builder()</span><br><span class="line">.appName(&quot;ParquetPartition&quot;)</span><br><span class="line">.master(&quot;local&quot;)</span><br><span class="line">.getOrCreate();</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; usersDF = spark.read().parquet(&quot;C://Users//asus//Desktop//users&quot;);</span><br><span class="line"></span><br><span class="line">usersDF.printSchema();</span><br><span class="line"></span><br><span class="line">usersDF.show();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后，得到如下结果，即自动根据目录结构提取分区信息，生成了两列gender和country<br><img src="http://pd8lpasbc.bkt.clouddn.com/100-2.png" width="70%" height="70%"><br><img src="http://pd8lpasbc.bkt.clouddn.com/100-3.png" width="70%" height="70%"><br><br></p><p><font size="4"><b>合并元数据</b></font><br>如同ProtocolBuffer、Avro、Thrift一样，Parquet也支持元数据合并。用户可以在一开始就定义一个简单的元数据，然后随着业务需要，逐渐往元数据中添加更多的列。在这种情况下，用户可能会创建多个Parquet文件，有着多个不同的但是却相互兼容的元数据。Parquet数据源支持自动推断出这种情况，并且进行多个Parquet文件的元数据的合并。</p><p>因为元数据合并是一种相对耗时的操作，而且在大多数情况下不是一种必要的特性，从Spark1.5.0版本开始，默认是关闭Parquet文件的自动合并元数据的特性的。<br>可以通过以下两种方式开启Parquet数据源的自动合并元数据的特性：<br>1、读取Parquet文件时，将数据源的选项，mergeSchema设置为true<br>2、使用SQLContext.setConf方法，将spark.sql.parquet.mergeSchema参数设置为true</p><p>例如：合并学生的基本信息和成绩信息的元数据<br>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.SaveMode</span><br><span class="line"></span><br><span class="line">object MergeSchema &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;MergeSchema&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    //创建一个DataFrame，作为学生的基本信息并写入一个parquet文件中</span><br><span class="line">    val ages = Array((&quot;Tom&quot;, 21), (&quot;Bob&quot;, 25), (&quot;Alice&quot;, 18)).toSeq</span><br><span class="line">    val agesDF = spark.sparkContext.parallelize(ages).toDF(&quot;name&quot;, &quot;age&quot;)</span><br><span class="line">    agesDF.write.format(&quot;parquet&quot;).mode(&quot;append&quot;).save(&quot;C://Users//asus//Desktop//students&quot;)</span><br><span class="line"></span><br><span class="line">    //创建一个DataFrame，作为学生的成绩信息并写入一个parquet文件中</span><br><span class="line">    val scores = Array((&quot;Jack&quot;, &quot;A&quot;), (&quot;Jerry&quot;, &quot;B&quot;)).toSeq</span><br><span class="line">    val scoresDF = spark.sparkContext.parallelize(scores).toDF(&quot;name&quot;, &quot;score&quot;)</span><br><span class="line">    scoresDF.write.format(&quot;parquet&quot;).mode(&quot;append&quot;).save(&quot;C://Users//asus//Desktop//students&quot;)</span><br><span class="line"></span><br><span class="line">    //用mergeSchema的方式读取students表中的数据，进行元数据的合并</span><br><span class="line">    val students = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;C://Users//asus//Desktop//students&quot;)</span><br><span class="line"></span><br><span class="line">    students.printSchema</span><br><span class="line">    students.show</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>第一个DataFrame包含了name和age两个列，第二个DataFrame包含了name和score两个列，所以自动合并两个文件的元数据，出现三个列，name、age、score，如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/100-4.png" width="60%" height="60%"><br><img src="http://pd8lpasbc.bkt.clouddn.com/100-5.png" width="30%" height="30%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;什么是Parquet文件？&lt;/b&gt;&lt;/font&gt;&lt;br&gt;Parquet是列式存储格式的一种文件类型。列式存储和行式存储相比有如下优势：&lt;br&gt;1、可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量。&lt;br&gt;2、压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码(如Run Length Encoding和Delta Encoding)进一步节约存储空间。&lt;br&gt;3、只读取需要的列，支持向量运算，能过获取更好的扫描性能。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL之通用的Load/Save操作</title>
    <link href="https://www.ggstu.com/2018/09/28/Spark-SQL%E4%B9%8B%E9%80%9A%E7%94%A8%E7%9A%84Load-Save%E6%93%8D%E4%BD%9C/"/>
    <id>https://www.ggstu.com/2018/09/28/Spark-SQL之通用的Load-Save操作/</id>
    <published>2018-09-28T07:57:24.000Z</published>
    <updated>2018-09-28T12:50:39.619Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>通用的Load/Save操作</b></font><br>对于Spark SQL的DataFrame来说，无论是从什么数据源创建出来的DataFrame，都有一些共同的load和save操作。load操作主要用于加载数据，创建出DataFrame。save操作，主要用于将DataFrame中的数据保存到文件中。<br>这里以spark官方包提供的示例文件users.parquet为例，将其下载到本地<br><a href="https://github.com/apache/spark/blob/master/examples/src/main/resources/users.parquet" target="_blank">https://github.com/apache/spark/blob/master/examples/src/main/resources/users.parquet</a><br><a id="more"></a><br>例如：load/save使用示例<br>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line"></span><br><span class="line">public class LoadAndSaveTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkSession spark = new SparkSession</span><br><span class="line">.Builder()</span><br><span class="line">.appName(&quot;LoadAndSaveTest&quot;)</span><br><span class="line">.master(&quot;local&quot;)</span><br><span class="line">.getOrCreate();</span><br><span class="line"></span><br><span class="line">//加载数据</span><br><span class="line">Dataset&lt;Row&gt; usersDF = spark.read().load(&quot;C://Users//asus//Desktop//users.parquet&quot;);</span><br><span class="line"></span><br><span class="line">usersDF.select(&quot;name&quot;, &quot;favorite_color&quot;).javaRDD().foreach(new VoidFunction&lt;Row&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Row row) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;Name: &quot; + row.getString(0) + &quot;\t&quot; + &quot;Favorite_color: &quot; + row.getString(1));</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">//保存数据</span><br><span class="line">usersDF.select(&quot;name&quot;, &quot;favorite_color&quot;).write().save(&quot;C://Users//asus//Desktop//namesAndFavColors.parquet&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后，在控制台输出如下结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/99-1.png" width="70%" height="70%"><br>并且在save的路径上生成了个名为namesAndFavColors.parquet的文件夹，文件夹中内容如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/99-2.png" width="90%" height="90%"><br>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object LoadAndSaveTest &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;LoadAndSaveTest&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">      </span><br><span class="line">    val usersDF = spark.read.load(&quot;C://Users//asus//Desktop//users.parquet&quot;)</span><br><span class="line">    </span><br><span class="line">    usersDF.select(&quot;name&quot;, &quot;favorite_color&quot;).rdd.foreach(row =&gt; println(&quot;Name: &quot; + row.getString(0) + &quot;\t&quot; + &quot;Favorite_color: &quot; + row.getString(1)))</span><br><span class="line">    </span><br><span class="line">    usersDF.select(&quot;name&quot;, &quot;favorite_color&quot;).write.save(&quot;C://Users//asus//Desktop//namesAndFavColors.parquet&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，同样得到如上的结果<br><br></p><p><font size="4"><b>手动指定数据源类型</b></font><br>可以手动指定用来操作的数据源类型。数据源通常需要使用全限定名来指定，比如parquet是org.apache.spark.sql.parquet，但是Spark SQL内置了一些数据源类型可以使用短名称，比如json，parquet，jdbc，orc，libsvm，csv，text。<br>通过这个功能，就可以在不同类型的数据源之间进行转换。比如将json文件中的数据保存到parquet文件中。<br>如果不指定数据源类型，默认是parquet类型。</p><p>首先在本地桌面上创建个json文件，student.json，文件内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;id&quot;:1, &quot;name&quot;:&quot;Tom&quot;, &quot;age&quot;:21&#125;</span><br><span class="line">&#123;&quot;id&quot;:2, &quot;name&quot;:&quot;Bob&quot;, &quot;age&quot;:25&#125;</span><br><span class="line">&#123;&quot;id&quot;:3, &quot;name&quot;:&quot;Alice&quot;, &quot;age&quot;:18&#125;</span><br><span class="line">&#123;&quot;id&quot;:4, &quot;name&quot;:&quot;Jack&quot;, &quot;age&quot;:23&#125;</span><br><span class="line">&#123;&quot;id&quot;:5, &quot;name&quot;:&quot;Jerry&quot;, &quot;age&quot;:25&#125;</span><br></pre></td></tr></table></figure></p><p>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line"></span><br><span class="line">public class ManuallySpecifiedType &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkSession spark = new SparkSession</span><br><span class="line">.Builder()</span><br><span class="line">.appName(&quot;ManuallySpecifiedType&quot;)</span><br><span class="line">.master(&quot;local&quot;)</span><br><span class="line">.getOrCreate();</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; studentDF = spark.read().format(&quot;json&quot;).load(&quot;C://Users//asus//Desktop//student.json&quot;);</span><br><span class="line"></span><br><span class="line">studentDF.select(&quot;name&quot;, &quot;age&quot;).write().format(&quot;parquet&quot;).save(&quot;C://Users//asus//Desktop//namesAndAges.parquet&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后，在save的路径上生成namesAndAges.parquet，即实现了在不同类型的数据源之间进行转换。</p><p>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object ManuallySpecifiedType &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;ManuallySpecifiedType&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">      </span><br><span class="line">    val studentDF = spark.read.format(&quot;json&quot;).load(&quot;C://Users//asus//Desktop//student.json&quot;)</span><br><span class="line">    </span><br><span class="line">    studentDF.select(&quot;name&quot;, &quot;age&quot;).write.format(&quot;parquet&quot;).save(&quot;C://Users//asus//Desktop//namesAndAges.parquet&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，在save的路径上生成namesAndAges.parquet<br><br></p><p><font size="4"><b>Save Modes（存储模式）</b></font><br>Spark SQL对于save操作，提供了不同的save mode。主要用来处理当目标位置已经有数据时，如何处理现有数据。而且save操作并不会执行锁操作，并且不是原子的，因此有一定风险出现脏数据。<br>Save Mode详细介绍如下表：</p><table><thead><tr><th style="text-align:left">Save Mode</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:left">SaveMode.ErrorIfExists(默认)</td><td style="text-align:left">如果目标位置已经存在数据，那么抛出一个异常</td></tr><tr><td style="text-align:left">SaveMode.Append</td><td style="text-align:left">如果目标位置已经存在数据，那么将数据追加进去</td></tr><tr><td style="text-align:left">SaveMode.Overwrite</td><td style="text-align:left">如果目标位置已经存在数据，那么将已经存在的数据删除，用新数据进行覆盖</td></tr><tr><td style="text-align:left">SaveMode.Ignore</td><td style="text-align:left">如果目标位置已经存在数据，那么就忽略，不做任何操作</td></tr></tbody></table><p>例如：usersDF.select($”name”).write.save(“/root/result/parquet1”)<br>出错，因为/root/result/parquet1已经存在<br>usersDF.select($”name”).write.mode(“overwrite”).save(“/root/result/parquet1”)<br>不会出错，因为进行了覆盖<br><br></p><p><font size="4"><b>将结果保存为表</b></font><br>使用saveAsTable方法，DataFrames也可以使用saveAsTable方法将持久化表保存到Hive Metastore中。<br>例如：usersDF.select($”name”).write.saveAsTable(“table1”)<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;通用的Load/Save操作&lt;/b&gt;&lt;/font&gt;&lt;br&gt;对于Spark SQL的DataFrame来说，无论是从什么数据源创建出来的DataFrame，都有一些共同的load和save操作。load操作主要用于加载数据，创建出DataFrame。save操作，主要用于将DataFrame中的数据保存到文件中。&lt;br&gt;这里以spark官方包提供的示例文件users.parquet为例，将其下载到本地&lt;br&gt;&lt;a href=&quot;https://github.com/apache/spark/blob/master/examples/src/main/resources/users.parquet&quot; target=&quot;_blank&quot;&gt;https://github.com/apache/spark/blob/master/examples/src/main/resources/users.parquet&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL之将RDD转换成Datasets的两种方式</title>
    <link href="https://www.ggstu.com/2018/09/28/Spark-SQL%E4%B9%8B%E5%B0%86RDD%E8%BD%AC%E6%8D%A2%E6%88%90Datasets%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/"/>
    <id>https://www.ggstu.com/2018/09/28/Spark-SQL之将RDD转换成Datasets的两种方式/</id>
    <published>2018-09-28T04:10:24.000Z</published>
    <updated>2018-09-28T07:58:45.984Z</updated>
    
    <content type="html"><![CDATA[<p>将RDD转换成Datasets，就可以直接针对集合、本地文件、HDFS等任何可以构建为RDD的数据，使用SparkSQL进行SQL查询。</p><p><b>Spark SQL支持两种方式来将RDD转换成Datasets：</b></p><p>第一种方法使用反射来推断包含了特定数据类型的RDD的元数据。这种基于反射的方式，代码比较简洁。如果知道RDD的元数据时，使用这种方式比较好。<br><a id="more"></a><br>第二种方法通过编程接口来创建Datasets，然后将其应用到已经存在的RDD上。这种方式更详细，但是如果在编写程序时，还不知道RDD的元数据，只有在程序运行时才能动态得知其元数据，那么使用这种方式。<br><br><br><b>方式一：使用反射方式推断元数据</b><br>Java代码：<br>Spark SQL支持将包含了JavaBean的RDD转换为DataFrame。JavaBean的信息，就定义了元数据。<br>在桌面创建一个文本文件studentInfo.txt，内容如下，三列分别对应学号，姓名，年龄<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1,Tom,21</span><br><span class="line">2,Bob,25</span><br><span class="line">3,Alice,18</span><br><span class="line">4,Jack,12</span><br><span class="line">5,Jerry,16</span><br></pre></td></tr></table></figure></p><p>创建一个学生类，定义学号，姓名，年龄三个字段，并生成其set和get方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import java.io.Serializable;</span><br><span class="line"></span><br><span class="line">public class Student implements Serializable &#123;</span><br><span class="line"></span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">private int id;</span><br><span class="line">private String name;</span><br><span class="line">private int age;</span><br><span class="line"></span><br><span class="line">public int getId() &#123;</span><br><span class="line">return id;</span><br><span class="line">&#125;</span><br><span class="line">public void setId(int id) &#123;</span><br><span class="line">this.id = id;</span><br><span class="line">&#125;</span><br><span class="line">public String getName() &#123;</span><br><span class="line">return name;</span><br><span class="line">&#125;</span><br><span class="line">public void setName(String name) &#123;</span><br><span class="line">this.name = name;</span><br><span class="line">&#125;</span><br><span class="line">public int getAge() &#123;</span><br><span class="line">return age;</span><br><span class="line">&#125;</span><br><span class="line">public void setAge(int age) &#123;</span><br><span class="line">this.age = age;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.function.MapFunction;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Encoder;</span><br><span class="line">import org.apache.spark.sql.Encoders;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line"></span><br><span class="line">public class RDD2DatasetsReflection &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkSession spark = new SparkSession</span><br><span class="line">.Builder()</span><br><span class="line">.appName(&quot;RDD2DatasetsReflection&quot;)</span><br><span class="line">.master(&quot;local&quot;)</span><br><span class="line">.getOrCreate();</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Student&gt; studentRDD = spark.read()</span><br><span class="line">.textFile(&quot;C://Users//asus//Desktop//studentInfo.txt&quot;)</span><br><span class="line">.javaRDD()</span><br><span class="line">.map(line -&gt; &#123;</span><br><span class="line">String[] parts = line.split(&quot;,&quot;);</span><br><span class="line">Student student = new Student();</span><br><span class="line">student.setId(Integer.valueOf(parts[0]));</span><br><span class="line">student.setName(parts[1]);</span><br><span class="line">student.setAge(Integer.valueOf(parts[2]));</span><br><span class="line">return student;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">//使用反射方式，将RDD转换为DataFrame</span><br><span class="line">Dataset&lt;Row&gt; studentDF = spark.createDataFrame(studentRDD, Student.class);</span><br><span class="line"></span><br><span class="line">//将DataFrame注册为一个临时表，然后针对其中的数据执行SQL语句</span><br><span class="line">studentDF.createOrReplaceTempView(&quot;student&quot;);</span><br><span class="line"></span><br><span class="line">//查询年龄大于18的学生</span><br><span class="line">Dataset&lt;Row&gt; adultDF = spark.sql(&quot;SELECT name,age FROM student where age &gt;= 18&quot;);</span><br><span class="line"></span><br><span class="line">Encoder&lt;String&gt; stringEncoder = Encoders.STRING();</span><br><span class="line">Dataset&lt;String&gt; adultNamesAndAges = adultDF.map(</span><br><span class="line">(MapFunction&lt;Row, String&gt;) row -&gt; &quot;Name: &quot; + row.getString(0)+ &quot;\t&quot; + &quot;Age: &quot; + row.getInt(1),</span><br><span class="line">stringEncoder);</span><br><span class="line"></span><br><span class="line">adultNamesAndAges.show();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在本地IDE运行后，得到如下结果，即查询出年龄大于18学生信息<br><img src="http://pd8lpasbc.bkt.clouddn.com/98-1.png" width="30%" height="30%"><br>Scala代码：<br>由于Scala具有隐式转换的特性，所以Spark SQL的Scala接口支持自动将包含了case class的RDD转换为DataFrame。case class就定义了元数据。Spark SQL会通过反射读取传递给case class的参数的名称，然后将其作为列名。<br>使用同样的例子用Scala代码来编写<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">case class Student(id:Int, name:String, age:Int)</span><br><span class="line"></span><br><span class="line">object RDD2DatasetsReflection &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;RDD2DatasetsReflection&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">   </span><br><span class="line">    import spark.implicits._</span><br><span class="line">      </span><br><span class="line">    val studentDF = spark.sparkContext</span><br><span class="line">      .textFile(&quot;C://Users//asus//Desktop//studentInfo.txt&quot;)</span><br><span class="line">      .map(_.split(&quot;,&quot;))</span><br><span class="line">      .map(attributes =&gt; Student(attributes(0).toInt, attributes(1), attributes(2).toInt))</span><br><span class="line">      .toDF()</span><br><span class="line">      </span><br><span class="line">    studentDF.createOrReplaceTempView(&quot;student&quot;)</span><br><span class="line">    </span><br><span class="line">    val adultDF = spark.sql(&quot;SELECT name,age FROM student where age&gt;= 18&quot;)</span><br><span class="line">    </span><br><span class="line">    adultDF.map(adult =&gt; &quot;Name: &quot; + adult(0) + &quot;\t&quot; + &quot;Age: &quot; + adult(1)).show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，同样得到如上结果<br><br><br><b>方式二：动态创建，以编程方式指定schema</b><br>如果在编写程序时，还不知道RDD的元数据，只有在程序运行时才能动态得知其元数据，那么使用这种方式。<br>这里仍然使用方式一中在本地创建的studentInfo.txt<br>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.api.java.function.MapFunction;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Encoder;</span><br><span class="line">import org.apache.spark.sql.Encoders;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.RowFactory;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line">import org.apache.spark.sql.types.DataTypes;</span><br><span class="line">import org.apache.spark.sql.types.StructField;</span><br><span class="line">import org.apache.spark.sql.types.StructType;</span><br><span class="line"></span><br><span class="line">public class RDD2DatasetsProgrammatically &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkSession spark = new SparkSession</span><br><span class="line">.Builder()</span><br><span class="line">.appName(&quot;RDD2DatasetsProgrammatically&quot;)</span><br><span class="line">.master(&quot;local&quot;)</span><br><span class="line">.getOrCreate();</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; studentRDD = spark.sparkContext()</span><br><span class="line">.textFile(&quot;C://Users//asus//Desktop//studentInfo.txt&quot;, 1)</span><br><span class="line">.toJavaRDD();</span><br><span class="line"></span><br><span class="line">//动态构造元数据</span><br><span class="line">String schemaString = &quot;id name age&quot;;</span><br><span class="line"></span><br><span class="line">List&lt;StructField&gt; fields = new ArrayList();</span><br><span class="line">for(String fieldName : schemaString.split(&quot; &quot;)) &#123;</span><br><span class="line">StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, true);</span><br><span class="line">fields.add(field);</span><br><span class="line">&#125;</span><br><span class="line">StructType schema = DataTypes.createStructType(fields);</span><br><span class="line"></span><br><span class="line">//将RDD转换为RDD&lt;Row&gt;这种格式</span><br><span class="line">JavaRDD&lt;Row&gt; rowRDD = studentRDD.map((Function&lt;String, Row&gt;) record -&gt; &#123;</span><br><span class="line">String[] attrubutes = record.split(&quot;,&quot;);</span><br><span class="line">return RowFactory.create(attrubutes[0], attrubutes[1], attrubutes[2]);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">//使用动态构造的元数据，将RDD转换为DataFrame</span><br><span class="line">Dataset&lt;Row&gt; studentDF = spark.createDataFrame(rowRDD, schema);</span><br><span class="line"></span><br><span class="line">studentDF.createOrReplaceTempView(&quot;student&quot;);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; adultDF = spark.sql(&quot;SELECT name,age FROM student where age &gt;= 18&quot;);</span><br><span class="line"></span><br><span class="line">Encoder&lt;String&gt; stringEncoder = Encoders.STRING();</span><br><span class="line">Dataset&lt;String&gt; adultNamesAndAges = adultDF.map(</span><br><span class="line">(MapFunction&lt;Row, String&gt;) row -&gt; &quot;Name: &quot; + row.getString(0)+ &quot;\t&quot; + &quot;Age: &quot; + row.getString(1),</span><br><span class="line">stringEncoder);</span><br><span class="line"></span><br><span class="line">adultNamesAndAges.show();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后，同样得到方式一运行后的结果</p><p>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.types.StructField</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line"></span><br><span class="line">object RDD2DatasetsProgrammatically &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;RDD2DatasetsProgrammatically&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    val studentRDD = spark.sparkContext.textFile(&quot;C://Users//asus//Desktop//studentInfo.txt&quot;)</span><br><span class="line"></span><br><span class="line">    val schemaString = &quot;id name age&quot;</span><br><span class="line"></span><br><span class="line">    import org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line">    val fields = schemaString.split(&quot; &quot;)</span><br><span class="line">      .map(fieldName =&gt; StructField(fieldName, StringType, true))</span><br><span class="line">    val schema = StructType(fields)</span><br><span class="line"></span><br><span class="line">    val rowRDD = studentRDD</span><br><span class="line">      .map(_.split(&quot;,&quot;))</span><br><span class="line">      .map(attributes =&gt; Row(attributes(0), attributes(1), attributes(2)))</span><br><span class="line">      </span><br><span class="line">    val studentDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line">    </span><br><span class="line">    studentDF.createOrReplaceTempView(&quot;student&quot;)</span><br><span class="line">    </span><br><span class="line">    val adultDF = spark.sql(&quot;SELECT name,age FROM student where age&gt;= 18&quot;)</span><br><span class="line">    </span><br><span class="line">    import spark.implicits._</span><br><span class="line">    </span><br><span class="line">    adultDF.map(adult =&gt; &quot;Name: &quot; + adult(0) + &quot;\t&quot; + &quot;Age: &quot; + adult(1)).show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，同样得到方式一运行后的结果<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;将RDD转换成Datasets，就可以直接针对集合、本地文件、HDFS等任何可以构建为RDD的数据，使用SparkSQL进行SQL查询。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Spark SQL支持两种方式来将RDD转换成Datasets：&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;第一种方法使用反射来推断包含了特定数据类型的RDD的元数据。这种基于反射的方式，代码比较简洁。如果知道RDD的元数据时，使用这种方式比较好。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL之创建Datasets</title>
    <link href="https://www.ggstu.com/2018/09/26/Spark-SQL%E4%B9%8B%E5%88%9B%E5%BB%BADatasets/"/>
    <id>https://www.ggstu.com/2018/09/26/Spark-SQL之创建Datasets/</id>
    <published>2018-09-26T13:27:52.000Z</published>
    <updated>2018-09-26T12:50:46.480Z</updated>
    
    <content type="html"><![CDATA[<p>DataFrame的引入，可以让Spark更好的处理结构数据的计算，但其中一个主要的问题是：缺乏编译时类型安全。为了解决这个问题，Spark采用新的Dataset API（DataFrame API的类型扩展）。</p><p>Dataset是分布式数据集合。Dataset是Spark 1.6中添加的一个接口，兼顾了RDD的优点（强类型化，使用功能强大的lambda函数）以及Spark SQL优化后的执行引擎的优点。<br><a id="more"></a><br><img src="http://pd8lpasbc.bkt.clouddn.com/97-1.png" width="70%" height="70%"><br><br></p><p><font size="4"><b>创建Dataset</b></font><br><b>方式一：使用序列</b><br>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">//定义case class</span><br><span class="line">case class StudentInfo(id: Int, name: String, age: Int)</span><br><span class="line"></span><br><span class="line">object CreateDatasetSeq &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;CreateDatasetSeq&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .getOrCreate</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    //生成序列，并创建Dataset</span><br><span class="line">    val ds = Seq(StudentInfo(1, &quot;Tom&quot;, 21), StudentInfo(2, &quot;Bob&quot;, 25), StudentInfo(3, &quot;Alice&quot;, 18)).toDS()</span><br><span class="line"></span><br><span class="line">    ds.show</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，得到如下结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/97-2.png" width="30%" height="30%"><br><br><br><b>方式二：使用本地文件或HDFS数据</b><br>这里以本地JSON为例<br>在桌面创建个JSON文件StudentInfo.json，文件内容如下所示<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;id&quot;:1, &quot;name&quot;:&quot;Tom&quot;, &quot;age&quot;:21&#125;</span><br><span class="line">&#123;&quot;id&quot;:2, &quot;name&quot;:&quot;Bob&quot;, &quot;age&quot;:25&#125;</span><br><span class="line">&#123;&quot;id&quot;:3, &quot;name&quot;:&quot;Alice&quot;, &quot;age&quot;:18&#125;</span><br></pre></td></tr></table></figure></p><p>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">//定义case class</span><br><span class="line">case class StudentInfo(id: Long, name: String, age: Long)</span><br><span class="line"></span><br><span class="line">object CreateDatasetJSON &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;CreateDatasetJSON&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .getOrCreate</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    //通过JSON数据生成DataFrame</span><br><span class="line">    val df = spark.read.json(&quot;C://Users//asus//Desktop//StudentInfo.json&quot;)</span><br><span class="line"></span><br><span class="line">    //将DataFrame转换成Dataset</span><br><span class="line">    df.as[StudentInfo].show</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，得到如下结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/97-3.png" width="30%" height="30%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DataFrame的引入，可以让Spark更好的处理结构数据的计算，但其中一个主要的问题是：缺乏编译时类型安全。为了解决这个问题，Spark采用新的Dataset API（DataFrame API的类型扩展）。&lt;/p&gt;
&lt;p&gt;Dataset是分布式数据集合。Dataset是Spark 1.6中添加的一个接口，兼顾了RDD的优点（强类型化，使用功能强大的lambda函数）以及Spark SQL优化后的执行引擎的优点。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL之DataFrame的操作</title>
    <link href="https://www.ggstu.com/2018/09/24/Spark-SQL%E4%B9%8BDataFrame%E7%9A%84%E6%93%8D%E4%BD%9C/"/>
    <id>https://www.ggstu.com/2018/09/24/Spark-SQL之DataFrame的操作/</id>
    <published>2018-09-24T15:10:28.000Z</published>
    <updated>2018-09-28T00:50:23.898Z</updated>
    
    <content type="html"><![CDATA[<p>以下是对DataFrame进行操作的基本示例：</p><p>首先在本地桌面上创建个json文件，student.json，文件内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;id&quot;:1, &quot;name&quot;:&quot;Tom&quot;, &quot;age&quot;:21&#125;</span><br><span class="line">&#123;&quot;id&quot;:2, &quot;name&quot;:&quot;Bob&quot;, &quot;age&quot;:25&#125;</span><br><span class="line">&#123;&quot;id&quot;:3, &quot;name&quot;:&quot;Alice&quot;, &quot;age&quot;:18&#125;</span><br><span class="line">&#123;&quot;id&quot;:4, &quot;name&quot;:&quot;Jack&quot;, &quot;age&quot;:23&#125;</span><br><span class="line">&#123;&quot;id&quot;:5, &quot;name&quot;:&quot;Jerry&quot;, &quot;age&quot;:25&#125;</span><br></pre></td></tr></table></figure></p><a id="more"></a><p><br><br>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line"></span><br><span class="line">public class DataFrameOperations &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkSession spark = SparkSession</span><br><span class="line">.builder()</span><br><span class="line">.appName(&quot;DataFrameOperations&quot;)</span><br><span class="line">.master(&quot;local&quot;)</span><br><span class="line">.getOrCreate();</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df = spark.read().json(&quot;C://Users//asus//Desktop//student.json&quot;);</span><br><span class="line"></span><br><span class="line">//打印DataFrame中所有的数据</span><br><span class="line">df.show();</span><br><span class="line"></span><br><span class="line">//打印DataFrame的元数据</span><br><span class="line">df.printSchema();</span><br><span class="line"></span><br><span class="line">//打印DataFrame某列数据</span><br><span class="line">df.select(&quot;name&quot;).show();</span><br><span class="line"></span><br><span class="line">//打印某几列所有的数据，并对列进行计算</span><br><span class="line">df.select(df.col(&quot;name&quot;), df.col(&quot;age&quot;).plus(1)).show();</span><br><span class="line"></span><br><span class="line">//对某一列的值进行过滤</span><br><span class="line">df.filter(df.col(&quot;age&quot;).gt(21)).show();</span><br><span class="line"></span><br><span class="line">//对某一列分组统计数量</span><br><span class="line">df.groupBy(&quot;age&quot;).count().show();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后，得到如下结果：<br>打印DataFrame中所有的数据<br><img src="http://pd8lpasbc.bkt.clouddn.com/96-1.png" width="20%" height="20%"><br>打印DataFrame的元数据<br><img src="http://pd8lpasbc.bkt.clouddn.com/96-2.png" width="50%" height="50%"><br>打印DataFrame某列数据<br><img src="http://pd8lpasbc.bkt.clouddn.com/96-3.png" width="10%" height="10%"><br>打印某几列所有的数据，并对列进行计算<br><img src="http://pd8lpasbc.bkt.clouddn.com/96-4.png" width="25%" height="25%"><br>对某一列的值进行过滤<br><img src="http://pd8lpasbc.bkt.clouddn.com/96-5.png" width="22%" height="22%"><br>对某一列分组统计数量<br><img src="http://pd8lpasbc.bkt.clouddn.com/96-6.png" width="18%" height="18%"><br><br><br>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object DataFrameOperations &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">        .builder</span><br><span class="line">        .appName(&quot;DataFrameOperations&quot;)</span><br><span class="line">        .master(&quot;local&quot;)</span><br><span class="line">        .getOrCreate</span><br><span class="line">        </span><br><span class="line">    val df = spark.read.json(&quot;C://Users//asus//Desktop//student.json&quot;)</span><br><span class="line">    </span><br><span class="line">    df.show</span><br><span class="line">    </span><br><span class="line">    df.printSchema</span><br><span class="line">    </span><br><span class="line">    df.select(&quot;name&quot;).show</span><br><span class="line">    </span><br><span class="line">    df.select(df(&quot;name&quot;), df(&quot;age&quot;)+1).show</span><br><span class="line">    </span><br><span class="line">    df.filter(df(&quot;age&quot;)&gt;21).show</span><br><span class="line">    </span><br><span class="line">    df.groupBy(&quot;age&quot;).count.show</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，同样得到如上结果<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以下是对DataFrame进行操作的基本示例：&lt;/p&gt;
&lt;p&gt;首先在本地桌面上创建个json文件，student.json，文件内容如下&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;id&amp;quot;:1, &amp;quot;name&amp;quot;:&amp;quot;Tom&amp;quot;, &amp;quot;age&amp;quot;:21&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;id&amp;quot;:2, &amp;quot;name&amp;quot;:&amp;quot;Bob&amp;quot;, &amp;quot;age&amp;quot;:25&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;id&amp;quot;:3, &amp;quot;name&amp;quot;:&amp;quot;Alice&amp;quot;, &amp;quot;age&amp;quot;:18&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;id&amp;quot;:4, &amp;quot;name&amp;quot;:&amp;quot;Jack&amp;quot;, &amp;quot;age&amp;quot;:23&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;id&amp;quot;:5, &amp;quot;name&amp;quot;:&amp;quot;Jerry&amp;quot;, &amp;quot;age&amp;quot;:25&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL之创建DataFrames</title>
    <link href="https://www.ggstu.com/2018/09/24/Spark-SQL%E4%B9%8B%E5%88%9B%E5%BB%BADataFrames/"/>
    <id>https://www.ggstu.com/2018/09/24/Spark-SQL之创建DataFrames/</id>
    <published>2018-09-24T04:27:54.000Z</published>
    <updated>2018-09-24T15:03:13.771Z</updated>
    
    <content type="html"><![CDATA[<p>DataFrame是组织成指定列的DataSet。它在概念上等同于关系型数据库中的表，但在底层具有更丰富的优化。创建DataFrame的来源有多种，例如：结构化数据文件，Hive中的表，外部数据库或现有RDD。</p><p><font size="4"><b>（1）通过case class创建DataFrame</b></font><br><a id="more"></a><br>首先在桌面创建个雇员信息表emp.csv文件，作为输入源<br>其每一列分别表示员工号，员工姓名，职位，对应的老板号，入职日期，薪金，佣金，部门编号<br>其内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">7369,SMITH,CLERK,7902,1980/12/17,800,,20</span><br><span class="line">7499,ALLEN,SALESMAN,7698,1981/2/20,1600,300,30</span><br><span class="line">7521,WARD,SALESMAN,7698,1981/2/22,1250,500,30</span><br><span class="line">7566,JONES,MANAGER,7839,1981/4/2,2975,,20</span><br><span class="line">7654,MARTIN,SALESMAN,7698,1981/9/28,1250,1400,30</span><br><span class="line">7698,BLAKE,MANAGER,7839,1981/5/1,2850,,30</span><br><span class="line">7782,CLARK,MANAGER,7839,1981/6/9,2450,,10</span><br><span class="line">7788,SCOTT,ANALYST,7566,1987/4/19,3000,,20</span><br><span class="line">7839,KING,PRESIDENT,,1981/11/17,5000,,10</span><br><span class="line">7844,TURNER,SALESMAN,7698,1981/9/8,1500,0,30</span><br><span class="line">7876,ADAMS,CLERK,7788,1987/5/23,1100,,20</span><br><span class="line">7900,JAMES,CLERK,7698,1981/12/3,950,,30</span><br><span class="line">7902,FORD,ANALYST,7566,1981/12/3,3000,,20</span><br><span class="line">7934,MILLER,CLERK,7782,1982/1/23,1300,,10</span><br></pre></td></tr></table></figure></p><p>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import org.apache.spark.sql.SQLContext</span><br><span class="line"></span><br><span class="line">//定义case class（相当于表的结构：Schema）</span><br><span class="line">case class Emp(empno:Int,ename:String,job:String,mgr:String,hiredate:String,sal:Int,comm:String,deptno:Int)</span><br><span class="line"></span><br><span class="line">object ShowEmp &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;ShowEmp&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">    </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line">    </span><br><span class="line">    //将本地文件上的数据读入RDD</span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//emp.csv&quot;).map(_.split(&quot;,&quot;))</span><br><span class="line">    </span><br><span class="line">    //将RDD与case class关联</span><br><span class="line">    val allEmp = lines.map(x =&gt; Emp(x(0).toInt,x(1),x(2),x(3),x(4),x(5).toInt,x(6),x(7).toInt))</span><br><span class="line">    </span><br><span class="line">    //RDD到DataFrame的转换，需要手动导入一个隐式转换</span><br><span class="line">    import sqlContext.implicits._</span><br><span class="line">    </span><br><span class="line">    //将RDD转换成DataFrame</span><br><span class="line">    val allEmpDF = allEmp.toDF()</span><br><span class="line">    </span><br><span class="line">    allEmpDF.show</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，得到如下结果：<br><img src="http://pd8lpasbc.bkt.clouddn.com/94-1.png" width="80%" height="80%"><br><br></p><p><font size="4"><b>（2）自定义Schema创建DataFrame</b></font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.sql.types.StructField</span><br><span class="line">import org.apache.spark.sql.types.StructType</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import org.apache.spark.sql.SQLContext</span><br><span class="line"></span><br><span class="line">object CreateSchema &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;CreateSchema&quot;)</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line"></span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line"></span><br><span class="line">    import org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line">    //创建StructType，来定义Schema结构信息</span><br><span class="line">    val myschema = StructType(List(</span><br><span class="line">      StructField(&quot;empno&quot;, DataTypes.IntegerType),</span><br><span class="line">      StructField(&quot;ename&quot;, DataTypes.StringType),</span><br><span class="line">      StructField(&quot;job&quot;, DataTypes.StringType),</span><br><span class="line">      StructField(&quot;mgr&quot;, DataTypes.StringType),</span><br><span class="line">      StructField(&quot;hiredate&quot;, DataTypes.StringType),</span><br><span class="line">      StructField(&quot;sal&quot;, DataTypes.IntegerType),</span><br><span class="line">      StructField(&quot;comm&quot;, DataTypes.StringType),</span><br><span class="line">      StructField(&quot;deptno&quot;, DataTypes.IntegerType)))</span><br><span class="line"></span><br><span class="line">    //读入数据并且切分数据</span><br><span class="line">    val empcsvRDD = sc.textFile(&quot;C://Users//asus//Desktop//emp.csv&quot;).map(_.split(&quot;,&quot;))</span><br><span class="line"></span><br><span class="line">    //将RDD中的数据映射成Row</span><br><span class="line">    val rowRDD = empcsvRDD.map(line =&gt; Row(line(0).toInt, line(1), line(2), line(3), line(4), line(5).toInt, line(6), line(7).toInt))</span><br><span class="line"></span><br><span class="line">    //创建DataFrame</span><br><span class="line">    val df = sqlContext.createDataFrame(rowRDD, myschema)</span><br><span class="line"></span><br><span class="line">    df.show</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，和上面的结果相同<br><br></p><p><font size="4"><b>（3）使用SparkSession</b></font><br>Apache Spark2.0引入了SparkSession，其为用户提供了一个统一的切入点来使用Spark的各项功能，并且允许用户通过它调用DataFrame和Dataset的相关API来编写Spark程序。最重要的是，它减少了用户需要了解的一些概念，可以很容易地与Spark进行交互。</p><p>在2.0版本之前，与Spark交互之前必须先创建SparkConf和SparkContext。然而在Spark 2.0中，可以通过SparkSession来实现同样的功能，而不需要显示地创建SparkConf，SparkContext以及SQLContext，因为这些对象已经封装在SparkSession中了。</p><p>例如：使用csv文件来创建DataFrame<br>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object ReadCSV &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">        .builder()</span><br><span class="line">        .appName(&quot;SparkSessionTest&quot;)</span><br><span class="line">        .master(&quot;local&quot;)</span><br><span class="line">        .getOrCreate()</span><br><span class="line">                                   </span><br><span class="line">    //读入数据</span><br><span class="line">    val df = spark.read.csv(&quot;C://Users//asus//Desktop//emp.csv&quot;)</span><br><span class="line">    </span><br><span class="line">    df.show</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，得到如下结果：<br><img src="http://pd8lpasbc.bkt.clouddn.com/94-2.png" width="80%" height="80%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DataFrame是组织成指定列的DataSet。它在概念上等同于关系型数据库中的表，但在底层具有更丰富的优化。创建DataFrame的来源有多种，例如：结构化数据文件，Hive中的表，外部数据库或现有RDD。&lt;/p&gt;
&lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;（1）通过case class创建DataFrame&lt;/b&gt;&lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>2018南京森林音乐会</title>
    <link href="https://www.ggstu.com/2018/09/23/2018%E5%8D%97%E4%BA%AC%E6%A3%AE%E6%9E%97%E9%9F%B3%E4%B9%90%E4%BC%9A/"/>
    <id>https://www.ggstu.com/2018/09/23/2018南京森林音乐会/</id>
    <published>2018-09-23T15:55:36.000Z</published>
    <updated>2018-09-24T13:53:13.781Z</updated>
    
    <content type="html"><![CDATA[<p>2018年9月23日，听了李云迪的钢琴曲，只有音乐才最纯粹吧<br>大学生活剩下不到四分之一，希望也能够纯粹的开始，纯粹的结束<br>有些事情也许早已知道结局，但也希望其中的经历能够美好的保存在记忆里<br>秋夜深，让心也静下来吧……<br><a id="more"></a><br><img src="http://pd8lpasbc.bkt.clouddn.com/%E9%9F%B3%E4%B9%90%E4%BC%9A1.jpg" width="60%" height="60%"></p><p><img src="http://pd8lpasbc.bkt.clouddn.com/%E9%9F%B3%E4%B9%90%E4%BC%9A2.jpg" width="60%" height="60%"></p><p><img src="http://pd8lpasbc.bkt.clouddn.com/%E9%9F%B3%E4%B9%90%E4%BC%9A3.jpg" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2018年9月23日，听了李云迪的钢琴曲，只有音乐才最纯粹吧&lt;br&gt;大学生活剩下不到四分之一，希望也能够纯粹的开始，纯粹的结束&lt;br&gt;有些事情也许早已知道结局，但也希望其中的经历能够美好的保存在记忆里&lt;br&gt;秋夜深，让心也静下来吧……&lt;br&gt;
    
    </summary>
    
      <category term="随便说说" scheme="https://www.ggstu.com/categories/%E9%9A%8F%E4%BE%BF%E8%AF%B4%E8%AF%B4/"/>
    
    
      <category term="心情" scheme="https://www.ggstu.com/tags/%E5%BF%83%E6%83%85/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL简介</title>
    <link href="https://www.ggstu.com/2018/09/20/Spark-SQL%E7%AE%80%E4%BB%8B/"/>
    <id>https://www.ggstu.com/2018/09/20/Spark-SQL简介/</id>
    <published>2018-09-20T11:38:57.000Z</published>
    <updated>2018-09-28T01:16:09.546Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>概述</b></font><br>Spark SQL是Apache Spark用来处理结构化数据的一个模块。它起着分布式SQL查询引擎的作用。</p><p>其实最早使用的都是Hadoop自己的Hive查询引擎，Hive的诞生，主要是为了让那些不熟悉Java，无法深入进行MapReduce编程的数据分析师，能够使用他们熟悉的关系型数据库的SQL模型来操作HDFS上的数据。由于Hive底层基于MapReduce，而MapReduce这种计算模型执行效率比较慢，所以在Spark 1.0版本开始推出了Spark SQL。<br><a id="more"></a><br><img src="http://pd8lpasbc.bkt.clouddn.com/93-1.png" width="95%" height="95%"><br><br></p><p><font size="4"><b>Spark SQL的特点</b></font><br><b>容易整合</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/93-2.png" width="95%" height="95%"><br><b>统一的数据访问方式</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/93-3.png" width="95%" height="95%"><br><b>兼容Hive</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/93-4.png" width="95%" height="95%"><br><b>标准的数据连接</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/93-5.png" width="95%" height="95%"><br><br></p><p><font size="4"><b>Datasets和DataFrames</b></font><br>Dataset是分布式数据集合。Dataset是Spark 1.6中添加的一个接口，是DataFrame上更高一级的抽象。它提供了RDD的优点（强类型化，使用强大的lambda函数的能力）以及Spark SQL优化后的执行引擎的优点。一个Dataset可以从JVM对象构造，然后使用函数转换(map,flatMap,filter等)去操作。Dataset API支持的语言有Scala和Java。</p><p>DataFrame是组织成指定列的Dataset。它在概念上等同于关系型数据库中的表，但在底层具有更丰富的优化。创建DataFrame的来源有多种，例如：结构化数据文件，Hive中的表，外部数据库或现有RDD。DataFrame API支持的语言有Scala，Java，Python和R。DataFrame也可以叫Dataset[Row]。<br><img src="http://pd8lpasbc.bkt.clouddn.com/93-6.png" width="70%" height="70%"><br>从上图可以看出，DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取、执行计划的优化。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;概述&lt;/b&gt;&lt;/font&gt;&lt;br&gt;Spark SQL是Apache Spark用来处理结构化数据的一个模块。它起着分布式SQL查询引擎的作用。&lt;/p&gt;
&lt;p&gt;其实最早使用的都是Hadoop自己的Hive查询引擎，Hive的诞生，主要是为了让那些不熟悉Java，无法深入进行MapReduce编程的数据分析师，能够使用他们熟悉的关系型数据库的SQL模型来操作HDFS上的数据。由于Hive底层基于MapReduce，而MapReduce这种计算模型执行效率比较慢，所以在Spark 1.0版本开始推出了Spark SQL。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark Shuffle的原理分析</title>
    <link href="https://www.ggstu.com/2018/09/20/Spark-Shuffle%E7%9A%84%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/"/>
    <id>https://www.ggstu.com/2018/09/20/Spark-Shuffle的原理分析/</id>
    <published>2018-09-20T00:12:45.000Z</published>
    <updated>2018-09-20T11:12:44.430Z</updated>
    
    <content type="html"><![CDATA[<p>Shuffle，翻译成中文就是洗牌。因为具有某种共同特征的一类数据需要最终汇聚到一个计算节点上进行计算，所以需要Shuffle。</p><p><font size="4"><b>Spark Shuffle的历史演进：</b></font><br>在Spark 1.1以前的版本采用的是Hash Based Shuffle的实现方式。到1.1版本时参考Hadoop MapReduce的实现引入了Sort Based Shuffle。在1.4版本时引入了Tungsten-Sort Based Shuffle。在1.6中将Tungsten统一到了Sort Based Shuffle中。到2.0版本，Hash Based Shuffle被弃用，所有的Shuffle方式全部统一到Sort Based Shuffle来实现。<br><a id="more"></a><br>下图是Spark Shuffle的版本演进：<br><img src="http://pd8lpasbc.bkt.clouddn.com/92-5.png" width="100%" height="100%"><br><br></p><p><font size="4"><b>Hash Based Shuffle v1</b></font><br>早期的Hash Based Shuffle，每一个Map会根据Reduce的数量创建出相应的临时文件bucket，bucket的数量是M*R个，其中M是Map的个数，R是Reduce的个数。这样就会产生大量的小文件，对文件系统压力过大，而且不利于IO吞吐量。<br>例如：有3个Map Task，3个Reduce Task，就会产生9个小文件。<br><img src="http://pd8lpasbc.bkt.clouddn.com/92-7.png" width="100%" height="100%"><br><br></p><p><font size="4"><b>Hash Based Shuffle v2</b></font><br>后来引入了Consolidation机制，使用这种机制，在同一个core上先后运行多个Map Task的输出。<br>产生的文件数为：core的数量*Reduce的数量。</p><p>如果没有使用Consolidation机制，有4个Map Task，3个Reduce Task，这样只会产生12个小文件。<br>如果使用了Consolidation机制，那么这4个Map Task会分两批运行在2个core上，这样只会产生6个小文件。<br><img src="http://pd8lpasbc.bkt.clouddn.com/92-3.png" width="100%" height="100%"><br>这样做，表面上是减少了文件数，但是如果下游partition数量很大，输出大量的文件(cores*R)，性能会降低。而且大量的文件写入，使文件系统开始变为随机写，性能比顺序写要低。缓存空间占用比较大。<br><br></p><p><font size="4"><b>Sort Based Shuffle</b></font><br>针对上述Hash Based Shuffle的弊端，在spark 1.1引入了Sort Based Shuffle。它参考了Hadoop MapReduce中shuffle的实现，对记录进行排序来shuffle。如下图所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/92-8.png" width="90%" height="90%"><br>map端的任务会按照key对应的partition id进行排序，属于同一个partition的key不会进行排序。将排序好的数据写在同一个文件中，该文件中的记录是按照partition id排序一个一个分区的顺序排列。Map Task运行期间会顺序写每个partition的数据，并通过一个索引文件记录每个partition的大小和偏移量。<br>reduce端拉取数据做合并时不再采用HashMap，而是采用ExternalAppendOnlyMap，该数据结构在做合并时，如果内存不足，会写到磁盘上。<br><br></p><p><font size="4"><b>Tungsten-Sort Based Shuffle</b></font><br>从spark 1.4开始，spark开始了钨丝计划(Tungsten)，目的是优化内存和CPU的使用，进而提升spark的性能。从spark-1.6开始，将Tungsten-sort并入Sort Based Shuffle。<br>有关Tungsten的说明可以查看如下这篇文章：<br><a href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html" target="_blank">Project Tungsten: Bringing Apache Spark Closer to Bare Metal</a><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Shuffle，翻译成中文就是洗牌。因为具有某种共同特征的一类数据需要最终汇聚到一个计算节点上进行计算，所以需要Shuffle。&lt;/p&gt;
&lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;Spark Shuffle的历史演进：&lt;/b&gt;&lt;/font&gt;&lt;br&gt;在Spark 1.1以前的版本采用的是Hash Based Shuffle的实现方式。到1.1版本时参考Hadoop MapReduce的实现引入了Sort Based Shuffle。在1.4版本时引入了Tungsten-Sort Based Shuffle。在1.6中将Tungsten统一到了Sort Based Shuffle中。到2.0版本，Hash Based Shuffle被弃用，所有的Shuffle方式全部统一到Sort Based Shuffle来实现。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中RDD的检查点(Checkpoint)机制</title>
    <link href="https://www.ggstu.com/2018/09/17/Spark%E4%B8%ADRDD%E7%9A%84%E6%A3%80%E6%9F%A5%E7%82%B9-Checkpoint-%E6%9C%BA%E5%88%B6/"/>
    <id>https://www.ggstu.com/2018/09/17/Spark中RDD的检查点-Checkpoint-机制/</id>
    <published>2018-09-17T11:34:02.000Z</published>
    <updated>2018-09-17T12:40:38.678Z</updated>
    
    <content type="html"><![CDATA[<p>RDD的检查点机制的本质是将RDD写入磁盘作为检查点，是为了避免缓存丢失重新计算带来的开销，或者lineage(血统)过长而计算时间过长造成容错成本过高。这样就不如在中间阶段做检查点容错，如果之后有结点出现问题而丢失分区，那么可以从做检查点的RDD开始重做lineage，进而可以减少开销。</p><p>设置checkpoint的目录，可以是本地目录，也可以是HDFS上的目录。一般是在具有容错能力，高可靠的文件系统上（比如HDFS、S3等）设置一个检查点路径，用于保存检查点数据。<br><a id="more"></a><br>例如：设置检查点目录为本地目录<br>我在桌面创建了个名为checkpoint的目录作为检查点目录</p><p>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">public class CheckpointTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;CheckpointTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">//设置检查点目录</span><br><span class="line">sc.setCheckpointDir(&quot;C://Users//asus//Desktop//checkpoint&quot;);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">//设置rdd的检查点</span><br><span class="line">numbers.checkpoint();</span><br><span class="line"></span><br><span class="line">//一旦触发action操作，就会在检查点目录下生成检查点</span><br><span class="line">numbers.count();</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行完成后，在检查点目录checkpoint下会生成一个文件夹，文件夹下的内容如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/91-1.png" width="100%" height="100%"><br><br><br>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object CheckpointTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;CheckpointTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    sc.setCheckpointDir(&quot;C://Users//asus//Desktop//checkpoint&quot;)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    numbers.checkpoint</span><br><span class="line">    </span><br><span class="line">    numbers.count</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，同样得到如上结果</p><p>若要将检查点目录设置成HDFS上的目录，将上面代码的本地目录改成HDFS的目录即可。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RDD的检查点机制的本质是将RDD写入磁盘作为检查点，是为了避免缓存丢失重新计算带来的开销，或者lineage(血统)过长而计算时间过长造成容错成本过高。这样就不如在中间阶段做检查点容错，如果之后有结点出现问题而丢失分区，那么可以从做检查点的RDD开始重做lineage，进而可以减少开销。&lt;/p&gt;
&lt;p&gt;设置checkpoint的目录，可以是本地目录，也可以是HDFS上的目录。一般是在具有容错能力，高可靠的文件系统上（比如HDFS、S3等）设置一个检查点路径，用于保存检查点数据。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中RDD的依赖关系以及stage划分</title>
    <link href="https://www.ggstu.com/2018/09/17/Spark%E4%B8%ADRDD%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB%E4%BB%A5%E5%8F%8Astage%E5%88%92%E5%88%86/"/>
    <id>https://www.ggstu.com/2018/09/17/Spark中RDD的依赖关系以及stage划分/</id>
    <published>2018-09-17T07:33:28.000Z</published>
    <updated>2018-09-17T08:33:42.313Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>RDD的依赖关系</b></font><br>RDD和它依赖的父RDD的关系有两种不同的类型，即窄依赖(Narrow Dependency)和宽依赖(Wide Dependency)<br><a id="more"></a><br><img src="http://pd8lpasbc.bkt.clouddn.com/90-1.png" width="80%" height="80%"><br><b>窄依赖：</b>指的是子RDD中的每一个partition仅仅依赖于父RDD中的一个partition。例如map、filter、union等都会产生窄依赖。<br>如果子RDD执行的时候某个分区执行失败（数据丢失），只需要重新执行父RDD对应的分区即可进行数据恢复。<br><b>宽依赖：</b>指的是子RDD中的每一个partition都依赖所有父RDD的所有partition。例如groupByKey、reduceByKey、sortByKey等都会产生宽依赖。<br>如果子RDD执行的时候某个分区执行失败（数据丢失），需要将父RDD的所有分区全部进行执行才可以进行数据恢复。<br><br></p><p><font size="4"><b>Spark任务中的stage</b></font><br>DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的stage。</p><p>对于窄依赖，partition的转换处理在stage中完成计算。<br>对于宽依赖，由于有shuffle的存在，只能在父RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分stage的依据。<br><img src="http://pd8lpasbc.bkt.clouddn.com/90-2.png" width="70%" height="70%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;RDD的依赖关系&lt;/b&gt;&lt;/font&gt;&lt;br&gt;RDD和它依赖的父RDD的关系有两种不同的类型，即窄依赖(Narrow Dependency)和宽依赖(Wide Dependency)&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中实现分组取Top N（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/17/Spark%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%88%86%E7%BB%84%E5%8F%96Top-N%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/17/Spark中实现分组取Top-N（Scala代码）/</id>
    <published>2018-09-17T05:28:28.000Z</published>
    <updated>2018-09-17T06:33:50.314Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/16/Spark%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%88%86%E7%BB%84%E5%8F%96Top-N%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中实现分组取Top N（Java代码）</a>这篇文章中用Java代码找出了每个省份对应的海拔最高的三个站点，实现了分组取Top N的功能，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object GroupTop3 &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;GroupTop3&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//meteorological_station.csv&quot;)</span><br><span class="line">    </span><br><span class="line">    val pairs = lines.map(line =&gt; (line.split(&quot;,&quot;)(0), line.split(&quot;,&quot;)(2)+&quot;,&quot;+line.split(&quot;,&quot;)(5)))</span><br><span class="line">  </span><br><span class="line">    val groupedPairs = pairs.groupByKey()</span><br><span class="line">    </span><br><span class="line">    val groupTop3 = groupedPairs.map&#123;t =&gt;</span><br><span class="line">      val top3 = new Array[String](3)</span><br><span class="line">      val top3Height = new Array[Double](3)</span><br><span class="line">      val province = t._1</span><br><span class="line">      val cityAndHeights = t._2.iterator</span><br><span class="line">      while(cityAndHeights.hasNext)&#123;</span><br><span class="line">        val cityAndHeight = cityAndHeights.next()</span><br><span class="line">        val city = cityAndHeight.split(&quot;,&quot;)(0)</span><br><span class="line">        val height = cityAndHeight.split(&quot;,&quot;)(1).toDouble</span><br><span class="line">        var flag = true</span><br><span class="line">        for(i &lt;- 0 until 3 if flag)&#123;</span><br><span class="line">          if(top3(i) == null)&#123;</span><br><span class="line">            top3Height(i) = height</span><br><span class="line">            top3(i) = city + &quot; &quot; + height</span><br><span class="line">            flag = false</span><br><span class="line">          &#125;else if(height &gt; top3Height(i))&#123;</span><br><span class="line">            for(j &lt;- (i+1 to 2).reverse)&#123;</span><br><span class="line">              top3Height(j) = top3Height(j-1)</span><br><span class="line">              top3(j) = top3(j-1)</span><br><span class="line">            &#125;</span><br><span class="line">            top3Height(i) = height</span><br><span class="line">            top3(i) = city + &quot; &quot; + height</span><br><span class="line">            flag = false</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      (province, top3)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    groupTop3.foreach&#123;t =&gt; </span><br><span class="line">      println(&quot;Province: &quot; + t._1)</span><br><span class="line">      for(i &lt;- 0 to 2)&#123;</span><br><span class="line">        print(t._2.array(i))</span><br><span class="line">        if(i != 2)&#123;</span><br><span class="line">          print(&quot;,&quot;)</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">          println</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      println(&quot;**********************************&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，找出了每个省份对应的海报最高的三个站点，实现了分组取Top n的功能<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">Province: 新疆</span><br><span class="line">吐尔尕特 3504.4,塔什库尔干 3090.1,巴音布鲁克 2458.0</span><br><span class="line">**********************************</span><br><span class="line">Province: 辽宁</span><br><span class="line">建平县 462.2,建昌 367.0,新宾 328.4</span><br><span class="line">**********************************</span><br><span class="line">Province: 内蒙古</span><br><span class="line">希拉穆仁气侯站 1602.3,阿拉善左旗 1561.4,阿右旗 1510.1</span><br><span class="line">**********************************</span><br><span class="line">Province: 浙江</span><br><span class="line">淳安 171.4,云和 163.0,石浦 128.4</span><br><span class="line">**********************************</span><br><span class="line">Province: 青海</span><br><span class="line">班玛 13530.0,河南 13500.0,刚察 13301.5</span><br><span class="line">**********************************</span><br><span class="line">Province: 陕西</span><br><span class="line">华山 2064.9,太白 1543.6,定边 1360.3</span><br><span class="line">**********************************</span><br><span class="line">Province: 福建</span><br><span class="line">九仙山 1653.5,屏南 869.5,寿宁 815.9</span><br><span class="line">**********************************</span><br><span class="line">Province: 山西</span><br><span class="line">五台山 2208.3,五寨 1401.0,右玉 1345.8</span><br><span class="line">**********************************</span><br><span class="line">Province: 甘肃</span><br><span class="line">玛曲 3471.4,乌鞘岭 3045.1,合作 2910.0</span><br><span class="line">**********************************</span><br><span class="line">Province: 河北</span><br><span class="line">张北 1393.3,蔚县 909.5,围场 892.7</span><br><span class="line">**********************************</span><br><span class="line">Province: 宁夏</span><br><span class="line">六盘山 2841.2,西吉 1916.5,海原 1854.2</span><br><span class="line">**********************************</span><br><span class="line">Province: 云南</span><br><span class="line">香格里拉 3341.5,德钦 3319.0,丽江 2380.9</span><br><span class="line">**********************************</span><br><span class="line">Province: 广西</span><br><span class="line">那坡 794.1,靖西 739.9,凤山 509.4</span><br><span class="line">**********************************</span><br><span class="line">Province: 安徽</span><br><span class="line">黄山 1840.4,屯溪 142.7,祁门 142.0</span><br><span class="line">**********************************</span><br><span class="line">Province: 湖北</span><br><span class="line">利川 1074.1,五峰 619.9,建始 609.2</span><br><span class="line">**********************************</span><br><span class="line">Province: 重庆</span><br><span class="line">酉阳 826.5,黔江 786.9,綦江 474.7</span><br><span class="line">**********************************</span><br><span class="line">Province: 广东</span><br><span class="line">连平 215.2,新丰 199.3,龙川 179.6</span><br><span class="line">**********************************</span><br><span class="line">Province: 江西</span><br><span class="line">庐山 1164.5,井冈山 843.0,寻乌 303.9</span><br><span class="line">**********************************</span><br><span class="line">Province: 山东</span><br><span class="line">泰山 1533.7,沂源 305.1,济南 170.3</span><br><span class="line">**********************************</span><br><span class="line">Province: 吉林</span><br><span class="line">长白 775.0,东岗 774.2,二道 721.4</span><br><span class="line">**********************************</span><br><span class="line">Province: 江苏</span><br><span class="line">徐州 41.2,盱眙 40.8,南京 35.2</span><br><span class="line">**********************************</span><br><span class="line">Province: 北京</span><br><span class="line">延庆 487.9,密云 71.8,北京 31.3</span><br><span class="line">**********************************</span><br><span class="line">Province: 海南</span><br><span class="line">三亚 419.4,琼中 250.9,儋州 169.0</span><br><span class="line">**********************************</span><br><span class="line">Province: 贵州</span><br><span class="line">威宁 2237.5,水城 1815.9,盘县 1800.0</span><br><span class="line">**********************************</span><br><span class="line">Province: 河南</span><br><span class="line">嵩山 1178.4,栾川 742.4,卢氏 658.5</span><br><span class="line">**********************************</span><br><span class="line">Province: 上海</span><br><span class="line">宝山 5.5,null,null</span><br><span class="line">**********************************</span><br><span class="line">Province: 四川</span><br><span class="line">石渠 14200.0,理塘 3948.9,色达 3893.9</span><br><span class="line">**********************************</span><br><span class="line">Province: 天津</span><br><span class="line">宝坻 5.1,塘沽 4.8,天津 3.5</span><br><span class="line">**********************************</span><br><span class="line">Province: 黑龙江</span><br><span class="line">绥芬河 567.8,呼中 514.5,新林 501.5</span><br><span class="line">**********************************</span><br><span class="line">Province: 西藏</span><br><span class="line">安多 14800.0,班戈 14700.0,申扎 14672.0</span><br><span class="line">**********************************</span><br><span class="line">Province: 湖南</span><br><span class="line">南岳 1265.9,桂东 835.9,城步 477.7</span><br><span class="line">**********************************</span><br></pre></td></tr></table></figure></p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/16/Spark%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%88%86%E7%BB%84%E5%8F%96Top-N%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中实现分组取Top N（Java代码）&lt;/a&gt;这篇文章中用Java代码找出了每个省份对应的海拔最高的三个站点，实现了分组取Top N的功能，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中实现分组取Top N（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/16/Spark%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%88%86%E7%BB%84%E5%8F%96Top-N%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/16/Spark中实现分组取Top-N（Java代码）/</id>
    <published>2018-09-16T10:40:00.000Z</published>
    <updated>2018-09-17T01:32:32.024Z</updated>
    
    <content type="html"><![CDATA[<p>分组取Top N，就是按照某一列进行分组，然后分别取每组中的前N个数据。<br>我这里下载了一个数据集，数据集文件名为meteorological_station.csv，是2016年的中国气象站站点数据。一共有六列，分别对应省份、区站号、站点、维度、经度、观测场海拔高度。<br><a id="more"></a><br>测试数据如下所示，一共有800多条数据，这里列举其中一部分：<br>安徽,58436,宁国,30.37,118.59,87.3<br>安徽,58437,黄山,30.08,118.09,1840.4<br>安徽,58520,祁门,29.51,117.43,142<br>安徽,58531,屯溪,29.43,118.17,142.7<br>北京,54406,延庆,40.27,115.58,487.9<br>北京,54416,密云,40.23,116.52,71.8<br>北京,54511,北京,39.48,116.28,31.3<br>福建,58725,邵武,27.2,117.28,218<br>福建,58730,武夷山,27.46,118.02,222.1<br>福建,58731,浦城,27.55,118.32,276.9<br>福建,58734,建阳,27.2,118.07,196.9<br>福建,58737,建瓯,27.03,118.19,154.9</p><p><b>需求：</b>按照省份进行分组，找出每个省份对应的海拔最高的三个站点。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class GroupTop3 &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;GroupTop3&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">//读取meteorological_station.csv，创建RDD</span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users/asus//Desktop//meteorological_station.csv&quot;);</span><br><span class="line"></span><br><span class="line">//将省份作为key，站点+海拔作为value，映射成key-value的形式</span><br><span class="line">JavaPairRDD&lt;String, String&gt; pairs = lines.mapToPair(new PairFunction&lt;String, String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, String&gt; call(String line) throws Exception &#123;</span><br><span class="line">String[] lineSplited = line.split(&quot;,&quot;);</span><br><span class="line">return new Tuple2&lt;String, String&gt;(lineSplited[0], lineSplited[2]+&quot;,&quot;+lineSplited[5]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">//按照省份进行分组</span><br><span class="line">JavaPairRDD&lt;String, Iterable&lt;String&gt;&gt; groupedPairs = pairs.groupByKey();</span><br><span class="line"></span><br><span class="line">//对海拔高度进行判断，取出最高的三个高度</span><br><span class="line">JavaPairRDD&lt;String, Iterable&lt;String&gt;&gt; groupTop3 = groupedPairs.mapToPair(new PairFunction&lt;Tuple2&lt;String,Iterable&lt;String&gt;&gt;, String, Iterable&lt;String&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, Iterable&lt;String&gt;&gt; call(Tuple2&lt;String, Iterable&lt;String&gt;&gt; t) throws Exception &#123;</span><br><span class="line">String[] top3 = new String[3];</span><br><span class="line">Double[] top3Height = new Double[3];</span><br><span class="line">String province = t._1;</span><br><span class="line">Iterator&lt;String&gt; cityAndHeights = t._2.iterator();</span><br><span class="line">while(cityAndHeights.hasNext()) &#123;</span><br><span class="line">String cityAndHeight = cityAndHeights.next();</span><br><span class="line">String city = cityAndHeight.split(&quot;,&quot;)[0];</span><br><span class="line">Double height = Double.valueOf(cityAndHeight.split(&quot;,&quot;)[1]);</span><br><span class="line">for(int i=0; i&lt;3; i++) &#123;</span><br><span class="line">if(top3[i] == null) &#123;</span><br><span class="line">top3Height[i] = height;</span><br><span class="line">top3[i] = city + &quot; &quot; + height;</span><br><span class="line">break;</span><br><span class="line">&#125;else if(height &gt; top3Height[i]) &#123;</span><br><span class="line">for(int j=2; j&gt;i; j--) &#123;</span><br><span class="line">top3Height[j] = top3Height[j-1];</span><br><span class="line">top3[j] = top3[j-1];</span><br><span class="line">&#125;</span><br><span class="line">top3Height[i] = height;</span><br><span class="line">top3[i] = city + &quot; &quot; + height;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">return new Tuple2&lt;String, Iterable&lt;String&gt;&gt;(province, Arrays.asList(top3));</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">groupTop3.foreach(new VoidFunction&lt;Tuple2&lt;String,Iterable&lt;String&gt;&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;String, Iterable&lt;String&gt;&gt; t) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;Province: &quot; + t._1);</span><br><span class="line">System.out.println(t._2);</span><br><span class="line">System.out.println(&quot;*************************************&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后，得到如下结果，即找出每个省份对应的海拔最高的三个站点，实现分组取Top N的功能。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">Province: 新疆</span><br><span class="line">[吐尔尕特 3504.4, 塔什库尔干 3090.1, 巴音布鲁克 2458.0]</span><br><span class="line">*************************************</span><br><span class="line">Province: 辽宁</span><br><span class="line">[建平县 462.2, 建昌 367.0, 新宾 328.4]</span><br><span class="line">*************************************</span><br><span class="line">Province: 内蒙古</span><br><span class="line">[希拉穆仁气侯站 1602.3, 阿拉善左旗 1561.4, 阿右旗 1510.1]</span><br><span class="line">*************************************</span><br><span class="line">Province: 浙江</span><br><span class="line">[淳安 171.4, 云和 163.0, 石浦 128.4]</span><br><span class="line">*************************************</span><br><span class="line">Province: 青海</span><br><span class="line">[班玛 13530.0, 河南 13500.0, 刚察 13301.5]</span><br><span class="line">*************************************</span><br><span class="line">Province: 陕西</span><br><span class="line">[华山 2064.9, 太白 1543.6, 定边 1360.3]</span><br><span class="line">*************************************</span><br><span class="line">Province: 福建</span><br><span class="line">[九仙山 1653.5, 屏南 869.5, 寿宁 815.9]</span><br><span class="line">*************************************</span><br><span class="line">Province: 山西</span><br><span class="line">[五台山 2208.3, 五寨 1401.0, 右玉 1345.8]</span><br><span class="line">*************************************</span><br><span class="line">Province: 甘肃</span><br><span class="line">[玛曲 3471.4, 乌鞘岭 3045.1, 合作 2910.0]</span><br><span class="line">*************************************</span><br><span class="line">Province: 河北</span><br><span class="line">[张北 1393.3, 蔚县 909.5, 围场 892.7]</span><br><span class="line">*************************************</span><br><span class="line">Province: 宁夏</span><br><span class="line">[六盘山 2841.2, 西吉 1916.5, 海原 1854.2]</span><br><span class="line">*************************************</span><br><span class="line">Province: 云南</span><br><span class="line">[香格里拉 3341.5, 德钦 3319.0, 丽江 2380.9]</span><br><span class="line">*************************************</span><br><span class="line">Province: 广西</span><br><span class="line">[那坡 794.1, 靖西 739.9, 凤山 509.4]</span><br><span class="line">*************************************</span><br><span class="line">Province: 安徽</span><br><span class="line">[黄山 1840.4, 屯溪 142.7, 祁门 142.0]</span><br><span class="line">*************************************</span><br><span class="line">Province: 湖北</span><br><span class="line">[利川 1074.1, 五峰 619.9, 建始 609.2]</span><br><span class="line">*************************************</span><br><span class="line">Province: 重庆</span><br><span class="line">[酉阳 826.5, 黔江 786.9, 綦江 474.7]</span><br><span class="line">*************************************</span><br><span class="line">Province: 广东</span><br><span class="line">[连平 215.2, 新丰 199.3, 龙川 179.6]</span><br><span class="line">*************************************</span><br><span class="line">Province: 江西</span><br><span class="line">[庐山 1164.5, 井冈山 843.0, 寻乌 303.9]</span><br><span class="line">*************************************</span><br><span class="line">Province: 山东</span><br><span class="line">[泰山 1533.7, 沂源 305.1, 济南 170.3]</span><br><span class="line">*************************************</span><br><span class="line">Province: 吉林</span><br><span class="line">[长白 775.0, 东岗 774.2, 二道 721.4]</span><br><span class="line">*************************************</span><br><span class="line">Province: 江苏</span><br><span class="line">[徐州 41.2, 盱眙 40.8, 南京 35.2]</span><br><span class="line">*************************************</span><br><span class="line">Province: 北京</span><br><span class="line">[延庆 487.9, 密云 71.8, 北京 31.3]</span><br><span class="line">*************************************</span><br><span class="line">Province: 海南</span><br><span class="line">[三亚 419.4, 琼中 250.9, 儋州 169.0]</span><br><span class="line">*************************************</span><br><span class="line">Province: 贵州</span><br><span class="line">[威宁 2237.5, 水城 1815.9, 盘县 1800.0]</span><br><span class="line">*************************************</span><br><span class="line">Province: 河南</span><br><span class="line">[嵩山 1178.4, 栾川 742.4, 卢氏 658.5]</span><br><span class="line">*************************************</span><br><span class="line">Province: 上海</span><br><span class="line">[宝山 5.5, null, null]</span><br><span class="line">*************************************</span><br><span class="line">Province: 四川</span><br><span class="line">[石渠 14200.0, 理塘 3948.9, 色达 3893.9]</span><br><span class="line">*************************************</span><br><span class="line">Province: 天津</span><br><span class="line">[宝坻 5.1, 塘沽 4.8, 天津 3.5]</span><br><span class="line">*************************************</span><br><span class="line">Province: 黑龙江</span><br><span class="line">[绥芬河 567.8, 呼中 514.5, 新林 501.5]</span><br><span class="line">*************************************</span><br><span class="line">Province: 西藏</span><br><span class="line">[安多 14800.0, 班戈 14700.0, 申扎 14672.0]</span><br><span class="line">*************************************</span><br><span class="line">Province: 湖南</span><br><span class="line">[南岳 1265.9, 桂东 835.9, 城步 477.7]</span><br><span class="line">*************************************</span><br></pre></td></tr></table></figure></p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分组取Top N，就是按照某一列进行分组，然后分别取每组中的前N个数据。&lt;br&gt;我这里下载了一个数据集，数据集文件名为meteorological_station.csv，是2016年的中国气象站站点数据。一共有六列，分别对应省份、区站号、站点、维度、经度、观测场海拔高度。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>使用Scala实现Spark的二次排序</title>
    <link href="https://www.ggstu.com/2018/09/16/%E4%BD%BF%E7%94%A8Scala%E5%AE%9E%E7%8E%B0Spark%E7%9A%84%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F/"/>
    <id>https://www.ggstu.com/2018/09/16/使用Scala实现Spark的二次排序/</id>
    <published>2018-09-16T08:51:03.000Z</published>
    <updated>2018-09-16T09:15:38.263Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/16/%E4%BD%BF%E7%94%A8Java%E5%AE%9E%E7%8E%B0Spark%E7%9A%84%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F/" target="_blank">使用Java实现Spark的二次排序</a>这篇文章中用Java代码实现了用户对电影评分数据的二次排序，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br>首先实现自定义的key，实现Ordered接口和Serializable接口，并在key中实现自己对多个列的排序算法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class SecondarySortKey(val first:Int, val second:Int) extends Ordered[SecondarySortKey] with Serializable &#123;</span><br><span class="line">  def compare(that:SecondarySortKey):Int = &#123;</span><br><span class="line">    if(this.first-that.first != 0)&#123;</span><br><span class="line">      this.first - that.first</span><br><span class="line">    &#125;else&#123;</span><br><span class="line">      this.second - that.second</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>主方法：将包含文本的RDD映射成key为自定义key，value为文本的JavaPairRDD。接着使用sortByKey算子使用自定义的key进行排序。最后再次映射，删除掉自定义的key，只保留文本行。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object SecondarySort &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;SecondarySort&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//movies.csv&quot;)</span><br><span class="line">    </span><br><span class="line">    val pairs = lines.map&#123;line =&gt;</span><br><span class="line">      (new SecondarySortKey(line.split(&quot;,&quot;)(0).toInt, line.split(&quot;,&quot;)(1).toInt), line)  </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    val sortedPairs = pairs.sortByKey()</span><br><span class="line">    </span><br><span class="line">    val sortedLines = sortedPairs.map(pair =&gt; pair._2)</span><br><span class="line">    </span><br><span class="line">    sortedLines.foreach(line =&gt; println(line))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，截取一部分数据，如下所示，可以看到首先按照第一个字段用户id进行排序，若用户id相同，按照第二个字段电影id进行排序。即，使用Scala实现了用户对电影评分数据的二次排序。<br><img src="http://pd8lpasbc.bkt.clouddn.com/87-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/16/%E4%BD%BF%E7%94%A8Java%E5%AE%9E%E7%8E%B0Spark%E7%9A%84%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F/&quot; target=&quot;_blank&quot;&gt;使用Java实现Spark的二次排序&lt;/a&gt;这篇文章中用Java代码实现了用户对电影评分数据的二次排序，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>使用Java实现Spark的二次排序</title>
    <link href="https://www.ggstu.com/2018/09/16/%E4%BD%BF%E7%94%A8Java%E5%AE%9E%E7%8E%B0Spark%E7%9A%84%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F/"/>
    <id>https://www.ggstu.com/2018/09/16/使用Java实现Spark的二次排序/</id>
    <published>2018-09-16T03:33:52.000Z</published>
    <updated>2018-09-16T07:48:48.476Z</updated>
    
    <content type="html"><![CDATA[<p>二次排序就是首先按照第一个字段进行排序，然后再对第一个字段相同的行按照第二个字段排序，并且不能破坏第一次排序的结果。<br>我这里下载了一个数据集，数据集文件名为movies.csv，是用户对电影的评分数据。一共有四列，分别是user_id(用户id)、item_id(电影id)、rating(评分)、timestamp(评分时间)。<br><a id="more"></a><br>测试数据如下所示，一共有10万条数据，这里只列举其中一部分：<br>186,302,3,891717742<br>22,377,1,878887116<br>244,51,2,880606923<br>166,346,1,886397596<br>298,474,4,884182806<br>115,265,2,881171488<br>253,465,5,891628467<br>305,451,3,886324817<br>6,86,3,883603013<br>62,257,2,879372434<br>286,1014,5,879781125<br>200,222,5,876042340<br>210,40,3,891035994<br>224,29,3,888104457<br>303,785,3,879485318<br>122,387,5,879270459<br>194,274,2,879539794<br>291,1042,4,874834944</p><p><b>需求：</b>使用二次排序，按照用户id升序排序，若用户id相同，即一个用户对多个电影评分，则按照电影id升序排序。</p><p>首先实现自定义的key，实现Ordered接口和Serializable接口，并在key中实现自己对多个列的排序算法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">import java.io.Serializable;</span><br><span class="line"></span><br><span class="line">import scala.math.Ordered;</span><br><span class="line"></span><br><span class="line">public class SecondarySortKey implements Ordered&lt;SecondarySortKey&gt;,Serializable &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line">private int first;</span><br><span class="line">private int second;</span><br><span class="line"></span><br><span class="line">public SecondarySortKey(int first, int second) &#123;</span><br><span class="line">super();</span><br><span class="line">this.first = first;</span><br><span class="line">this.second = second;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public int getFirst() &#123;</span><br><span class="line">return first;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void setFirst(int first) &#123;</span><br><span class="line">this.first = first;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public int getSecond() &#123;</span><br><span class="line">return second;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void setSecond(int second) &#123;</span><br><span class="line">this.second = second;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public boolean $greater(SecondarySortKey other) &#123;</span><br><span class="line">if(this.first &gt; other.getFirst()) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else if(this.first==other.getFirst() &amp;&amp; this.second&gt;other.getSecond()) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return false;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public boolean $greater$eq(SecondarySortKey other) &#123;</span><br><span class="line">if(this.$greater(other)) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else if(this.first==other.getFirst() &amp;&amp; this.second==other.getSecond()) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return false;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public boolean $less(SecondarySortKey other) &#123;</span><br><span class="line">if(this.first &lt; other.getFirst()) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else if(this.first==other.getFirst() &amp;&amp; this.second&lt;other.getSecond()) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return false;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public boolean $less$eq(SecondarySortKey other) &#123;</span><br><span class="line">if(this.$less(other)) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else if(this.first==other.getFirst() &amp;&amp; this.second==other.getSecond()) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return false;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public int compare(SecondarySortKey other) &#123;</span><br><span class="line">if(this.first-other.getFirst() != 0) &#123;</span><br><span class="line">return this.first - other.getFirst();</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return this.second - other.getSecond();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public int compareTo(SecondarySortKey other) &#123;</span><br><span class="line">if(this.first-other.getFirst() != 0) &#123;</span><br><span class="line">return this.first - other.getFirst();</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return this.second - other.getSecond();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>主方法：将包含文本的RDD映射成key为自定义key，value为文本的JavaPairRDD。接着使用sortByKey算子使用自定义的key进行排序。最后再次映射，删除掉自定义的key，只保留文本行。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class SecondarySort &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;SecondarySort&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus//Desktop//movies.csv&quot;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;SecondarySortKey, String&gt; pairs = lines.mapToPair(new PairFunction&lt;String, SecondarySortKey, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;SecondarySortKey, String&gt; call(String line) throws Exception &#123;</span><br><span class="line">String[] lineSplited = line.split(&quot;,&quot;);</span><br><span class="line">SecondarySortKey key = new SecondarySortKey(Integer.valueOf(lineSplited[0]), Integer.valueOf(lineSplited[1]));</span><br><span class="line">return new Tuple2&lt;SecondarySortKey, String&gt;(key, line);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;SecondarySortKey, String&gt; sortedPairs = pairs.sortByKey();</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; sortedLines = sortedPairs.map(new Function&lt;Tuple2&lt;SecondarySortKey,String&gt;, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public String call(Tuple2&lt;SecondarySortKey, String&gt; line) throws Exception &#123;</span><br><span class="line">return line._2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sortedLines.foreach(new VoidFunction&lt;String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(String line) throws Exception &#123;</span><br><span class="line">System.out.println(line);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>由于数据过多，我就截取一部分输出数据，如下所示，可以看到首先按照第一个字段用户id进行排序，若用户id相同，按照第二个字段电影id进行排序<br><img src="http://pd8lpasbc.bkt.clouddn.com/86-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;二次排序就是首先按照第一个字段进行排序，然后再对第一个字段相同的行按照第二个字段排序，并且不能破坏第一次排序的结果。&lt;br&gt;我这里下载了一个数据集，数据集文件名为movies.csv，是用户对电影的评分数据。一共有四列，分别是user_id(用户id)、item_id(电影id)、rating(评分)、timestamp(评分时间)。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark基于排序机制的wordcount程序（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/16/Spark%E5%9F%BA%E4%BA%8E%E6%8E%92%E5%BA%8F%E6%9C%BA%E5%88%B6%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/16/Spark基于排序机制的wordcount程序（Scala代码）/</id>
    <published>2018-09-16T03:00:33.000Z</published>
    <updated>2018-09-16T03:22:14.657Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/16/Spark%E5%9F%BA%E4%BA%8E%E6%8E%92%E5%BA%8F%E6%9C%BA%E5%88%B6%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark基于排序机制的wordcount程序（Java代码）</a>这篇文章中用Java代码实现了wordcount的排序，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object SortWordCount &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;SortWordCount&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    val pairs = words.map(word =&gt; (word,1))</span><br><span class="line">    </span><br><span class="line">    val wordCount = pairs.reduceByKey(_ + _)</span><br><span class="line">    </span><br><span class="line">    val countWord = wordCount.map(t =&gt; (t._2,t._1))</span><br><span class="line">    </span><br><span class="line">    val sortCountWord = countWord.sortByKey(false)</span><br><span class="line">    </span><br><span class="line">    val sortWordCount = sortCountWord.map(t =&gt; (t._2,t._1))</span><br><span class="line">    </span><br><span class="line">    sortWordCount.foreach(t =&gt; println(t._1 + &quot;: &quot; + t._2))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，实现了基于排序机制的wordcount程序<br><img src="http://pd8lpasbc.bkt.clouddn.com/85-1.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/16/Spark%E5%9F%BA%E4%BA%8E%E6%8E%92%E5%BA%8F%E6%9C%BA%E5%88%B6%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark基于排序机制的wordcount程序（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了wordcount的排序，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark基于排序机制的wordcount程序（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/16/Spark%E5%9F%BA%E4%BA%8E%E6%8E%92%E5%BA%8F%E6%9C%BA%E5%88%B6%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/16/Spark基于排序机制的wordcount程序（Java代码）/</id>
    <published>2018-09-16T00:41:39.000Z</published>
    <updated>2018-09-16T01:30:25.551Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Java%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/" target="_blank">使用Java开发Spark的wordcount程序</a>这篇文章中开发了Java版本的wordcount程序，得到了如下所示的结果。可以发现，并没有按照单词的总数进行排序，而是乱序的。<br><img src="http://pd8lpasbc.bkt.clouddn.com/84-1.png" width="70%" height="70%"><br><a id="more"></a><br>下面就来使用Java开发基于排序机制的Spark的wordcount程序<br>按照单词出现总数降序排序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class SortWordCount &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;SortWordCount&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus/Desktop//test.txt&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; pairs = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;String, Integer&gt;(word, 1);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; wordCount = pairs.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer v1, Integer v2) throws Exception &#123;</span><br><span class="line">return v1 + v2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; countWord = wordCount.mapToPair(new PairFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;Integer, String&gt; call(Tuple2&lt;String, Integer&gt; t) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;Integer, String&gt;(t._2, t._1);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; sortCountWord = countWord.sortByKey(false);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; sortWordCount = sortCountWord.mapToPair(new PairFunction&lt;Tuple2&lt;Integer,String&gt;, String, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Integer, String&gt; t) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;String, Integer&gt;(t._2, t._1);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sortWordCount.foreach(new VoidFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;String, Integer&gt; t) throws Exception &#123;</span><br><span class="line">System.out.println(t._1 + &quot;: &quot; + t._2);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后，得到如下结果，即按照单词出现总数降序排序<br><img src="http://pd8lpasbc.bkt.clouddn.com/84-2.png" width="60%" height="60%"></p><p>这个程序的关键步骤就是在使用reduceByKey统计出每个单词和其出现的总次数后，进行了key-value的转换，此时的key是单词总数，value是单词。然后使用sortByKey(false)按照key降序排序。排序完成后，再次进行key-value转换，转换为原来的key是单词，value是单词总数的情况。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Java%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/&quot; target=&quot;_blank&quot;&gt;使用Java开发Spark的wordcount程序&lt;/a&gt;这篇文章中开发了Java版本的wordcount程序，得到了如下所示的结果。可以发现，并没有按照单词的总数进行排序，而是乱序的。&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/84-1.png&quot; width=&quot;70%&quot; height=&quot;70%&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中的共享变量（Shared Variables）</title>
    <link href="https://www.ggstu.com/2018/09/15/Spark%E4%B8%AD%E7%9A%84%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F%EF%BC%88Shared-Variables%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/15/Spark中的共享变量（Shared-Variables）/</id>
    <published>2018-09-15T06:57:41.000Z</published>
    <updated>2018-09-15T14:56:30.315Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>共享变量工作原理：</b></font><br>默认情况下，如果在一个算子的函数中使用到了某个外部的变量，那么这个变量的值会被拷贝到每个task中。此时每个task操作的是拷贝的那份变量副本，而不是原变量。因此，这种方式无法共享此变量。</p><p>Spark为此提供了两种共享变量，一种是Broadcast Variables(广播变量)，另一种是Accumulators(累加器)。Broadcast Variables会将使用到的变量，仅仅为每个节点(机器)拷贝一份，因此可以优化性能，减少网络传输以及内存消耗。Accumulators则可以让多个task共同操作一份变量，主要可以进行累加操作。<br><a id="more"></a><br><b>Broadcast Variables：</b><br>广播变量可以在每台机器上只保留一个变量，并且这个变量是只读的，而不会为每个task都拷贝一份副本。因此其可以减少变量到各个节点的网络传输消耗，以及在各个节点上的内存消耗。此外，spark内部也使用了高效的广播算法来分发广播变量，以减少通信成本。</p><p>可以通过调用SparkContext的broadcast()方法，来针对某个变量创建广播变量。然后在算子的函数内，使用到广播变量时，每个节点只会拷贝一份副本。每个节点可以使用广播变量的value()方法获取值。</p><p>例如：将集合中的每个元素乘以10，即把10当成广播变量<br>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line">import org.apache.spark.broadcast.Broadcast;</span><br><span class="line"></span><br><span class="line">public class BroadcastTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;BroadcastTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">int factor = 10;</span><br><span class="line"></span><br><span class="line">Broadcast&lt;Integer&gt; factorBroadcast  = sc.broadcast(factor);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; result = numbers.map(new Function&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer num) throws Exception &#123;</span><br><span class="line">int factor = factorBroadcast.value();</span><br><span class="line">return num * factor;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">result.foreach(new VoidFunction&lt;Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Integer res) throws Exception &#123;</span><br><span class="line">System.out.println(res);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE后，运行结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/83-1.png" width="60%" height="60%"></p><p>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object BroadcastTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;BroadcastTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"> </span><br><span class="line">    val factor = 10</span><br><span class="line">    </span><br><span class="line">    val factorBroadcast = sc.broadcast(factor)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    val result = numbers.map(num =&gt; num*factorBroadcast.value)</span><br><span class="line"> </span><br><span class="line">    result.foreach(res =&gt; println(res))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE上运行，同样得到如上结果。<br><br><br><b>Accumulators:</b><br>Spark提供的Accumulator，主要用于多个节点对一个变量进行共享性的操作。Accumulator只提供了累加的功能，可以多个task对一个变量并行操作。<br>task只能对Accumulator进行累加操作，不能读取它的值。只有Driver程序可以读取Accumulator的值。</p><p>例如：累加1到5<br>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line">import org.apache.spark.util.LongAccumulator;</span><br><span class="line"></span><br><span class="line">public class AccumulatorTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;AccumulatorTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">LongAccumulator sum  = sc.sc().longAccumulator();</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">numbers.foreach(new VoidFunction&lt;Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Integer num) throws Exception &#123;</span><br><span class="line">sum.add(num);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;1+2+3+4+5 = &quot; + sum.value());</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE后，运行结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/83-2.png" width="60%" height="60%"></p><p>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object AccumulatorTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;AccumulatorTest&quot;)</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line">      </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val sum = sc.longAccumulator(&quot;My Accumulator&quot;)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    numbers.foreach(num =&gt; sum.add(num))</span><br><span class="line">    </span><br><span class="line">    println(&quot;1+2+3+4+5 = &quot; + sum.value)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE上运行，同样得到如上结果。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;共享变量工作原理：&lt;/b&gt;&lt;/font&gt;&lt;br&gt;默认情况下，如果在一个算子的函数中使用到了某个外部的变量，那么这个变量的值会被拷贝到每个task中。此时每个task操作的是拷贝的那份变量副本，而不是原变量。因此，这种方式无法共享此变量。&lt;/p&gt;
&lt;p&gt;Spark为此提供了两种共享变量，一种是Broadcast Variables(广播变量)，另一种是Accumulators(累加器)。Broadcast Variables会将使用到的变量，仅仅为每个节点(机器)拷贝一份，因此可以优化性能，减少网络传输以及内存消耗。Accumulators则可以让多个task共同操作一份变量，主要可以进行累加操作。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中RDD的持久化</title>
    <link href="https://www.ggstu.com/2018/09/14/Spark%E4%B8%ADRDD%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96/"/>
    <id>https://www.ggstu.com/2018/09/14/Spark中RDD的持久化/</id>
    <published>2018-09-14T06:20:41.000Z</published>
    <updated>2018-09-14T15:38:02.773Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>RDD持久化原理</b></font><br>Spark非常重要的一个功能特性就是可以将RDD持久化到内存中。当对RDD执行持久化操作时，每个节点都会将自己操作的RDD的partition持久化到内存中，并且在之后对该RDD的反复使用中，可以直接使用内存缓存的partition。这样的话，若对于一个RDD反复执行多个操作，就只要对RDD计算一次即可，后面直接使用该RDD，而不需要多次计算该RDD。<br><a id="more"></a><br>要持久化一个RDD，只要调用其cache()或者persist()方法即可。在该RDD第一次被计算出来时，就会直接缓存每个节点。而且Spark的持久化机制还是自动容错的，如果持久化的RDD的任何partition丢失了，那么Spark会自动通过其源RDD，使用transformation操作重新计算该partition。</p><p>通过查看Spark源码可以发现，cache()的底层其实调用的persist()的无参版本，亦即调用persist(StorageLevel.MEMORY_ONLY)将数据持久化到内存中。<br><img src="http://pd8lpasbc.bkt.clouddn.com/82-1.png" width="100%" height="100%"></p><p>Spark自己会在shuffle操作时，进行数据持久化，比如写入磁盘，主要是为了在结点失败时，避免需要重新计算整个过程。</p><p>cache()或persist()的使用必须在transformation或者textFile等创建了一个RDD后直接调用cache()或persist()，单独用一条语句执行cache()或persist()方法会报错。如果需要从内存中清除缓存，那么可以使用unpersist()方法。<br><br><br>例如：使用持久化统计文件中单词总数<br>我在桌面创建了个文件，名为test.txt，文件中的内容是以空格隔开的单词<br><b>Java代码：</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"></span><br><span class="line">public class PersistTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;PersistTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;).cache();</span><br><span class="line"></span><br><span class="line">long beginTime = System.currentTimeMillis();</span><br><span class="line">JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">long count = words.count();</span><br><span class="line">System.out.println(&quot;Count: &quot; + count);</span><br><span class="line">long endTime = System.currentTimeMillis();</span><br><span class="line">System.out.println(&quot;Cost: &quot; + (endTime-beginTime) + &quot; milliseconds.&quot;);</span><br><span class="line"></span><br><span class="line">beginTime = System.currentTimeMillis();</span><br><span class="line">words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">count = words.count();</span><br><span class="line">System.out.println(&quot;Count: &quot; + count);</span><br><span class="line">endTime = System.currentTimeMillis();</span><br><span class="line">System.out.println(&quot;Cost: &quot; + (endTime-beginTime) + &quot; milliseconds.&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后得到如下结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/82-3.png" width="60%" height="60%"><br><img src="http://pd8lpasbc.bkt.clouddn.com/82-4.png" width="60%" height="60%"><br>如果没有使用持久化，即没有在代码中添加.cache()，结果如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/82-5.png" width="60%" height="60%"><br><img src="http://pd8lpasbc.bkt.clouddn.com/82-6.png" width="60%" height="60%"><br>可以发现使用持久化后，第二次执行同样操作，其执行的时间缩短了。<br><br><br><b>Scala代码：</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object PersistTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;PersistTest&quot;)</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line">      </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;).cache()</span><br><span class="line">    </span><br><span class="line">    var beginTime = System.currentTimeMillis()</span><br><span class="line">    var words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    var count = words.count()</span><br><span class="line">    println(&quot;Count: &quot; + count)</span><br><span class="line">    var endTime = System.currentTimeMillis()</span><br><span class="line">    println(&quot;Cost: &quot; + (endTime-beginTime) + &quot; milliseconds.&quot;)</span><br><span class="line">    </span><br><span class="line">    beginTime = System.currentTimeMillis()</span><br><span class="line">    words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    count = words.count()</span><br><span class="line">    println(&quot;Count: &quot; + count)</span><br><span class="line">    endTime = System.currentTimeMillis()</span><br><span class="line">    println(&quot;Cost: &quot; + (endTime-beginTime) + &quot; milliseconds.&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，同样得到如上结果<br><br></p><p><font size="4"><b>RDD持久化策略</b></font><br>RDD持久化可以手动选择不同的策略。比如可以将RDD持久化到内存中、持久化到磁盘上、使用序列化的方式持久化、多持久化的数据进行多路复用。只要在调用persist()时传入对应的StorageLevel即可。</p><p><b>StorageLevel（持久化级别）：</b><br>DISK_ONLY：使用非序列化Java对象的方式持久化，完全存储到磁盘上。</p><p>MEMORY_ONLY：以非序列化的Java对象的方式持久化在JVM内存中。如果内存无法完全存储RDD所有的partition，那些没有持久化的partition就会在下一次需要使用它的时候，重新被计算。</p><p>MEMORY_ONLY_SER：同MEMORY_ONLY，但是会使用Java序列化方式，将Java对象序列化后进行持久化。可以减少内存开销，但是需要进行反序列化，因此会加大CPU开销。</p><p>MEMORY_AND_DISK：同上，但是当某些partition无法存储在内存中时，会持久化到磁盘中。下次需要使用这些partition时，需要从磁盘上读取。</p><p>MEMORY_AND_DISK_SER：同MEMORY_AND_DISK。但是使用序列化方式持久化Java对象。</p><p>DISK_ONLY_2、MEMORY_ONLY_2、MEMORY_ONLY_SER_2、<br>MEMORY_AND_DISK_2、MEMORY_AND_DISK_SER_2：如果是尾部加了2的持久化级别，表示会将持久化数据复用一份保存到其它节点，从而在数据丢失时不需要再次计算，只需要使用备份数据即可。</p><p>查看Spark的文档，可以看到这些持久化级别，如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/82-2.png" width="100%" height="100%"><br><br></p><p><font size="4"><b>如何选择RDD持久化策略？</b></font><br>Spark提供的多种持久化级别，主要是为了在CPU和内存消耗之间进行取舍。下面是一些通用的持久化级别的选择建议：<br>1、优先使用MEMORY_ONLY，如果可以缓存所有数据，就使用这种策略。因为内存速度最快，而且没有序列化，不需要消耗CPU进行反序列化操作。<br>2、如果MEMORY_ONLY策略，无法存储的下所有数据的话，那么使用MEMORY_ONLY_SER，将数据进行序列化进行存储，但是要消耗CPU进行反序列化。<br>3、如果需要进行快速的失败恢复，那么就选择带后缀为_2的策略，进行数据的备份，这样在失败时，就不需要重新计算了。<br>4、尽量不使用DISK相关的策略，因为从磁盘读取数据的时间可能要比重新计算一次的时间长。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;RDD持久化原理&lt;/b&gt;&lt;/font&gt;&lt;br&gt;Spark非常重要的一个功能特性就是可以将RDD持久化到内存中。当对RDD执行持久化操作时，每个节点都会将自己操作的RDD的partition持久化到内存中，并且在之后对该RDD的反复使用中，可以直接使用内存缓存的partition。这样的话，若对于一个RDD反复执行多个操作，就只要对RDD计算一次即可，后面直接使用该RDD，而不需要多次计算该RDD。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
</feed>
