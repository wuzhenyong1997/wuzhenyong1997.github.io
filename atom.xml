<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>GGSTU</title>
  
  <subtitle>Good Good Study</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.ggstu.com/"/>
  <updated>2018-09-08T01:53:38.685Z</updated>
  <id>https://www.ggstu.com/</id>
  
  <author>
    <name>Wu Zhenyong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark中transformation的groupByKey算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/08/Spark%E4%B8%ADtransformation%E7%9A%84groupByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/08/Spark中transformation的groupByKey算子的使用（Scala代码）/</id>
    <published>2018-09-07T16:39:17.000Z</published>
    <updated>2018-09-08T01:53:38.685Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/07/Spark%E4%B8%ADtransformation%E7%9A%84groupByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的groupByKey算子的使用（Java代码）</a>这篇文章中用Java代码实现了groupByKey算子对每个学生的成绩进行分组，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object GroupByKeyTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;GroupByKeyTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val scoreList = Array(</span><br><span class="line">        Tuple2(&quot;Tom&quot;, 85),</span><br><span class="line">        Tuple2(&quot;Bob&quot;, 75),</span><br><span class="line">        Tuple2(&quot;Alice&quot;, 92),</span><br><span class="line">        Tuple2(&quot;Tom&quot;, 70),</span><br><span class="line">        Tuple2(&quot;Bob&quot;, 95),</span><br><span class="line">        Tuple2(&quot;Alice&quot;, 80)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    val scores = sc.parallelize(scoreList)</span><br><span class="line">    </span><br><span class="line">    val groupedScores = scores.groupByKey()</span><br><span class="line">    </span><br><span class="line">    groupedScores.foreach(score =&gt; &#123;</span><br><span class="line">      println(&quot;Name: &quot; + score._1)</span><br><span class="line">      score._2.foreach(subjectScore =&gt; println(subjectScore))</span><br><span class="line">      println(&quot;******************&quot;)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用groupByKey算子对每个学生的成绩进行分组<br><img src="http://pd8lpasbc.bkt.clouddn.com/58-1.png" width="60%" height="60%"></p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/07/Spark%E4%B8%ADtransformation%E7%9A%84groupByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的groupByKey算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了groupByKey算子对每个学生的成绩进行分组，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的groupByKey算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/07/Spark%E4%B8%ADtransformation%E7%9A%84groupByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/07/Spark中transformation的groupByKey算子的使用（Java代码）/</id>
    <published>2018-09-07T15:38:42.000Z</published>
    <updated>2018-09-07T16:27:10.629Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对groupByKey算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/57-1.png" width="100%" height="100%"><br>在一个(K,V)的RDD上调用，返回一个(K, Iterable<v>)的RDD<br><a id="more"></a><br>例如：对每个学生的成绩进行分组<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class GroupByKeyTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;GroupByKeyTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;String,Integer&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Tom&quot;, 85),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Bob&quot;, 75),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Alice&quot;, 92),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Tom&quot;, 70),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Bob&quot;, 95),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Alice&quot;, 80)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Iterable&lt;Integer&gt;&gt; groupedScores = scores.groupByKey();</span><br><span class="line"></span><br><span class="line">groupedScores.foreach(new VoidFunction&lt;Tuple2&lt;String,Iterable&lt;Integer&gt;&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;String, Iterable&lt;Integer&gt;&gt; score) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;Name: &quot; + score._1);</span><br><span class="line">Iterator&lt;Integer&gt; ite = score._2.iterator();</span><br><span class="line">while(ite.hasNext()) &#123;</span><br><span class="line">System.out.println(ite.next());</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(&quot;******************************&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></v></p><p>在本地IDE执行后，得到如下结果，即，使用groupByKey操作对每个学生的成绩进行分组<br><img src="http://pd8lpasbc.bkt.clouddn.com/57-2.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对groupByKey算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/57-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在一个(K,V)的RDD上调用，返回一个(K, Iterable&lt;v&gt;)的RDD&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的flatMap算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/07/Spark%E4%B8%ADtransformation%E7%9A%84flatMap%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/07/Spark中transformation的flatMap算子的使用（Scala代码）/</id>
    <published>2018-09-07T00:16:22.000Z</published>
    <updated>2018-09-07T00:36:58.399Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84flatMap%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的flatMap算子的使用（Java代码）</a>这篇文章中用Java代码实现了flatMap算子将文本行拆分成一个个单词，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br>同样，在桌面创建了个文件test.txt，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object FlatMapTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;FlatMapTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    words.foreach(word =&gt; println(word))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用flatMap算子将文本行拆分成一个个单词<br><img src="http://pd8lpasbc.bkt.clouddn.com/56-1.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84flatMap%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的flatMap算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了flatMap算子将文本行拆分成一个个单词，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的flatMap算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84flatMap%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/06/Spark中transformation的flatMap算子的使用（Java代码）/</id>
    <published>2018-09-06T14:46:23.000Z</published>
    <updated>2018-09-06T15:06:06.941Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对flatMap算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/55-1.png" width="100%" height="100%"><br>flatMap类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）<br><a id="more"></a><br>例如：将文本行拆分成一个个单词<br>我在桌面创建了个文件test.txt，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">public class FlatMapTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;FlatMapTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">    JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line">    </span><br><span class="line">    JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;);</span><br><span class="line">    </span><br><span class="line">    JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">    </span><br><span class="line">    words.foreach(new VoidFunction&lt;String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(String word) throws Exception &#123;</span><br><span class="line">System.out.println(word);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">    </span><br><span class="line">    sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用flatMap操作将文本行拆分成一个个单词<br><img src="http://pd8lpasbc.bkt.clouddn.com/55-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对flatMap算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/55-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;flatMap类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的filter算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84filter%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/06/Spark中transformation的filter算子的使用（Scala代码）/</id>
    <published>2018-09-06T11:09:57.000Z</published>
    <updated>2018-09-06T11:27:09.626Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84filter%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的filter算子的使用（Java代码）</a>这篇文章中用Java代码实现了filter算子过滤出集合中的所有偶数，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object FilterTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;FilterTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numbers = Array(1,2,3,4,5,6,7,8,9,10)</span><br><span class="line">    </span><br><span class="line">    val numberRDD = sc.parallelize(numbers)</span><br><span class="line">    </span><br><span class="line">    val resultRDD = numberRDD.filter(num =&gt; num%2 == 0)</span><br><span class="line">    </span><br><span class="line">    resultRDD.foreach(res =&gt; println(res))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用filter算子过滤出集合中的所有偶数<br><img src="http://pd8lpasbc.bkt.clouddn.com/54-1.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84filter%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的filter算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了filter算子过滤出集合中的所有偶数，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的filter算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84filter%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/06/Spark中transformation的filter算子的使用（Java代码）/</id>
    <published>2018-09-06T02:54:46.000Z</published>
    <updated>2018-09-06T03:21:00.283Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对filter算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/53-1.png" width="100%" height="100%"><br>意思就是，返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成<br><a id="more"></a><br>例如：过滤出集合中的所有偶数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">public class FilterTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;FilterTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numbers = Arrays.asList(1,2,3,4,5,6,7,8,9,10);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numberRDD = sc.parallelize(numbers);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; resultRDD = numberRDD.filter(new Function&lt;Integer, Boolean&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Boolean call(Integer num) throws Exception &#123;</span><br><span class="line">return num%2 == 0;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">resultRDD.foreach(new VoidFunction&lt;Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Integer res) throws Exception &#123;</span><br><span class="line">System.out.println(res);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用filter操作过滤出一个集合中的所有偶数<br><img src="http://pd8lpasbc.bkt.clouddn.com/53-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对filter算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/53-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;意思就是，返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的map算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84map%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/06/Spark中transformation的map算子的使用（Scala代码）/</id>
    <published>2018-09-05T23:56:05.000Z</published>
    <updated>2018-09-06T00:20:08.177Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/05/Spark%E4%B8%ADtransformation%E7%9A%84map%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的map算子的使用（Java代码）</a>这篇文章中用Java代码实现了map算子对集合中的每个元素开平方，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object MapTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;MapTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numbers = Array(1,2,3,4,5,6,7,8,9,10)</span><br><span class="line">    </span><br><span class="line">    val numberRDD = sc.parallelize(numbers)</span><br><span class="line">    </span><br><span class="line">    val resultRDD = numberRDD.map(num =&gt; num*num)</span><br><span class="line">    </span><br><span class="line">    resultRDD.foreach(res =&gt; println(res))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用map操作将一个集合中的每个元素开平方<br><img src="http://pd8lpasbc.bkt.clouddn.com/52-1.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/05/Spark%E4%B8%ADtransformation%E7%9A%84map%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的map算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了map算子对集合中的每个元素开平方，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的map算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/05/Spark%E4%B8%ADtransformation%E7%9A%84map%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/05/Spark中transformation的map算子的使用（Java代码）/</id>
    <published>2018-09-05T15:14:42.000Z</published>
    <updated>2018-09-06T00:20:06.748Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对map算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/51-1.png" width="100%" height="100%"><br>意思就是，返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成。<br><a id="more"></a><br>例如：将集合中的每个元素开平方<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">public class MapTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;MapTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numbers = Arrays.asList(1,2,3,4,5,6,7,8,9,10);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numberRDD = sc.parallelize(numbers);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; resultRDD = numberRDD.map(new Function&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer num) throws Exception &#123;</span><br><span class="line">return num * num;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">resultRDD.foreach(new VoidFunction&lt;Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Integer res) throws Exception &#123;</span><br><span class="line">System.out.println(res);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用map操作将一个集合中的每个元素开平方<br><img src="http://pd8lpasbc.bkt.clouddn.com/51-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对map算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/51-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;意思就是，返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中RDD的transformation操作和action操作介绍</title>
    <link href="https://www.ggstu.com/2018/09/05/Spark%E4%B8%ADRDD%E7%9A%84transformation%E6%93%8D%E4%BD%9C%E5%92%8Caction%E6%93%8D%E4%BD%9C%E4%BB%8B%E7%BB%8D/"/>
    <id>https://www.ggstu.com/2018/09/05/Spark中RDD的transformation操作和action操作介绍/</id>
    <published>2018-09-05T07:48:35.000Z</published>
    <updated>2018-09-05T12:10:25.878Z</updated>
    
    <content type="html"><![CDATA[<p><b>Spark支持两种RDD操作，transformation和action</b></p><p><b>transformation</b>：针对已有的RDD创建一个新的RDD。比如map就是一种transformation操作，它用于将已有RDD的每个元素传入一个自定义的函数，并获取一个新的元素，然后将所有的新元素组成一个新的RDD。</p><p><b>action</b>：主要是对RDD进行最后的操作，比如遍历、reduce、保存到文件等，并将结果返回给Driver程序。</p><a id="more"></a><font size="4"><b>特点</b></font><br>transformation的特点就是<b>延迟加载</b>特性。如果一个spark应用中只定义了transformation操作，那么即使执行了该应用，这些操作也不会执行。也就是说，transformation不会触发spark程序的执行，它们只是记录了对RDD所做的操作，但是不会自发的执行。只有当transformation之后接着执行了一个action操作，那么所有的transformation才会执行。Spark通过这种延迟加载特性，避免产生过多的中间结果，从而让Spark更加高效的运行。<br>action操作执行，会触发一个spark job的运行，从而触发这个action之前所有的transformation的执行。<br><br><br>例如以之前写的Scala版本的Spark的wordcount程序为例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object WordCountLocal &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;WordCountLocal&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    val pairs = words.map(word =&gt; (word,1))</span><br><span class="line">    </span><br><span class="line">    val wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line">    </span><br><span class="line">    wordCounts.foreach(wordCount =&gt; println(wordCount._1 + &quot;: &quot; + wordCount._2))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><br>使用textFile创建一个RDD后，其后的flatMap、map、reduceByKey都是transformation操作，不会触发程序的执行，只有执行了最后的foreach操作，前面的transformation操作才开始执行。<br><br><br><font size="4"><b>transformation介绍</b></font><table><thead><tr><th style="text-align:left">transformation</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:left">map(func)</td><td style="text-align:left">返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</td></tr><tr><td style="text-align:left">filter(func)</td><td style="text-align:left">返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成</td></tr><tr><td style="text-align:left">flatMap(func)</td><td style="text-align:left">类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）</td></tr><tr><td style="text-align:left">mapPartitions(func)</td><td style="text-align:left">类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator<t> =&gt; Iterator<u></u></t></td></tr><tr><td style="text-align:left">mapPartitionsWithIndex(func)</td><td style="text-align:left">类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Iterator<t>) =&gt; Iterator<u></u></t></td></tr><tr><td style="text-align:left">sample(withReplacement,fraction,seed)</td><td style="text-align:left">根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子</td></tr><tr><td style="text-align:left">union(otherDataset)</td><td style="text-align:left">对源RDD和参数RDD求并集后返回一个新的RDD</td></tr><tr><td style="text-align:left">intersection(otherDataset)</td><td style="text-align:left">对源RDD和参数RDD求交集后返回一个新的RDD</td></tr><tr><td style="text-align:left">distinct([numPartitions]))</td><td style="text-align:left">对源RDD进行去重后返回一个新的RDD</td></tr><tr><td style="text-align:left">groupByKey([numPartitions])</td><td style="text-align:left">在一个(K,V)的RDD上调用，返回一个(K, Iterable&lt;V>)的RDD</td></tr><tr><td style="text-align:left">reduceByKey(func, [numPartitions])</td><td style="text-align:left">在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置</td></tr><tr><td style="text-align:left">sortByKey([ascending], [numPartitions])</td><td style="text-align:left">在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</td></tr><tr><td style="text-align:left">join(otherDataset, [numPartitions])</td><td style="text-align:left">在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD</td></tr><tr><td style="text-align:left">cogroup(otherDataset, [numPartitions])</td><td style="text-align:left">在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable&lt;V>,Iterable&lt;W>))类型的RDD</td></tr></tbody></table><p><br></p><font size="4"><b>action介绍</b></font><table><thead><tr><th style="text-align:left">action</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:left">reduce(func)</td><td style="text-align:left">通过func函数聚集RDD中的所有元素，这个函数必须是可交换且可并联的</td></tr><tr><td style="text-align:left">collect(func)</td><td style="text-align:left">在驱动程序中，以数组的形式返回数据集的所有元素</td></tr><tr><td style="text-align:left">count()</td><td style="text-align:left">返回RDD的元素个数</td></tr><tr><td style="text-align:left">first()</td><td style="text-align:left">返回RDD的第一个元素（类似于take(1)）</td></tr><tr><td style="text-align:left">take(n)</td><td style="text-align:left">返回一个由数据集的前n个元素组成的数组</td></tr><tr><td style="text-align:left">takeSample(withReplacement, num, [seed])</td><td style="text-align:left">返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</td></tr><tr><td style="text-align:left">takeOrdered(n, [ordering])</td><td style="text-align:left">返回使用自然顺序或自定义比较器的RDD的前n个元素</td></tr><tr><td style="text-align:left">saveAsTextFile(path)</td><td style="text-align:left">将数据集的元素以text file的形式保存到HDFS文件系统或者其它支持的文件系统，对于每个元素，Spark会调用toString方法，将它转换为文件中的文本</td></tr><tr><td style="text-align:left">saveAsSequenceFile(path)</td><td style="text-align:left">将数据集中的元素以Hadoop SequenceFile的格式保存到指定的目录下，可以是HDFS或者其它Hadoop支持的文件系统</td></tr><tr><td style="text-align:left">countByKey()</td><td style="text-align:left">针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数</td></tr><tr><td style="text-align:left">foreach(func)</td><td style="text-align:left">在数据集的每个元素上，运行func函数进行更新</td></tr></tbody></table><p><br><br>之后的文章会分别使用Java代码和Scala代码来示范常用的transformation算子和action算子的使用。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;b&gt;Spark支持两种RDD操作，transformation和action&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;transformation&lt;/b&gt;：针对已有的RDD创建一个新的RDD。比如map就是一种transformation操作，它用于将已有RDD的每个元素传入一个自定义的函数，并获取一个新的元素，然后将所有的新元素组成一个新的RDD。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;action&lt;/b&gt;：主要是对RDD进行最后的操作，比如遍历、reduce、保存到文件等，并将结果返回给Driver程序。&lt;/p&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark弹性分布式数据集RDD介绍</title>
    <link href="https://www.ggstu.com/2018/09/05/Spark%E5%BC%B9%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86RDD%E4%BB%8B%E7%BB%8D/"/>
    <id>https://www.ggstu.com/2018/09/05/Spark弹性分布式数据集RDD介绍/</id>
    <published>2018-09-05T02:47:09.000Z</published>
    <updated>2018-09-05T05:40:44.753Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>什么是RDD?</b></font><br>RDD(Resilient Distributed Dataset)，叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表了一个不可变、可分区、其中的元素可并行计算的集合。RDD既有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作机缓存在内存中，后续的查询操作能够重用工作机，因此能够大大提高查询速度。<br><a id="more"></a><br><br></p><p><font size="4"><b>RDD的属性</b></font><br>在Spark源码中有RDD属性的描述<br><img src="http://pd8lpasbc.bkt.clouddn.com/49-1.png" width="100%" height="100%"><br><b>1、一组分片（Partition）</b>，即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p><p><b>2、一个计算每个分区的函数</b>。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的，compute函数会对迭代器进行复合，不需要保存每次计算的结果。</p><p><b>3、RDD之间的依赖关系</b>。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p><p><b>4、一个Partitioner（即RDD的分片函数）</b>。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于key-value的RDD，才会有Partitioner，非key-value的RDD的Partitioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p><p><b>5、一个列表</b>，存储每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。<br><br></p><p><font size="4"><b>RDD的基本原理</b></font><br>创建一个RDD：val rdd = sc.parallelize(Array(1,2,3,4,5,6,7,8),3)<br>这个RDD创建了3个分区，一个分区运行在一个Worker节点上，但是实际上一个Worker上可以运行多个分区。<br>下图的红框标注的就是一个RDD。<br><img src="http://pd8lpasbc.bkt.clouddn.com/49-2.png" width="100%" height="100%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;什么是RDD?&lt;/b&gt;&lt;/font&gt;&lt;br&gt;RDD(Resilient Distributed Dataset)，叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表了一个不可变、可分区、其中的元素可并行计算的集合。RDD既有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作机缓存在内存中，后续的查询操作能够重用工作机，因此能够大大提高查询速度。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark创建RDD的几种方式（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/03/Spark%E5%88%9B%E5%BB%BARDD%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/03/Spark创建RDD的几种方式（Scala代码）/</id>
    <published>2018-09-03T11:01:43.000Z</published>
    <updated>2018-09-03T12:54:21.008Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/03/Spark%E5%88%9B%E5%BB%BARDD%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark创建RDD的几种方式（Java代码）</a>这篇文章中介绍了Spark创建RDD的三种方式的Java代码。<br>下面介绍Scala版本的创建RDD，直接上代码。</p><p><font size="4"><b>并行化集合创建RDD</b></font><br>例如：累加1到10<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object ParallelizeCollection &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">          .setAppName(&quot;ParallelizeCollection&quot;)</span><br><span class="line">          .setMaster(&quot;local&quot;)</span><br><span class="line">          </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numbers = Array(1,2,3,4,5,6,7,8,9,10)</span><br><span class="line">    </span><br><span class="line">    val numberRDD = sc.parallelize(numbers)</span><br><span class="line">    </span><br><span class="line">    val sum = numberRDD.reduce(_ + _)</span><br><span class="line">    </span><br><span class="line">    println(&quot;1+2+3+......+10 = &quot; + sum)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行程序，在控制台上看到如下所示的结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/48-1.png" width="60%" height="60%"><br><br></p><p><font size="4"><b>使用本地文件创建RDD</b></font><br>例如：累加1到10<br>我在本地桌面上创建了个文本文件，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 2 3 4 5 6 7 8 9 10</span><br></pre></td></tr></table></figure></p><p>代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object LocalFile &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">          .setAppName(&quot;LocalFile&quot;)</span><br><span class="line">          .setMaster(&quot;local&quot;)</span><br><span class="line">          </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val nums = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    val numsToInt = nums.map(num =&gt; num.toInt)</span><br><span class="line">    </span><br><span class="line">    val sum = numsToInt.reduce(_ + _)</span><br><span class="line">    </span><br><span class="line">    println(&quot;1+2+3+4+......+10 = &quot; + sum)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行程序，在控制台上看到如下所示的结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/48-2.png" width="60%" height="60%"><br><br></p><p><font size="4"><b>使用HDFS文件创建RDD</b></font><br>例如：累加1到10<br>在Linux上创建文本文件，文件内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd input/</span><br><span class="line">[root@ggstu input]# cat test.txt </span><br><span class="line">1 2 3 4 5 6 7 8 9 10</span><br></pre></td></tr></table></figure></p><p>启动hadoop集群，将test.txt上传到hdfs上<br>接着启动spark集群<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu input]# hdfs dfs -put test.txt /test.txt</span><br><span class="line">[root@ggstu input]# hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 root supergroup         21 2018-09-03 20:43 /test.txt</span><br></pre></td></tr></table></figure></p><p>代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object HDFSFile &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">          .setAppName(&quot;HDFSFile&quot;)</span><br><span class="line">          </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;hdfs://ggstu:9000/test.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val nums = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    val numsToInt = nums.map(num =&gt; num.toInt)</span><br><span class="line">    </span><br><span class="line">    val sum = numsToInt.reduce(_ + _)</span><br><span class="line">    </span><br><span class="line">    println(&quot;1+2+3+4+5+......+10 = &quot; + sum)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>将程序打包成jar包，并上传到Linux上，之后执行执行脚本。<br>如果这几步不懂，在<a href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Scala%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/" target="_blank">使用Scala开发Spark的wordcount程序</a>这篇文章中有详细过程。<br>执行脚本结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/48-3.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/03/Spark%E5%88%9B%E5%BB%BARDD%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark创建RDD的几种方式（Java代码）&lt;/a&gt;这篇文章中介绍了Spark创建RDD的三种方式的Java代码。&lt;br&gt;下面介绍Scala版本的创建RDD，直接上代码。&lt;/p&gt;
&lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;并行化集合创建RDD&lt;/b&gt;&lt;/font&gt;&lt;br&gt;例如：累加1到10&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark创建RDD的几种方式（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/03/Spark%E5%88%9B%E5%BB%BARDD%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/03/Spark创建RDD的几种方式（Java代码）/</id>
    <published>2018-09-03T05:02:48.000Z</published>
    <updated>2018-09-03T07:20:36.594Z</updated>
    
    <content type="html"><![CDATA[<p>Spark核心编程，首先要做的是创建一个初始的RDD。该RDD通常包含了Spark应用程序的输入源数据。只有在创建了初始的RDD之后，才可以使用transformation算子，对该RDD进行转换，得到其它的RDD。<br>Spark Core提供了三种创建RDD的方式：<br>1、使用程序中的集合创建RDD<br>2、使用本地文件创建RDD<br>3、使用HDFS文件创建RDD<br><a id="more"></a></p><p><font size="4"><b>并行化集合创建RDD</b></font><br>使用并行化集合来创建RDD，需要对集合调用SparkContext的parallelize()方法。Spark会将集合中的数据拷贝到集群上去，形成一个分布式的数据集，也就是一个RDD。<br>例如：累加1到10<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line"></span><br><span class="line">public class ParallelizeCollection &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;ParallelizeCollection&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numbers = Arrays.asList(1,2,3,4,5,6,7,8,9,10);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numberRDD = sc.parallelize(numbers,3);</span><br><span class="line"></span><br><span class="line">int sum = numberRDD.reduce(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer num1, Integer num2) throws Exception &#123;</span><br><span class="line">return num1 + num2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;1+2+3+......+10 = &quot; + sum);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行程序，在控制台上看到如下所示的结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-1.png" width="50%" height="50%"><br>调用parallelize()时，可以指定一个参数将集合切分成多少个partition。Spark会为每一个partition运行一个task。<br>Spark官方的建议是，为集群中的每个CPU创建2~4个partition。Spark会根据集群的情况来设置partition的数量。也可以在调用parallelize()方法时，传入第二个参数，设置RDD的partition数量，比如parallelize(numbers,3)。<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-2.png" width="100%" height="100%"><br><br></p><p><font size="4"><b>使用本地文件创建RDD</b></font><br>Spark支持使用任何Hadoop支持的存储系统上的文件创建RDD的，比如HDFS、Cassandra、HBase以及本地文件。<br>通过调用SparkContext的textFile()方法，对本地文件创建RDD。<br>例如：累加1到10<br>我在本地桌面上创建了个文本文件，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 2 3 4 5 6 7 8 9 10</span><br></pre></td></tr></table></figure></p><p>代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line"></span><br><span class="line">public class LocalFile &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;LocalFile&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus/Desktop//test.txt&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; nums = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">String sum = nums.reduce(new Function2&lt;String, String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public String call(String num1, String num2) throws Exception &#123;</span><br><span class="line">return (Integer.parseInt(num1) + Integer.parseInt(num2))+&quot;&quot;;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;1+2+3+4+......+10 = &quot; + sum);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行程序，在控制台上看到如下所示的结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-3.png" width="60%" height="60%"><br><br></p><p><font size="4"><b>使用HDFS文件创建RDD</b></font><br>通过调用SparkContext的textFile()方法，对HDFS文件创建RDD。<br>例如：累加1到10<br>在Linux上创建文本文件，文件内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd input/</span><br><span class="line">[root@ggstu input]# cat test.txt </span><br><span class="line">1 2 3 4 5 6 7 8 9 10</span><br></pre></td></tr></table></figure></p><p>启动hadoop集群，将test.txt上传到hdfs上<br>接着启动spark集群<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu input]# hdfs dfs -put test.txt /test.txt</span><br><span class="line">[root@ggstu input]# hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 root supergroup         21 2018-09-03 14:26 /test.txt</span><br></pre></td></tr></table></figure></p><p>代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line"></span><br><span class="line">public class HDFSFile &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;HDFSFile&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;hdfs://ggstu:9000/test.txt&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; nums = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">String sum = nums.reduce(new Function2&lt;String, String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public String call(String num1, String num2) throws Exception &#123;</span><br><span class="line">return (Integer.parseInt(num1)+Integer.parseInt(num2))+&quot;&quot;;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;1+2+3+4+5+......+10 = &quot; + sum);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>将程序打包成jar包，并上传到Linux上，之后执行执行脚本。<br>如果这几步不懂，在<a href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Java%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/" target="_blank">使用Java开发Spark的wordcount程序</a>这篇文章中有详细过程。<br>执行脚本结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-4.png" width="50%" height="50%"><br><br></p><p><font size="4"><b>其它创建RDD方式</b></font><br>Spark的官方文档中可以看到除了通过使用textFile()方法创建RDD，还有如下几种方式：<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-5.png" width="100%" height="100%"><br>1、SparkContext.wholeTextFiles方法，可以针对一个目录中大量的小文件，返回&lt;filename,content&gt;组成的pair，作为一个PairRDD，而不是普通的RDD。普通的textFile()返回的RDD是文件中的一行文本。<br>2、SparkContext.sequenceFile[K,V]方法，可以针对SequenceFile创建RDD，K和V泛型类型就是SequenceFile的key和value的类型。K和V要求必须是Hadoop的序列化类型，如IntWritable、Text等。<br>3、SparkContext.hadoopRDD方法，对于Hadoop的自定义输入类型，可以创建RDD。该方法接收JobConf、InputFormatClass、Key和Value的class。<br>4、SparkContext.objectFile方法，可以针对之前调用RDD.saveAsObjectFile创建的对象序列化的文件，反序列化文件中的数据，并创建一个RDD。</p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spark核心编程，首先要做的是创建一个初始的RDD。该RDD通常包含了Spark应用程序的输入源数据。只有在创建了初始的RDD之后，才可以使用transformation算子，对该RDD进行转换，得到其它的RDD。&lt;br&gt;Spark Core提供了三种创建RDD的方式：&lt;br&gt;1、使用程序中的集合创建RDD&lt;br&gt;2、使用本地文件创建RDD&lt;br&gt;3、使用HDFS文件创建RDD&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>使用Scala开发Spark的wordcount程序</title>
    <link href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Scala%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/"/>
    <id>https://www.ggstu.com/2018/09/02/使用Scala开发Spark的wordcount程序/</id>
    <published>2018-09-02T05:29:33.000Z</published>
    <updated>2018-09-02T12:31:31.768Z</updated>
    
    <content type="html"><![CDATA[<p>使用Scala IDE for Eclipse开发Scala程序，如果没有下载，点击如下链接进行下载<br><a href="http://scala-ide.org/" target="_blank">http://scala-ide.org/</a><br><a id="more"></a></p><p><b>1、打开Scala IDE for Eclipse，File-&gt;New-&gt;Project，然后找到如下图的Maven，创建Maven工程。</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/46-1.png" width="70%" height="70%"><br><b>2、直接点击Next</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/46-2.png" width="70%" height="70%"><br><b>3、选择maven-archetype-quickstart，然后点击Next</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/46-3.png" width="100%" height="100%"><br><b>4、添加Group ID和Artifact ID，然后点击Finish</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/46-4.png" width="100%" height="100%"></p><p>创建好的工程目录结构如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-5.png" width="40%" height="40%"><br><b>5、修改JRE</b><br>右键JRE System Library，选择Properties，修改JRE<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-6.png" width="100%" height="100%"><br><b>6、添加依赖</b><br>在pom.xml文件的dependencies块中添加如下依赖<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.7.7&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p><p><b>7、添加Scala属性</b><br>右键工程名-&gt;Configure-&gt;Add Scala Nature<br><b>8、修改Scala Library container</b><br>右键Scala Library container，选择Properties，修改为Fixed Scala Library container:2.11.11<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-8.png" width="70%" height="70%"><br><b>9、创建Scala Object</b><br>右键创建好的Package-&gt;New-&gt;Scala Object<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-7.png" width="70%" height="70%"><br><br></p><p><font size="4"><b>首先用Scala开发在本地运行的wordcount程序</b></font><br>在桌面创建了个test.txt文件，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p>执行的步骤与Java开发wordcount程序相同，步骤在Java开发wordcount程序那篇文章中写的比较详细，这里就不赘述了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">package com.ggstu.spark</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object WordCountLocal &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;WordCountLocal&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    val pairs = words.map(word =&gt; (word,1))</span><br><span class="line">    </span><br><span class="line">    val wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line">    </span><br><span class="line">    wordCounts.foreach(wordCount =&gt; println(wordCount._1 + &quot;: &quot; + wordCount._2))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>最后在本地Scala IDE运行程序，在控制台上看到如下所示的结果，即每个单词及其出现的频率。<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-9.png" width="40%" height="40%"><br><br></p><p><font size="4"><b>用Scala开发wordcount程序提交到Spark集群上运行</b></font><br>再创建一个Scala Object，命名为WordCountCluster</p><p><font size="4"><b>开发步骤：</b></font><br><b>1、创建数据源文件，并提交到hdfs上</b><br>在Linux上创建文件，并在文件中添加如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd /root/input/</span><br><span class="line">[root@ggstu input]# cat data.txt </span><br><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p>启动hadoop集群，将data.txt上传到hdfs上<br>接着启动spark集群<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu input]# hdfs dfs -put data.txt /wordcount.txt</span><br><span class="line">[root@ggstu input]# hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 root supergroup         27 2018-09-02 10:19 /wordcount.txt</span><br></pre></td></tr></table></figure></p><p><b>2、编写Scala代码</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package com.ggstu.spark</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object WordCountCluster &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;WordCountCluster&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;hdfs://ggstu:9000/wordcount.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    val pairs = words.map(word =&gt; (word,1))</span><br><span class="line">    </span><br><span class="line">    val wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line">    </span><br><span class="line">    wordCounts.foreach(wordCount =&gt; println(wordCount._1 + &quot;: &quot; + wordCount._2))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><b>3、将程序打包成jar包，并上传到Linux上</b><br>右击WordCountCluster.scala-&gt;Export，选择JAR file，然后点击Next<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-10.png" width="60%" height="60%"><br>选择jar包保存路径及其名称<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-11.png" width="60%" height="60%"><br>直接点击Next<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-12.png" width="60%" height="60%"><br>直接点击Finish<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-13.png" width="60%" height="60%"><br>这样，就可以在保存的路径下看到这个jar包</p><p>将jar包上传到事先在Linux创建好的目录中<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-14.png" width="100%" height="100%"><br><b>4、执行wordcount程序</b><br>使用vi编辑器创建执行脚本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jar]# vi wordcount.sh</span><br></pre></td></tr></table></figure></p><p>添加如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/root/software/spark-2.3.1-bin-hadoop2.7/bin/spark-submit \</span><br><span class="line">--class com.ggstu.spark.WordCountCluster \</span><br><span class="line">--num-executors 3 \</span><br><span class="line">--driver-memory 500m \</span><br><span class="line">--executor-memory 500m \</span><br><span class="line">--executor-cores 3 \</span><br><span class="line">/root/jar/WordCountCluster.jar \</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p>修改脚本的执行权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jar]# chmod 777 wordcount.sh</span><br></pre></td></tr></table></figure></p><p>执行脚本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jar]# ./wordcount.sh</span><br></pre></td></tr></table></figure></p><p>就可以在Linux机器上看到如下的打印结果，即每个单词及其出现的频率。<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-15.png" width="60%" height="60%"></p><p>到这里，使用Scala开发Spark的wordcount程序，在本地上运行和在Spark集群上运行都实现完成了。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用Scala IDE for Eclipse开发Scala程序，如果没有下载，点击如下链接进行下载&lt;br&gt;&lt;a href=&quot;http://scala-ide.org/&quot; target=&quot;_blank&quot;&gt;http://scala-ide.org/&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>使用Java开发Spark的wordcount程序</title>
    <link href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Java%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/"/>
    <id>https://www.ggstu.com/2018/09/02/使用Java开发Spark的wordcount程序/</id>
    <published>2018-09-02T00:38:25.000Z</published>
    <updated>2018-09-02T04:10:39.750Z</updated>
    
    <content type="html"><![CDATA[<p>在开发wordcount程序前，需要搭建Maven工程并添加开发spark程序所需的依赖。<br><b>1、打开Eclipse，File-&gt;New-&gt;Project，然后找到如下图的Maven，创建Maven工程。</b><br><a id="more"></a><br><img src="http://pd8lpasbc.bkt.clouddn.com/45-1.png" width="70%" height="70%"><br><b>2、直接点击Next</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/45-2.png" width="70%" height="70%"><br><b>3、选择maven-archetype-quickstart，然后点击Next</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/45-3.png" width="70%" height="70%"><br><b>4、添加Group ID和Artifact ID，然后点击Finish</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/45-4.png" width="70%" height="70%"></p><p>创建好的工程目录结构如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-5.png" width="40%" height="40%"><br><b>5、修改JRE</b><br>右键JRE System Library，选择Properties，修改JRE<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-6.png" width="100%" height="100%"><br><b>6、添加依赖</b><br>pom.xml文件内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">  &lt;groupId&gt;com.ggstu&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;spark&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;</span><br><span class="line">  &lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class="line"></span><br><span class="line">  &lt;name&gt;spark&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.apache.org&lt;/url&gt;</span><br><span class="line"></span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;3.8.1&lt;/version&gt;</span><br><span class="line">      &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;2.3.0&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;2.7.7&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure></p><p><br></p><p><font size="4"><b>首先用Java开发在本地运行的wordcount程序</b></font><br>如下所示，一共有八个步骤：<br><b>第一步：</b>创建SparkConf对象，设置Spark应用的配置信息，setAppName()设置运行程序的名称，使用setMaster()可以设置Spark应用程序要连接的Spark集群的master结点的url，但是如果设置为local，表示在本地运行。<br><b>第二步：</b>创建JavaSparkContext对象，SparkContext是Spark所有功能的入口，主要作用包括初始化Spark应用程序所需的核心组件，包括调度器(DAGScheduler、TaskScheduler)，还会到Spark Master节点上进行注册，等等。<br>编写不同类型的Spark应用程序，使用的SparkContext是不同的。使用scala编写，就是原生的SparkContext对象。使用Java编写，就是JavaSparkContext对象。开发Spark SQL程序，就是SQLContext或HiveContext对象。开发Spark Streaming程序，就是它独有的SparkContext对象。<br><b>第三步：</b>SparkContext中，用于根据文件类型的输入源创建RDD的方法，叫做textFile。输入源可以是hdfs文件或本地文件。输入源中的数据会被打散，分配到RDD的每个partition中，从而形成一个初始的分布式的数据集。<br>这里开发wordcount程序，我在桌面建了个test.txt文件，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p><b>第四步：</b>对初始RDD进行transformation操作，也就是一些计算操作。通常操作通过创建function，并配合RDD的map、flatMap的等算子来执行。FlatMapFunction有两个泛型参数，分别代表了输入和输出类型。<br><b>第五步：</b>将每个单词映射为(单词,1)的这种格式。mapToPair是将每个元素映射为一个(v1,v2)这样的Tuple2类型的元素。mapToPair这个算子，与PairFunction配合使用，第一个泛型参数代表了输入类型，第二个和第三个泛型参数，代表的输出的Tuple2的第一个值和第二个值的类型。JavaPairRDD的两个泛型参数，分别代表了tuple元素的第一个值和第二个值的类型。<br><b>第六步：</b>以单词作为key，统计每个单词出现的次数。使用reduceByKey这个算子，对每个key对应的value进行reduce合并操作。最后返回的JavaPairRDD中的元素也是tuple，第一个值是每个key，第二个值是key的value合并之后的结果，即每个单词出现的次数。<br><b>第七步：</b>使用foreach这种action操作触发程序的执行。之前使用的flatMap、mapToPair、reduceByKey这种操作，都叫做transformation操作。一个Spark应用中，若只有transformation操作是不会执行的，必须要有action操作。<br><b>第八步：</b>关闭SparkContext对象。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">package com.ggstu.spark.core;</span><br><span class="line"></span><br><span class="line">import java.awt.List;</span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class WordCountLocal &#123;</span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">//第一步</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;WordCountLocal&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">//第二步</span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">//第三步</span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;);</span><br><span class="line"></span><br><span class="line">//第四步</span><br><span class="line">JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">//第五步</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; pairs = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;String, Integer&gt;(word, 1);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">//第六步</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; wordCounts = pairs.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer v1, Integer v2) throws Exception &#123;</span><br><span class="line">return v1 + v2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">//第七步</span><br><span class="line">wordCounts.foreach(new VoidFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;String, Integer&gt; wordCount) throws Exception &#123;</span><br><span class="line">System.out.println(wordCount._1 + &quot;: &quot; + wordCount._2);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">//第八步</span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>最后在本地IDE运行程序，在控制台上看到如下所示的结果，即每个单词及其出现的频率。<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-7.png" width="70%" height="70%"><br><br></p><p><font size="4"><b>用Java开发wordcount程序提交到Spark集群上运行</b></font><br>如果要在spark集群上运行wordcount，代码与本地开发wordcount程序不同的只有两处：<br>一、将SparkConf的setMaster()方法删除掉，默认会去连接Spark集群。<br>二、文件路径不是本地文件，而是hadoop hdfs上的文件。</p><p><font size="4"><b>开发步骤：</b></font><br><b>1、创建数据源文件，并提交到hdfs上</b><br>在Linux上创建文件，并在文件中添加如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd /root/input/</span><br><span class="line">[root@ggstu input]# cat data.txt </span><br><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p>启动hadoop集群，将data.txt上传到hdfs上<br>接着启动spark集群<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu input]# hdfs dfs -put data.txt /wordcount.txt</span><br><span class="line">[root@ggstu input]# hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 root supergroup         27 2018-09-02 10:19 /wordcount.txt</span><br></pre></td></tr></table></figure></p><p><b>2、编写Java代码</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">package com.ggstu.spark.core;</span><br><span class="line"></span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class WordCountCluster &#123;</span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;WordCountCluster&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;hdfs://ggstu:9000/wordcount.txt&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; pairs = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;String, Integer&gt;(word, 1);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; wordCounts = pairs.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer v1, Integer v2) throws Exception &#123;</span><br><span class="line">return v1 + v2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">wordCounts.foreach(new VoidFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;String, Integer&gt; wordCount) throws Exception &#123;</span><br><span class="line">System.out.println(wordCount._1 + &quot;: &quot; + wordCount._2);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><b>3、将程序打包成jar包，并上传到Linux上</b><br>右击WordCountCluster.java-&gt;Export，选择JAR file，然后点击Next<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-8.png" width="60%" height="60%"><br>选择jar包保存路径及其名称<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-9.png" width="60%" height="60%"><br>直接点击Next<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-10.png" width="60%" height="60%"><br>选择主类，然后点击Finish<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-11.png" width="60%" height="60%"><br>这样，就可以在保存的路径下看到这个jar包</p><p>将jar包上传到事先在Linux创建好的目录中<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-12.png" width="100%" height="100%"><br><b>4、执行wordcount程序</b><br>使用vi编辑器创建执行脚本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jar]# vi wordcount.sh</span><br></pre></td></tr></table></figure></p><p>添加如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/root/software/spark-2.3.1-bin-hadoop2.7/bin/spark-submit \</span><br><span class="line">--class com.ggstu.spark.core.WordCountCluster \</span><br><span class="line">--num-executors 3 \</span><br><span class="line">--driver-memory 500m \</span><br><span class="line">--executor-memory 500m \</span><br><span class="line">--executor-cores 3 \</span><br><span class="line">/root/jar/WordCountCluster.jar \</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p>修改脚本的执行权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jar]# chmod 777 wordcount.sh</span><br></pre></td></tr></table></figure></p><p>执行脚本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jar]# ./wordcount.sh</span><br></pre></td></tr></table></figure></p><p>就可以在Linux机器上看到如下的打印结果，即每个单词及其出现的频率。<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-13.png" width="60%" height="60%"></p><p>到这里，使用Java开发Spark的wordcount程序，在本地上运行和在Spark集群上运行都实现完成了。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在开发wordcount程序前，需要搭建Maven工程并添加开发spark程序所需的依赖。&lt;br&gt;&lt;b&gt;1、打开Eclipse，File-&amp;gt;New-&amp;gt;Project，然后找到如下图的Maven，创建Maven工程。&lt;/b&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>林润故居</title>
    <link href="https://www.ggstu.com/2018/09/02/%E6%9E%97%E6%B6%A6%E6%95%85%E5%B1%85/"/>
    <id>https://www.ggstu.com/2018/09/02/林润故居/</id>
    <published>2018-09-01T23:50:23.000Z</published>
    <updated>2018-09-01T14:57:39.213Z</updated>
    
    <content type="html"><![CDATA[<p>最近查看了下邮件，发现了我第一封发送的邮件。那是在高一，老师布置的语文作业，写一篇小几百字的描述校园一角的文章，写完后邮箱发给老师。就这样，它被存下来了。<br><a id="more"></a><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size="4"><b>温暖古朴的校园一角</b></font></p><p>&emsp;&emsp;从校外的栅栏往里望，看到了绿茵草地点缀着校园,看到了崭新建筑装饰着校园，看到了彩蝶扑粉美丽了校园。但是槛外人是不会发现，在那，在校园的一角，还有着一个“与世隔绝”的地方。那是古朴的林润故居。</p><p>&emsp;&emsp;首先吸引我眼球的不是它老式的建筑形式，而是从矮矮的墙头冒出的一簇花，“一枝红杏出墙来”，红色的花朵安静地守在故居的身边。即使红色是热烈奔放的象征，但它与古朴的小屋结合起来，又有与众不同的感觉。故居周围的树都不是很高大，或许是为了让这校园一角更多地展现在人们的眼前，以至于不被人们忽视。</p><p>&emsp;&emsp;推开木制的门，历史的气息扑面而来，脚下却被高高的门槛所绊到，门槛这么高，应该是为了防止雨天雨水涌入屋内所设计的。跨进屋内，首先映入眼帘的是大门对面一扇扇带孔的门，手从一扇扇门前划过，触摸到了历史的气息。从他们的建筑中我看到了那一代人的智慧，故居正门前的那个地方是被设计成低平的，角落处还有一个小洞通到屋外，这样无论雨下得多大，雨水都会流到这个低处，再顺着洞流出去。走进正屋，看着门前的几根木柱，有的掉了一小层皮，留下了岁月的痕迹。屋内的光线虽然不亮，但还是可以清楚地看到林润的雕像被放在了屋子的正中央，左右两侧还有一对金色的对联。环顾周围的一切，其实它的空间很小，大概只有一间教室的大小，但是小也有它的好处，能够营造一种温暖的感觉，家的味道。</p><p>&emsp;&emsp;踏出门槛，掩上木门，看着眼前一排排整齐的现代建筑，与身后的林润故居可是天壤之别，但是校园内有了这一角故居，更会给人温暖的感觉。门口的水泥道路，在林润故居的陪衬下，更是大家茶余饭后散步的绝佳地带。有了这古朴的一角，才更有温暖的感觉。</p><p>&emsp;&emsp;古朴的林润故居，你虽然占据的仅是校园的一角，带来的却是无穷的温暖。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近查看了下邮件，发现了我第一封发送的邮件。那是在高一，老师布置的语文作业，写一篇小几百字的描述校园一角的文章，写完后邮箱发给老师。就这样，它被存下来了。&lt;br&gt;
    
    </summary>
    
      <category term="随便说说" scheme="https://www.ggstu.com/categories/%E9%9A%8F%E4%BE%BF%E8%AF%B4%E8%AF%B4/"/>
    
    
      <category term="心情" scheme="https://www.ggstu.com/tags/%E5%BF%83%E6%83%85/"/>
    
  </entry>
  
  <entry>
    <title>Spark2.3.1版本全分布模式的安装与部署</title>
    <link href="https://www.ggstu.com/2018/09/01/Spark2-3-1%E7%89%88%E6%9C%AC%E5%85%A8%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2/"/>
    <id>https://www.ggstu.com/2018/09/01/Spark2-3-1版本全分布模式的安装与部署/</id>
    <published>2018-09-01T13:31:02.000Z</published>
    <updated>2018-09-01T14:25:16.968Z</updated>
    
    <content type="html"><![CDATA[<p>部署Spark的全分布模式，至少需要三台Linux机器，首先要有一些准备工作，就是配置Hadoop的全分布环境。我准备了三台机器，分别命名为Master，Worker1，Worker2<br>如果不清楚如何配置Hadoop的全分布环境，点击如下链接进行配置：<br><a href="https://www.ggstu.com/2018/08/31/Hadoop2-X%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%EF%BC%88%E5%9B%9B%EF%BC%89%E5%85%A8%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F/" target="_blank">Hadoop2.X的安装与配置（全分布模式）</a><br><a id="more"></a><br>配置完Hadoop全分布环境后，接下来开始安装配置Spark2.3.1版本全分布模式，这里部署的全分布模式是standalone模式。</p><p><font size="4"><b>接下来开始进行配置，在主节点Master上进行配置，然后把配置好的文件复制到其它机器</b></font><br><b>1、在Windows上下载Spark2.3.1安装包</b><br>点击如下链接进行下载：<br>由于配置的Hadoop版本是2.7.7,所以下载spark-2.3.1-bin-hadoop2.7.tgz这个压缩包<br><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">Spark官网下载地址</a></p><p><b>2、将安装包上传到Linux上，并解压缩</b><br>使用WinSCP将压缩包上传到事先创建好的tools目录下<br><img src="http://pd8lpasbc.bkt.clouddn.com/43-1.png" width="100%" height="100%"><br>使用SecureCRT远程连接上Linux，将压缩包解压到事先创建好的software目录下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# cd tools/</span><br><span class="line">[root@Master tools]# ls</span><br><span class="line">hadoop-2.7.7.tar.gz  jdk-8u131-linux-x64.tar.gz  spark-2.3.1-bin-hadoop2.7.tgz</span><br><span class="line">[root@Master tools]# tar -zxvf spark-2.3.1-bin-hadoop2.7.tgz -C ~/software/</span><br></pre></td></tr></table></figure></p><p><b>3、修改spark-env.sh配置文件</b><br>使用vi编辑器修改spark-env.sh配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Master tools]# cd ~/software/spark-2.3.1-bin-hadoop2.7/conf/</span><br><span class="line">[root@Master conf]# cp spark-env.sh.template spark-env.sh</span><br><span class="line">[root@Master conf]# vi spark-env.sh</span><br></pre></td></tr></table></figure></p><p>在spark-env.sh文件末尾添加如下内容：分别是JAVA_HOME，主机名和端口号<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/root/software/jdk1.8.0_131</span><br><span class="line">export SPARK_MASTER_HOST=Master</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure></p><p>这里添加的是你自己那台机器的JAVA_HOME路径，主机名，端口号不变，就是7077。<br>添加完成后，保存退出。</p><p><b>4、修改slaves配置文件</b><br>使用vi编辑器修改slaves配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@Master conf]# cp slaves.template slaves</span><br><span class="line">[root@Master conf]# vi slaves</span><br></pre></td></tr></table></figure></p><p>删除slaves文件末尾的localhost，并添加从节点的主机名:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Worker1</span><br><span class="line">Worker2</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p><b>5、把主节点上配置好的spark环境复制到从节点上</b><br>由于配置了SSH免密码登录，所以这里复制到其他机器上不需要输入密码。<br>复制到第一个从节点Worker1上<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# cd software/</span><br><span class="line">[root@Master software]# scp -r spark-2.3.1-bin-hadoop2.7/ root@Worker1:/root/software/</span><br></pre></td></tr></table></figure></p><p>复制到第二个从节点Worker2上<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master software]# scp -r spark-2.3.1-bin-hadoop2.7/ root@Worker2:/root/software/</span><br></pre></td></tr></table></figure></p><p>可以在从节点Worker1上看到复制过来的spark-2.3.1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# cd software/</span><br><span class="line">[root@Worker1 software]# ls</span><br><span class="line">hadoop-2.7.7  jdk1.8.0_131  spark-2.3.1-bin-hadoop2.7</span><br></pre></td></tr></table></figure></p><p>可以在从节点Worker2上看到复制过来的spark-2.3.1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker2 ~]# cd software/</span><br><span class="line">[root@Worker2 software]# ls</span><br><span class="line">hadoop-2.7.7  jdk1.8.0_131  spark-2.3.1-bin-hadoop2.7</span><br></pre></td></tr></table></figure></p><p>到这里Spark的全分布模式的配置就完成了。<br><br></p><p><font size="4"><b>启动Spark全分布集群：</b></font><br>由于hadoop和spark命令启动脚本start-all.sh有冲突，所以在系统的环境变量配置文件bash_profile中，不配置Spark的环境。<br>启动Spark，在主节点Master上，进入spark目录中进行启动。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# cd ~/software/spark-2.3.1-bin-hadoop2.7/</span><br><span class="line">[root@Master spark-2.3.1-bin-hadoop2.7]# sbin/start-all.sh </span><br><span class="line">starting org.apache.spark.deploy.master.Master, logging to /root/software/spark-2.3.1-bin-hadoop2.7/logs/spark-root-org.apache.spark.deploy.master.Master-1-Master.out</span><br><span class="line">Worker2: starting org.apache.spark.deploy.worker.Worker, logging to /root/software/spark-2.3.1-bin-hadoop2.7/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-Worker2.out</span><br><span class="line">Worker1: starting org.apache.spark.deploy.worker.Worker, logging to /root/software/spark-2.3.1-bin-hadoop2.7/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-Worker1.out</span><br></pre></td></tr></table></figure></p><p>从打印出的日志可以看到，在主节点上启动了Master，两个从节点上各自启动了一个Worker。<br>因为在系统的环境变量配置文件bash_profile中配置了Hadoop的环境，所以除了在spark-2.3.1-bin-hadoop2.7/sbin目录下使用start-all.sh命令启动的是Spark集群的全分布环境，其它任何位置使用start-all.sh命令启动的是Hadoop集群的全分布环境。</p><p>使用jps命令查看后台进程，也可以看到各个节点启动的进程。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# jps</span><br><span class="line">18576 Jps</span><br><span class="line">18510 Master</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# jps</span><br><span class="line">14097 Jps</span><br><span class="line">14026 Worker</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker2 ~]# jps</span><br><span class="line">13669 Worker</span><br><span class="line">13739 Jps</span><br></pre></td></tr></table></figure><p><br><br>若要查看集群的基本信息，可以在网页中输入主节点IP:8080端口进行访问。</p><p><font color="#f00">注意：</font>若使用购买的Linux服务器，这里的IP为公网IP。<br><img src="http://pd8lpasbc.bkt.clouddn.com/43-2.png" width="100%" height="100%"><br>在这个网页中也可以看到此时有两个从节点Worker<br><br><br>若要关闭Spark集群，在主节点使用如下命令关闭<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Master spark-2.3.1-bin-hadoop2.7]# sbin/stop-all.sh </span><br><span class="line">Worker2: stopping org.apache.spark.deploy.worker.Worker</span><br><span class="line">Worker1: stopping org.apache.spark.deploy.worker.Worker</span><br><span class="line">stopping org.apache.spark.deploy.master.Master</span><br></pre></td></tr></table></figure></p><p>到这里Spark2.3.1版本全分布模式的配置，以及启动关闭就介绍完了。<br><br>如果在安装过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;部署Spark的全分布模式，至少需要三台Linux机器，首先要有一些准备工作，就是配置Hadoop的全分布环境。我准备了三台机器，分别命名为Master，Worker1，Worker2&lt;br&gt;如果不清楚如何配置Hadoop的全分布环境，点击如下链接进行配置：&lt;br&gt;&lt;a href=&quot;https://www.ggstu.com/2018/08/31/Hadoop2-X%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%EF%BC%88%E5%9B%9B%EF%BC%89%E5%85%A8%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F/&quot; target=&quot;_blank&quot;&gt;Hadoop2.X的安装与配置（全分布模式）&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark2.3.1版本伪分布模式的安装与部署</title>
    <link href="https://www.ggstu.com/2018/09/01/Spark2-3-1%E7%89%88%E6%9C%AC%E4%BC%AA%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2/"/>
    <id>https://www.ggstu.com/2018/09/01/Spark2-3-1版本伪分布模式的安装与部署/</id>
    <published>2018-09-01T02:07:44.000Z</published>
    <updated>2018-09-01T03:28:02.803Z</updated>
    
    <content type="html"><![CDATA[<p><br><br>部署Spark的伪分布环境，需要一台Linux机器，首先要有一些准备工作，就是配置Hadoop的伪分布环境。<br>如果不清楚如何配置Hadoop的伪分布环境，点击如下链接进行配置：<br><a href="https://www.ggstu.com/2018/08/31/Hadoop2-X%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%EF%BC%88%E4%B8%89%EF%BC%89%E4%BC%AA%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F/" target="_blank">Hadoop2.X的安装与配置（伪分布模式）</a></p><p>配置完Hadoop伪分布环境后，接下来开始安装配置Spark2.3.1版本伪分布模式，这里部署的伪分布模式是standalone模式。<br><a id="more"></a></p><p><font size="4"><b>配置过程：</b></font><br><b>1、在Windows上下载Spark2.3.1安装包</b><br>点击如下链接进行下载：<br>由于配置的Hadoop版本是2.7.7,所以下载spark-2.3.1-bin-hadoop2.7.tgz这个压缩包<br><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">Spark官网下载地址</a></p><p><b>2、将安装包上传到Linux上，并解压缩</b><br>使用WinSCP将压缩包上传到事先创建好的tools目录下<br><img src="http://pd8lpasbc.bkt.clouddn.com/42-1.png" width="100%" height="100%"><br>使用SecureCRT远程连接上Linux，将压缩包解压到事先创建好的software目录下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd tools/</span><br><span class="line">[root@ggstu tools]# ls</span><br><span class="line">hadoop-2.7.7.tar.gz  jdk-8u131-linux-x64.tar.gz  spark-2.3.1-bin-hadoop2.7.tgz</span><br><span class="line">[root@ggstu tools]# tar -zxvf spark-2.3.1-bin-hadoop2.7.tgz -C ~/software/</span><br></pre></td></tr></table></figure></p><p><b>3、修改spark-env.sh配置文件</b><br>使用vi编辑器修改spark-env.sh配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu tools]# cd ~/software/spark-2.3.1-bin-hadoop2.7/conf/</span><br><span class="line">[root@ggstu conf]# cp spark-env.sh.template spark-env.sh</span><br><span class="line">[root@ggstu conf]# vi spark-env.sh</span><br></pre></td></tr></table></figure></p><p>在spark-env.sh文件末尾添加如下内容：分别是JAVA_HOME，主机名和端口号<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/root/software/jdk1.8.0_131</span><br><span class="line">export SPARK_MASTER_HOST=ggstu</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure></p><p>这里添加的是你自己那台机器的JAVA_HOME路径，主机名，端口号不变，就是7077。<br>添加完成后，保存退出。</p><p><b>4、修改slaves配置文件</b><br>使用vi编辑器修改slaves配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu conf]# cp slaves.template slaves</span><br><span class="line">[root@ggstu conf]# vi slaves</span><br></pre></td></tr></table></figure></p><p>删除slaves文件末尾的localhost，并添加这台主机的主机名:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ggstu</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p>到这里Spark的伪分布模式的配置就完成了。<br><br></p><p><font size="4"><b>启动Spark伪分布集群：</b></font><br>由于hadoop和spark命令启动脚本start-all.sh有冲突，所以在系统的环境变量配置文件bash_profile中，不配置Spark的环境。<br>启动Spark，进入spark目录中进行启动。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd ~/software/spark-2.3.1-bin-hadoop2.7/</span><br><span class="line">[root@ggstu spark-2.3.1-bin-hadoop2.7]# sbin/start-all.sh </span><br><span class="line">starting org.apache.spark.deploy.master.Master, logging to /root/software/spark-2.3.1-bin-hadoop2.7/logs/spark-root-org.apache.spark.deploy.master.Master-1-ggstu.out</span><br><span class="line">ggstu: starting org.apache.spark.deploy.worker.Worker, logging to /root/software/spark-2.3.1-bin-hadoop2.7/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-ggstu.out</span><br></pre></td></tr></table></figure></p><p>因为在系统的环境变量配置文件bash_profile中配置了Hadoop的环境，所以除了在spark-2.3.1-bin-hadoop2.7/sbin目录下使用start-all.sh命令启动的是Spark集群的伪分布环境，其它任何位置使用start-all.sh命令启动的是Hadoop集群的伪分布环境。</p><p>使用jps命令查看后台进程，看到启动了Spark的主节点Master和从节点Worker<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu spark-2.3.1-bin-hadoop2.7]# jps</span><br><span class="line">11858 Jps</span><br><span class="line">11736 Master</span><br><span class="line">11805 Worker</span><br></pre></td></tr></table></figure></p><p>若要查看集群的基本信息，可以在网页中输入IP:8080端口进行访问。</p><p><font color="#f00">注意：</font>若使用购买的Linux服务器，这里的IP为公网IP。<br><img src="http://pd8lpasbc.bkt.clouddn.com/42-2.png" width="100%" height="100%"><br>若要关闭Spark集群，使用如下命令关闭<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu spark-2.3.1-bin-hadoop2.7]# sbin/stop-all.sh </span><br><span class="line">ggstu: stopping org.apache.spark.deploy.worker.Worker</span><br><span class="line">stopping org.apache.spark.deploy.master.Master</span><br></pre></td></tr></table></figure></p><p>到这里Spark2.3.1版本伪分布模式的配置，以及启动关闭就介绍完了。</p><p>如果在安装过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;br&gt;&lt;br&gt;部署Spark的伪分布环境，需要一台Linux机器，首先要有一些准备工作，就是配置Hadoop的伪分布环境。&lt;br&gt;如果不清楚如何配置Hadoop的伪分布环境，点击如下链接进行配置：&lt;br&gt;&lt;a href=&quot;https://www.ggstu.com/2018/08/31/Hadoop2-X%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%EF%BC%88%E4%B8%89%EF%BC%89%E4%BC%AA%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F/&quot; target=&quot;_blank&quot;&gt;Hadoop2.X的安装与配置（伪分布模式）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;配置完Hadoop伪分布环境后，接下来开始安装配置Spark2.3.1版本伪分布模式，这里部署的伪分布模式是standalone模式。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Linux下NTP时间服务器的配置与搭建</title>
    <link href="https://www.ggstu.com/2018/08/31/Linux%E4%B8%8BNTP%E6%97%B6%E9%97%B4%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E6%90%AD%E5%BB%BA/"/>
    <id>https://www.ggstu.com/2018/08/31/Linux下NTP时间服务器的配置与搭建/</id>
    <published>2018-08-31T13:56:39.000Z</published>
    <updated>2018-08-31T14:11:12.915Z</updated>
    
    <content type="html"><![CDATA[<p>NTP服务[Network Time Protocol]是用来使计算机时间同步化的一种协议，它可以使计算机对其服务器或时钟源（如石英钟，GPS等等)做同步化，它可以提供高精准度的时间校正，且可介由加密确认的方式来防止恶毒的协议攻击。<br><a id="more"></a></p><p><font size="4"><b>在Linux上配置NTP服务器来实现多台机器间的时间同步步骤：</b></font><br>分别在每台机器上执行如下命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install ntp</span><br><span class="line">systemctl enable ntpd</span><br><span class="line">systemctl start ntpd</span><br><span class="line">systemctl is-enabled ntpd</span><br></pre></td></tr></table></figure></p><p><br></p><p><font size="4"><b>其中一台机器的执行过程如下：</b></font><br><b>1、安装NTP服务器</b><br>安装中出现会Is this ok [y/d/N]:     输入 y即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# yum install ntp</span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">base                                                     | 3.6 kB     00:00     </span><br><span class="line">epel                                                     | 3.2 kB     00:00     </span><br><span class="line">extras                                                   | 3.4 kB     00:00     </span><br><span class="line">updates                                                  | 3.4 kB     00:00     </span><br><span class="line">(1/7): base/7/x86_64/group_gz                              | 166 kB   00:00     </span><br><span class="line">(2/7): epel/x86_64/group_gz                                |  88 kB   00:00     </span><br><span class="line">(3/7): epel/x86_64/updateinfo                              | 938 kB   00:00     </span><br><span class="line">(4/7): extras/7/x86_64/primary_db                          | 187 kB   00:00     </span><br><span class="line">(5/7): epel/x86_64/primary                                 | 3.6 MB   00:00     </span><br><span class="line">(6/7): base/7/x86_64/primary_db                            | 5.9 MB   00:00     </span><br><span class="line">(7/7): updates/7/x86_64/primary_db                         | 5.2 MB   00:00     </span><br><span class="line">Determining fastest mirrors</span><br><span class="line">epel                                                                12662/12662</span><br><span class="line">Resolving Dependencies</span><br><span class="line">--&gt; Running transaction check</span><br><span class="line">---&gt; Package ntp.x86_64 0:4.2.6p5-25.el7.centos.2 will be updated</span><br><span class="line">---&gt; Package ntp.x86_64 0:4.2.6p5-28.el7.centos will be an update</span><br><span class="line">--&gt; Processing Dependency: ntpdate = 4.2.6p5-28.el7.centos for package: ntp-4.2.6p5-28.el7.centos.x86_64</span><br><span class="line">--&gt; Running transaction check</span><br><span class="line">---&gt; Package ntpdate.x86_64 0:4.2.6p5-25.el7.centos.2 will be updated</span><br><span class="line">---&gt; Package ntpdate.x86_64 0:4.2.6p5-28.el7.centos will be an update</span><br><span class="line">--&gt; Finished Dependency Resolution</span><br><span class="line"></span><br><span class="line">Dependencies Resolved</span><br><span class="line"></span><br><span class="line">================================================================================</span><br><span class="line"> Package        Arch          Version                         Repository   Size</span><br><span class="line">================================================================================</span><br><span class="line">Updating:</span><br><span class="line"> ntp            x86_64        4.2.6p5-28.el7.centos           base        549 k</span><br><span class="line">Updating for dependencies:</span><br><span class="line"> ntpdate        x86_64        4.2.6p5-28.el7.centos           base         86 k</span><br><span class="line"></span><br><span class="line">Transaction Summary</span><br><span class="line">================================================================================</span><br><span class="line">Upgrade  1 Package (+1 Dependent package)</span><br><span class="line"></span><br><span class="line">Total download size: 635 k</span><br><span class="line">Is this ok [y/d/N]: y</span><br><span class="line">Downloading packages:</span><br><span class="line">Delta RPMs disabled because /usr/bin/applydeltarpm not installed.</span><br><span class="line">(1/2): ntpdate-4.2.6p5-28.el7.centos.x86_64.rpm            |  86 kB   00:00     </span><br><span class="line">(2/2): ntp-4.2.6p5-28.el7.centos.x86_64.rpm                | 549 kB   00:00     </span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">Total                                              3.8 MB/s | 635 kB  00:00     </span><br><span class="line">Running transaction check</span><br><span class="line">Running transaction test</span><br><span class="line">Transaction test succeeded</span><br><span class="line">Running transaction</span><br><span class="line">  Updating   : ntpdate-4.2.6p5-28.el7.centos.x86_64                         1/4 </span><br><span class="line">  Updating   : ntp-4.2.6p5-28.el7.centos.x86_64                             2/4 </span><br><span class="line">  Cleanup    : ntp-4.2.6p5-25.el7.centos.2.x86_64                           3/4 </span><br><span class="line">  Cleanup    : ntpdate-4.2.6p5-25.el7.centos.2.x86_64                       4/4 </span><br><span class="line">  Verifying  : ntpdate-4.2.6p5-28.el7.centos.x86_64                         1/4 </span><br><span class="line">  Verifying  : ntp-4.2.6p5-28.el7.centos.x86_64                             2/4 </span><br><span class="line">  Verifying  : ntp-4.2.6p5-25.el7.centos.2.x86_64                           3/4 </span><br><span class="line">  Verifying  : ntpdate-4.2.6p5-25.el7.centos.2.x86_64                       4/4 </span><br><span class="line"></span><br><span class="line">Updated:</span><br><span class="line">  ntp.x86_64 0:4.2.6p5-28.el7.centos                                            </span><br><span class="line"></span><br><span class="line">Dependency Updated:</span><br><span class="line">  ntpdate.x86_64 0:4.2.6p5-28.el7.centos                                        </span><br><span class="line"></span><br><span class="line">Complete!</span><br></pre></td></tr></table></figure></p><p><b>2、开启NTP服务器</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# systemctl enable ntpd</span><br><span class="line">[root@Worker1 ~]# systemctl start ntpd</span><br><span class="line">[root@Worker1 ~]# systemctl is-enabled ntpd</span><br><span class="line">enabled</span><br></pre></td></tr></table></figure></p><p><br><br>这样，第一台机器上的NTP服务器就安装好了，在剩下的机器上安装NTP服务器，重复以上步骤即可。<br>在所有的机器上安装并配置好NTP服务器后，这些机器的时间就同步了。<br><br><br>如果在安装过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NTP服务[Network Time Protocol]是用来使计算机时间同步化的一种协议，它可以使计算机对其服务器或时钟源（如石英钟，GPS等等)做同步化，它可以提供高精准度的时间校正，且可介由加密确认的方式来防止恶毒的协议攻击。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Linux" scheme="https://www.ggstu.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop2.X的安装与配置（四）全分布模式</title>
    <link href="https://www.ggstu.com/2018/08/31/Hadoop2-X%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%EF%BC%88%E5%9B%9B%EF%BC%89%E5%85%A8%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F/"/>
    <id>https://www.ggstu.com/2018/08/31/Hadoop2-X的安装与配置（四）全分布模式/</id>
    <published>2018-08-31T11:54:45.000Z</published>
    <updated>2018-09-01T01:22:13.815Z</updated>
    
    <content type="html"><![CDATA[<p>配置全分布模式，至少需要三台Linux机器，我这里就以3台结点为例，配置全分布环境。<br>三台Linux机器，都完成准备阶段的配置后还需要如下的三个步骤，之后才开始配置全分布环境。<br>1、配置主机名<br>使用vi编辑器修改/etc/hosts文件，在配置文件中要添加三台Linux机器的IP地址和对应的主机名。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">172.20.77.47 Master</span><br><span class="line">172.20.77.48 Worker1</span><br><span class="line">172.20.77.49 Worker2</span><br></pre></td></tr></table></figure></p><a id="more"></a><p><font color="#f00">注意：</font>如果使用的是购买的Linux服务器，这里的IP是内网IP。<br>添加完成后，保存退出</p><p>2、对三台机器两两之间配置SSH免密码登录<br>如果不清楚如何配置免密码登录，点击如下链接进行配置：<br><a href="https://www.ggstu.com/2018/08/31/SSH%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E5%9C%A8Linux%E4%B8%8A%E7%9A%84%E9%85%8D%E7%BD%AE/" target="_blank">SSH免密码登录在Linux上的配置</a></p><p>3、保证每台机器的时间同步<br>在每台机器上配置NTP时间服务器保证多台机器间的时间同步，如果不清楚如何配置NTP时间服务器，点击如下链接进行配置：<br><a href="https://www.ggstu.com/2018/08/31/Linux%E4%B8%8BNTP%E6%97%B6%E9%97%B4%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E6%90%AD%E5%BB%BA/" target="_blank">Linux下NTP时间服务器的配置与搭建</a><br><br><br>完成准备工作完成后，接着就可以开始配置全分布模式。<br><img src="http://pd8lpasbc.bkt.clouddn.com/40-5.png" width="60%" height="60%"><br><br><br><b>全分布模式的配置需要在如下的配置文件中添加配置参数。</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/40-1.png" width="100%" height="100%"><br>这些参数的作用，在伪分布模式的配置中介绍过了，这里唯一不同的就是多了个slaves配置文件，它的作用是配置从节点的地址。<br><br></p><p><font size="4"><b>接下来开始进行配置，在主节点Master上进行配置，然后把配置好的文件复制到其它机器</b></font><br><b>1、修改hadoop-env.sh配置文件</b><br>使用vi编辑器修改hadoop-env.sh配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# cd software/hadoop-2.7.7/etc/hadoop/</span><br><span class="line">[root@Master hadoop]# vi hadoop-env.sh</span><br></pre></td></tr></table></figure></p><p>:set number打开行号，在hadoop-env.sh配置文件中添加JAVA_HOME路径<br><img src="http://pd8lpasbc.bkt.clouddn.com/40-2.png" width="70%" height="70%"><br>添加完成后，保存退出。</p><p><b>2、修改hdfs-site.xml配置文件</b><br>使用vi编辑器修改hdfs-site.xml配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master hadoop]# vi hdfs-site.xml</span><br></pre></td></tr></table></figure></p><p>在configuration代码块中添加如下的两个property代码块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.permissions&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p><b>3、修改core-site.xml配置文件</b><br>创建hdfs数据保存的目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master hadoop]# mkdir /root/software/hadoop-2.7.7/tmp</span><br></pre></td></tr></table></figure></p><p>使用vi编辑器修改core-site.xml配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master hadoop]# vi core-site.xml</span><br></pre></td></tr></table></figure></p><p>在configuration代码块中添加如下的两个property代码块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://Master:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/root/software/hadoop-2.7.7/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p><b>4、修改mapred-site.xml配置文件</b><br>使用vi编辑器修改mapred-site.xml配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@Master hadoop]# cp mapred-site.xml.template mapred-site.xml</span><br><span class="line">[root@Master hadoop]# vi mapred-site.xml</span><br></pre></td></tr></table></figure></p><p>在configuration代码块中添加如下这个property代码块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p><b>5、修改yarn-site.xml配置文件</b><br>使用vi编辑器修改yarn-site.xml配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master hadoop]# vi yarn-site.xml</span><br></pre></td></tr></table></figure></p><p>在configuration代码块中添加如下的两个property代码块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;Master&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p><b>6、修改slaves配置文件</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master hadoop]# vi slaves</span><br></pre></td></tr></table></figure></p><p>删除文件中的内容，并将从节点添加到此文件中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Worker1</span><br><span class="line">Worker2</span><br></pre></td></tr></table></figure></p><p><b>7、对主节点NameNode格式化</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master hadoop]# hdfs namenode -format</span><br></pre></td></tr></table></figure></p><p>在打印的日志中，若看到如下这一条，说明格式化成功。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">18/08/31 23:48:47 INFO common.Storage: Storage directory /root/software/hadoop-2.7.7/tmp/dfs/name has been successfully formatted.</span><br></pre></td></tr></table></figure></p><p><b>8、把主节点上配置好的hadoop环境复制到从节点上</b><br>由于配置了SSH免密码登录，所以这里复制到其他机器上不需要输入密码。<br>复制到第一个从节点Worker1上<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master software]# scp -r hadoop-2.7.7/ root@Worker1:/root/software/</span><br></pre></td></tr></table></figure></p><p>复制到第二个从节点Worker2上<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master software]# scp -r hadoop-2.7.7/ root@Worker2:/root/software/</span><br></pre></td></tr></table></figure></p><p>可以在从节点Worker1上看到复制过来的hadoop-2.7.7<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# cd software/</span><br><span class="line">[root@Worker1 software]# ls</span><br><span class="line">hadoop-2.7.7  jdk1.8.0_131</span><br></pre></td></tr></table></figure></p><p>可以在从节点Worker2上看到复制过来的hadoop-2.7.7<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker2 ~]# cd software/</span><br><span class="line">[root@Worker2 software]# ls</span><br><span class="line">hadoop-2.7.7  jdk1.8.0_131</span><br></pre></td></tr></table></figure></p><p>到这里Hadoop的全分布模式的配置就完成了。<br><br></p><p><font size="4"><b>启动Hadoop全分布集群：</b></font><br>在主节点上启动集群，首次启动需要在启动的过程中输入一次yes<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# start-all.sh</span><br><span class="line">This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh</span><br><span class="line">Starting namenodes on [Master]</span><br><span class="line">Master: starting namenode, logging to /root/software/hadoop-2.7.7/logs/hadoop-root-namenode-Master.out</span><br><span class="line">Worker1: starting datanode, logging to /root/software/hadoop-2.7.7/logs/hadoop-root-datanode-Worker1.out</span><br><span class="line">Worker2: starting datanode, logging to /root/software/hadoop-2.7.7/logs/hadoop-root-datanode-Worker2.out</span><br><span class="line">Starting secondary namenodes [0.0.0.0]</span><br><span class="line">The authenticity of host &apos;0.0.0.0 (0.0.0.0)&apos; can&apos;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:ygoutFzn8lgE6ObOfMEpysiW7fzow7qA5I+32p3K0bU.</span><br><span class="line">ECDSA key fingerprint is MD5:92:5c:41:87:ba:b5:a3:83:f3:4a:86:87:61:0a:cc:5f.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">0.0.0.0: Warning: Permanently added &apos;0.0.0.0&apos; (ECDSA) to the list of known hosts.</span><br><span class="line">0.0.0.0: starting secondarynamenode, logging to /root/software/hadoop-2.7.7/logs/hadoop-root-secondarynamenode-Master.out</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /root/software/hadoop-2.7.7/logs/yarn-root-resourcemanager-Master.out</span><br><span class="line">Worker1: starting nodemanager, logging to /root/software/hadoop-2.7.7/logs/yarn-root-nodemanager-Worker1.out</span><br><span class="line">Worker2: starting nodemanager, logging to /root/software/hadoop-2.7.7/logs/yarn-root-nodemanager-Worker2.out</span><br></pre></td></tr></table></figure></p><p>从打印出的日志可以看到namenode、secondarynamenode、resourcemanager在主节点Master上，datanode、nodemanager在从节点上。<br>使用jps命令查看后台进程，也可以看到各个节点启动的进程。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# jps</span><br><span class="line">13139 ResourceManager</span><br><span class="line">12803 NameNode</span><br><span class="line">13429 Jps</span><br><span class="line">12989 SecondaryNameNode</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# jps</span><br><span class="line">11410 DataNode</span><br><span class="line">11655 Jps</span><br><span class="line">11514 NodeManager</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker2 ~]# jps</span><br><span class="line">11584 Jps</span><br><span class="line">11444 NodeManager</span><br><span class="line">11340 DataNode</span><br></pre></td></tr></table></figure><p><br></p><p>若要查看集群的基本信息，可以在网页中输入IP:50070端口进行访问。</p><p><font color="#f00">注意：</font>若使用购买的Linux服务器，这里的IP为公网IP。<br><img src="http://pd8lpasbc.bkt.clouddn.com/40-3.png" width="100%" height="100%"></p><p>若要查看运行的MapReduce程序，可以在网页中输入IP:8088端口进行访问。<br><img src="http://pd8lpasbc.bkt.clouddn.com/40-4.png" width="100%" height="100%"></p><p>若要关闭集群，在主节点使用如下命令关闭<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# stop-all.sh</span><br><span class="line">This script is Deprecated. Instead use stop-dfs.sh and stop-yarn.sh</span><br><span class="line">Stopping namenodes on [Master]</span><br><span class="line">Master: stopping namenode</span><br><span class="line">Worker2: stopping datanode</span><br><span class="line">Worker1: stopping datanode</span><br><span class="line">Stopping secondary namenodes [0.0.0.0]</span><br><span class="line">0.0.0.0: stopping secondarynamenode</span><br><span class="line">stopping yarn daemons</span><br><span class="line">stopping resourcemanager</span><br><span class="line">Worker2: stopping nodemanager</span><br><span class="line">Worker1: stopping nodemanager</span><br><span class="line">no proxyserver to stop</span><br></pre></td></tr></table></figure></p><p>到这里Hadoop的全分布模式的配置，以及启动关闭就介绍完了。</p><p>如果在安装过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;配置全分布模式，至少需要三台Linux机器，我这里就以3台结点为例，配置全分布环境。&lt;br&gt;三台Linux机器，都完成准备阶段的配置后还需要如下的三个步骤，之后才开始配置全分布环境。&lt;br&gt;1、配置主机名&lt;br&gt;使用vi编辑器修改/etc/hosts文件，在配置文件中要添加三台Linux机器的IP地址和对应的主机名。&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;172.20.77.47 Master&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;172.20.77.48 Worker1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;172.20.77.49 Worker2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Hadoop" scheme="https://www.ggstu.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>SSH免密码登录的原理及其在Linux上的配置</title>
    <link href="https://www.ggstu.com/2018/08/31/SSH%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E5%9C%A8Linux%E4%B8%8A%E7%9A%84%E9%85%8D%E7%BD%AE/"/>
    <id>https://www.ggstu.com/2018/08/31/SSH免密码登录的原理及其在Linux上的配置/</id>
    <published>2018-08-31T10:52:27.000Z</published>
    <updated>2018-08-31T13:39:38.697Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>基本原理：采用非对称加密算法</b></font><br>1、产生一个密钥对：公钥（锁）、私钥（钥匙）<br>2、公钥：给别人来加密<br>3、私钥：给自己来解密<br>4、RSA算法是一种非对称加密算法<br><a id="more"></a><br><img src="http://pd8lpasbc.bkt.clouddn.com/39-2.png" alt="原理图"></p><p>若没有配置SSH免密码登录，要想登录另外一台机器，如Worker1这台机器登录Worker2这台机器，需要输入Worker2的登录密码，如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# ssh Worker2</span><br><span class="line">The authenticity of host &apos;worker2 (172.20.77.49)&apos; can&apos;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:MsBRF2Dy1tNNcVbeLRa7x+dTTAh4Yz0P1eE6IK53SZQ.</span><br><span class="line">ECDSA key fingerprint is MD5:5f:f3:a0:18:13:68:84:76:6e:95:a5:18:a2:41:98:0f.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added &apos;worker2,172.20.77.49&apos; (ECDSA) to the list of known hosts.</span><br><span class="line">root@worker2&apos;s password: </span><br><span class="line">Last login: Fri Aug 31 21:11:02 2018 from 183.212.160.91</span><br><span class="line"></span><br><span class="line">Welcome to Alibaba Cloud Elastic Compute Service !</span><br><span class="line"></span><br><span class="line">[root@Worker2 ~]#</span><br></pre></td></tr></table></figure></p><p>显然若不配置SSH免密码登录，每次登录另外一台机器都要输入密码，这很麻烦。因此需要配置免密码登录，以后登录另外这台机器就不需要输入密码。<br><br></p><p><font size="4"><b>SSH免密码登录配置：</b></font><br><b>1、在/etc/hosts文件中添加对应的主机名与IP地址的映射</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# vi /etc/hosts</span><br></pre></td></tr></table></figure></p><p>例如我这里有三台机器，则要修改每台机器中的hosts文件，添加三台Linux机器的IP地址和对应的主机名。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">172.20.77.47 Master</span><br><span class="line">172.20.77.48 Worker1</span><br><span class="line">172.20.77.49 Worker2</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出</p><p><b>2、产生密钥对：输入ssh-keygen -t rsa，之后按回车</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# ssh-keygen -t rsa</span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/root/.ssh/id_rsa): </span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /root/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /root/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:asnaNLVI1Nvw0XBXF+u6L3aWbdqwRtIdb6ZVDg/+wPs root@Worker1</span><br><span class="line">The key&apos;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">|          . . .o+|</span><br><span class="line">|       .   + .  o|</span><br><span class="line">|      . o . .  . |</span><br><span class="line">|     .   = .  +..|</span><br><span class="line">|      . S o  + B+|</span><br><span class="line">|     o = .  . B O|</span><br><span class="line">|      O .    +.Oo|</span><br><span class="line">|     = .     o=B+|</span><br><span class="line">|    . .     .oB=E|</span><br><span class="line">+----[SHA256]-----+</span><br></pre></td></tr></table></figure></p><p><b>3、把公钥分别拷给三台机器</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# ssh-copy-id -i .ssh/id_rsa.pub root@Worker2</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;.ssh/id_rsa.pub&quot;</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys</span><br><span class="line">root@worker2&apos;s password: </span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &apos;root@Worker2&apos;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br></pre></td></tr></table></figure></p><p><b>4、查看公钥</b><br>可以在Worker2这台机器上看到从Worker1传过来的公钥<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker2 ~]# ls .ssh/</span><br><span class="line">authorized_keys  id_rsa  id_rsa.pub</span><br><span class="line">[root@Worker2 ~]# more .ssh/authorized_keys</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDV4JxjxfQ5+juCguh+tJqy0YonZyIK0eyecSXynqKD</span><br><span class="line">YwNfgJ6uvwpDHHNffUSMoM7TjZrpF3g/Bu+ERFYn+YdJeMpmsftHlhiJgZoxDfQD/mAqmVV7XrQOgf9t</span><br><span class="line">tjfCjJFVYGSN00PdiH9s9xHkdoOVpNR1+jYBpA22AbtlOdKaz5eTluYO6klaz9VsEJ0gOu29l/CLyCHb</span><br><span class="line">ggTAryB/XHj/CFnPju71uaFCWdYpeqNxuc5SyUY6X5Oo88jK2Z/fyrwajHVh1i/nYHfr5bG4TEh6UUdr</span><br><span class="line">7EPgBQSekFik1gHHfXEt6hUcadRGUE7uRW38/VFothFcmtPyUMOm7o06lhN7 root@Worker1</span><br></pre></td></tr></table></figure></p><p>其中ssh-rsa表示使用RSA算法，中间的一长串字符串就是公钥，root@Worker1表示公钥是由Worker1这台机器产生的。</p><p><b>4、Worker1免密码登录Worker2</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# ssh Worker2</span><br><span class="line">Last login: Fri Aug 31 21:32:08 2018 from 183.212.160.91</span><br><span class="line"></span><br><span class="line">Welcome to Alibaba Cloud Elastic Compute Service !</span><br></pre></td></tr></table></figure></p><p><b>5、若要退出Worker2，返回Worker1，使用exit命令</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker2 ~]# exit</span><br><span class="line">logout</span><br><span class="line">Connection to worker2 closed.</span><br><span class="line">[root@Worker1 ~]#</span><br></pre></td></tr></table></figure></p><p>同样的道理，我这里有三台机器，若Worker1想要登录另外一台机器，就得将它的公钥拷贝给那台机器，就可以实现免密码登录；若Worker2想要登录其他机器，同样的重复如上步骤，先产生密钥对，然后将公钥拷贝给其他机器。<br><br><br>如果在配置过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;基本原理：采用非对称加密算法&lt;/b&gt;&lt;/font&gt;&lt;br&gt;1、产生一个密钥对：公钥（锁）、私钥（钥匙）&lt;br&gt;2、公钥：给别人来加密&lt;br&gt;3、私钥：给自己来解密&lt;br&gt;4、RSA算法是一种非对称加密算法&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Linux" scheme="https://www.ggstu.com/tags/Linux/"/>
    
  </entry>
  
</feed>
