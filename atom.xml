<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>GGSTU</title>
  
  <subtitle>Good Good Study</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.ggstu.com/"/>
  <updated>2018-09-03T07:20:36.594Z</updated>
  <id>https://www.ggstu.com/</id>
  
  <author>
    <name>Wu Zhenyong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark创建RDD的几种方式（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/03/Spark%E5%88%9B%E5%BB%BARDD%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/03/Spark创建RDD的几种方式（Java代码）/</id>
    <published>2018-09-03T05:02:48.000Z</published>
    <updated>2018-09-03T07:20:36.594Z</updated>
    
    <content type="html"><![CDATA[<p>Spark核心编程，首先要做的是创建一个初始的RDD。该RDD通常包含了Spark应用程序的输入源数据。只有在创建了初始的RDD之后，才可以使用transformation算子，对该RDD进行转换，得到其它的RDD。<br>Spark Core提供了三种创建RDD的方式：<br>1、使用程序中的集合创建RDD<br>2、使用本地文件创建RDD<br>3、使用HDFS文件创建RDD<br><a id="more"></a></p><p><font size="4"><b>并行化集合创建RDD</b></font><br>使用并行化集合来创建RDD，需要对集合调用SparkContext的parallelize()方法。Spark会将集合中的数据拷贝到集群上去，形成一个分布式的数据集，也就是一个RDD。<br>例如：累加1到10<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line"></span><br><span class="line">public class ParallelizeCollection &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;ParallelizeCollection&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numbers = Arrays.asList(1,2,3,4,5,6,7,8,9,10);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numberRDD = sc.parallelize(numbers,3);</span><br><span class="line"></span><br><span class="line">int sum = numberRDD.reduce(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer num1, Integer num2) throws Exception &#123;</span><br><span class="line">return num1 + num2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;1+2+3+......+10 = &quot; + sum);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行程序，在控制台上看到如下所示的结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-1.png" width="50%" height="50%"><br>调用parallelize()时，可以指定一个参数将集合切分成多少个partition。Spark会为每一个partition运行一个task。<br>Spark官方的建议是，为集群中的每个CPU创建2~4个partition。Spark会根据集群的情况来设置partition的数量。也可以在调用parallelize()方法时，传入第二个参数，设置RDD的partition数量，比如parallelize(numbers,3)。<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-2.png" width="100%" height="100%"><br><br></p><p><font size="4"><b>使用本地文件创建RDD</b></font><br>Spark支持使用任何Hadoop支持的存储系统上的文件创建RDD的，比如HDFS、Cassandra、HBase以及本地文件。<br>通过调用SparkContext的textFile()方法，对本地文件创建RDD。<br>例如：累加1到10<br>我在本地桌面上创建了个文本文件，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 2 3 4 5 6 7 8 9 10</span><br></pre></td></tr></table></figure></p><p>代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line"></span><br><span class="line">public class LocalFile &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;LocalFile&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus/Desktop//test.txt&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; nums = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">String sum = nums.reduce(new Function2&lt;String, String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public String call(String num1, String num2) throws Exception &#123;</span><br><span class="line">return (Integer.parseInt(num1) + Integer.parseInt(num2))+&quot;&quot;;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;1+2+3+4+......+10 = &quot; + sum);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行程序，在控制台上看到如下所示的结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-3.png" width="60%" height="60%"><br><br></p><p><font size="4"><b>使用HDFS文件创建RDD</b></font><br>通过调用SparkContext的textFile()方法，对HDFS文件创建RDD。<br>例如：累加1到10<br>在Linux上创建文本文件，文件内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd input/</span><br><span class="line">[root@ggstu input]# cat test.txt </span><br><span class="line">1 2 3 4 5 6 7 8 9 10</span><br></pre></td></tr></table></figure></p><p>启动hadoop集群，将test.txt上传到hdfs上<br>接着启动spark集群<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu input]# hdfs dfs -put test.txt /test.txt</span><br><span class="line">[root@ggstu input]# hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 root supergroup         21 2018-09-03 14:26 /test.txt</span><br></pre></td></tr></table></figure></p><p>代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line"></span><br><span class="line">public class HDFSFile &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;HDFSFile&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;hdfs://ggstu:9000/test.txt&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; nums = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">String sum = nums.reduce(new Function2&lt;String, String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public String call(String num1, String num2) throws Exception &#123;</span><br><span class="line">return (Integer.parseInt(num1)+Integer.parseInt(num2))+&quot;&quot;;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;1+2+3+4+5+......+10 = &quot; + sum);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>将程序打包成jar包，并上传到Linux上，之后执行执行脚本。<br>如果这几步不懂，在<a href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Java%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/" target="_blank">使用Java开发Spark的wordcount程序</a>这篇文章中有详细过程。<br>执行脚本结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-4.png" width="50%" height="50%"><br><br></p><p><font size="4"><b>其它创建RDD方式</b></font><br>Spark的官方文档中可以看到除了通过使用textFile()方法创建RDD，还有如下几种方式：<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-5.png" width="100%" height="100%"><br>1、SparkContext.wholeTextFiles方法，可以针对一个目录中大量的小文件，返回&lt;filename,content&gt;组成的pair，作为一个PairRDD，而不是普通的RDD。普通的textFile()返回的RDD是文件中的一行文本。<br>2、SparkContext.sequenceFile[K,V]方法，可以针对SequenceFile创建RDD，K和V泛型类型就是SequenceFile的key和value的类型。K和V要求必须是Hadoop的序列化类型，如IntWritable、Text等。<br>3、SparkContext.hadoopRDD方法，对于Hadoop的自定义输入类型，可以创建RDD。该方法接收JobConf、InputFormatClass、Key和Value的class。<br>4、SparkContext.objectFile方法，可以针对之前调用RDD.saveAsObjectFile创建的对象序列化的文件，反序列化文件中的数据，并创建一个RDD。</p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spark核心编程，首先要做的是创建一个初始的RDD。该RDD通常包含了Spark应用程序的输入源数据。只有在创建了初始的RDD之后，才可以使用transformation算子，对该RDD进行转换，得到其它的RDD。&lt;br&gt;Spark Core提供了三种创建RDD的方式：&lt;br&gt;1、使用程序中的集合创建RDD&lt;br&gt;2、使用本地文件创建RDD&lt;br&gt;3、使用HDFS文件创建RDD&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>使用Scala开发Spark的wordcount程序</title>
    <link href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Scala%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/"/>
    <id>https://www.ggstu.com/2018/09/02/使用Scala开发Spark的wordcount程序/</id>
    <published>2018-09-02T05:29:33.000Z</published>
    <updated>2018-09-02T12:31:31.768Z</updated>
    
    <content type="html"><![CDATA[<p>使用Scala IDE for Eclipse开发Scala程序，如果没有下载，点击如下链接进行下载<br><a href="http://scala-ide.org/" target="_blank">http://scala-ide.org/</a><br><a id="more"></a></p><p><b>1、打开Scala IDE for Eclipse，File-&gt;New-&gt;Project，然后找到如下图的Maven，创建Maven工程。</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/46-1.png" width="70%" height="70%"><br><b>2、直接点击Next</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/46-2.png" width="70%" height="70%"><br><b>3、选择maven-archetype-quickstart，然后点击Next</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/46-3.png" width="100%" height="100%"><br><b>4、添加Group ID和Artifact ID，然后点击Finish</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/46-4.png" width="100%" height="100%"></p><p>创建好的工程目录结构如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-5.png" width="40%" height="40%"><br><b>5、修改JRE</b><br>右键JRE System Library，选择Properties，修改JRE<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-6.png" width="100%" height="100%"><br><b>6、添加依赖</b><br>在pom.xml文件的dependencies块中添加如下依赖<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.7.7&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p><p><b>7、添加Scala属性</b><br>右键工程名-&gt;Configure-&gt;Add Scala Nature<br><b>8、修改Scala Library container</b><br>右键Scala Library container，选择Properties，修改为Fixed Scala Library container:2.11.11<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-8.png" width="70%" height="70%"><br><b>9、创建Scala Object</b><br>右键创建好的Package-&gt;New-&gt;Scala Object<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-7.png" width="70%" height="70%"><br><br></p><p><font size="4"><b>首先用Scala开发在本地运行的wordcount程序</b></font><br>在桌面创建了个test.txt文件，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p>执行的步骤与Java开发wordcount程序相同，步骤在Java开发wordcount程序那篇文章中写的比较详细，这里就不赘述了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">package com.ggstu.spark</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object WordCountLocal &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;WordCountLocal&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    val pairs = words.map(word =&gt; (word,1))</span><br><span class="line">    </span><br><span class="line">    val wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line">    </span><br><span class="line">    wordCounts.foreach(wordCount =&gt; println(wordCount._1 + &quot;: &quot; + wordCount._2))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>最后在本地Scala IDE运行程序，在控制台上看到如下所示的结果，即每个单词及其出现的频率。<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-9.png" width="40%" height="40%"><br><br></p><p><font size="4"><b>用Scala开发wordcount程序提交到Spark集群上运行</b></font><br>再创建一个Scala Object，命名为WordCountCluster</p><p><font size="4"><b>开发步骤：</b></font><br><b>1、创建数据源文件，并提交到hdfs上</b><br>在Linux上创建文件，并在文件中添加如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd /root/input/</span><br><span class="line">[root@ggstu input]# cat data.txt </span><br><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p>启动hadoop集群，将data.txt上传到hdfs上<br>接着启动spark集群<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu input]# hdfs dfs -put data.txt /wordcount.txt</span><br><span class="line">[root@ggstu input]# hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 root supergroup         27 2018-09-02 10:19 /wordcount.txt</span><br></pre></td></tr></table></figure></p><p><b>2、编写Scala代码</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package com.ggstu.spark</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object WordCountCluster &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;WordCountCluster&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;hdfs://ggstu:9000/wordcount.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    val pairs = words.map(word =&gt; (word,1))</span><br><span class="line">    </span><br><span class="line">    val wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line">    </span><br><span class="line">    wordCounts.foreach(wordCount =&gt; println(wordCount._1 + &quot;: &quot; + wordCount._2))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><b>3、将程序打包成jar包，并上传到Linux上</b><br>右击WordCountCluster.scala-&gt;Export，选择JAR file，然后点击Next<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-10.png" width="60%" height="60%"><br>选择jar包保存路径及其名称<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-11.png" width="60%" height="60%"><br>直接点击Next<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-12.png" width="60%" height="60%"><br>直接点击Finish<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-13.png" width="60%" height="60%"><br>这样，就可以在保存的路径下看到这个jar包</p><p>将jar包上传到事先在Linux创建好的目录中<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-14.png" width="100%" height="100%"><br><b>4、执行wordcount程序</b><br>使用vi编辑器创建执行脚本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jar]# vi wordcount.sh</span><br></pre></td></tr></table></figure></p><p>添加如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/root/software/spark-2.3.1-bin-hadoop2.7/bin/spark-submit \</span><br><span class="line">--class com.ggstu.spark.WordCountCluster \</span><br><span class="line">--num-executors 3 \</span><br><span class="line">--driver-memory 500m \</span><br><span class="line">--executor-memory 500m \</span><br><span class="line">--executor-cores 3 \</span><br><span class="line">/root/jar/WordCountCluster.jar \</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p>修改脚本的执行权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jar]# chmod 777 wordcount.sh</span><br></pre></td></tr></table></figure></p><p>执行脚本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jar]# ./wordcount.sh</span><br></pre></td></tr></table></figure></p><p>就可以在Linux机器上看到如下的打印结果，即每个单词及其出现的频率。<br><img src="http://pd8lpasbc.bkt.clouddn.com/46-15.png" width="60%" height="60%"></p><p>到这里，使用Scala开发Spark的wordcount程序，在本地上运行和在Spark集群上运行都实现完成了。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用Scala IDE for Eclipse开发Scala程序，如果没有下载，点击如下链接进行下载&lt;br&gt;&lt;a href=&quot;http://scala-ide.org/&quot; target=&quot;_blank&quot;&gt;http://scala-ide.org/&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>使用Java开发Spark的wordcount程序</title>
    <link href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Java%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/"/>
    <id>https://www.ggstu.com/2018/09/02/使用Java开发Spark的wordcount程序/</id>
    <published>2018-09-02T00:38:25.000Z</published>
    <updated>2018-09-02T04:10:39.750Z</updated>
    
    <content type="html"><![CDATA[<p>在开发wordcount程序前，需要搭建Maven工程并添加开发spark程序所需的依赖。<br><b>1、打开Eclipse，File-&gt;New-&gt;Project，然后找到如下图的Maven，创建Maven工程。</b><br><a id="more"></a><br><img src="http://pd8lpasbc.bkt.clouddn.com/45-1.png" width="70%" height="70%"><br><b>2、直接点击Next</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/45-2.png" width="70%" height="70%"><br><b>3、选择maven-archetype-quickstart，然后点击Next</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/45-3.png" width="70%" height="70%"><br><b>4、添加Group ID和Artifact ID，然后点击Finish</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/45-4.png" width="70%" height="70%"></p><p>创建好的工程目录结构如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-5.png" width="40%" height="40%"><br><b>5、修改JRE</b><br>右键JRE System Library，选择Properties，修改JRE<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-6.png" width="100%" height="100%"><br><b>6、添加依赖</b><br>pom.xml文件内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">  &lt;groupId&gt;com.ggstu&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;spark&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;</span><br><span class="line">  &lt;packaging&gt;jar&lt;/packaging&gt;</span><br><span class="line"></span><br><span class="line">  &lt;name&gt;spark&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.apache.org&lt;/url&gt;</span><br><span class="line"></span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;3.8.1&lt;/version&gt;</span><br><span class="line">      &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;2.3.0&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;2.7.7&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure></p><p><br></p><p><font size="4"><b>首先用Java开发在本地运行的wordcount程序</b></font><br>如下所示，一共有八个步骤：<br><b>第一步：</b>创建SparkConf对象，设置Spark应用的配置信息，setAppName()设置运行程序的名称，使用setMaster()可以设置Spark应用程序要连接的Spark集群的master结点的url，但是如果设置为local，表示在本地运行。<br><b>第二步：</b>创建JavaSparkContext对象，SparkContext是Spark所有功能的入口，主要作用包括初始化Spark应用程序所需的核心组件，包括调度器(DAGScheduler、TaskScheduler)，还会到Spark Master节点上进行注册，等等。<br>编写不同类型的Spark应用程序，使用的SparkContext是不同的。使用scala编写，就是原生的SparkContext对象。使用Java编写，就是JavaSparkContext对象。开发Spark SQL程序，就是SQLContext或HiveContext对象。开发Spark Streaming程序，就是它独有的SparkContext对象。<br><b>第三步：</b>SparkContext中，用于根据文件类型的输入源创建RDD的方法，叫做textFile。输入源可以是hdfs文件或本地文件。输入源中的数据会被打散，分配到RDD的每个partition中，从而形成一个初始的分布式的数据集。<br>这里开发wordcount程序，我在桌面建了个test.txt文件，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p><b>第四步：</b>对初始RDD进行transformation操作，也就是一些计算操作。通常操作通过创建function，并配合RDD的map、flatMap的等算子来执行。FlatMapFunction有两个泛型参数，分别代表了输入和输出类型。<br><b>第五步：</b>将每个单词映射为(单词,1)的这种格式。mapToPair是将每个元素映射为一个(v1,v2)这样的Tuple2类型的元素。mapToPair这个算子，与PairFunction配合使用，第一个泛型参数代表了输入类型，第二个和第三个泛型参数，代表的输出的Tuple2的第一个值和第二个值的类型。JavaPairRDD的两个泛型参数，分别代表了tuple元素的第一个值和第二个值的类型。<br><b>第六步：</b>以单词作为key，统计每个单词出现的次数。使用reduceByKey这个算子，对每个key对应的value进行reduce合并操作。最后返回的JavaPairRDD中的元素也是tuple，第一个值是每个key，第二个值是key的value合并之后的结果，即每个单词出现的次数。<br><b>第七步：</b>使用foreach这种action操作触发程序的执行。之前使用的flatMap、mapToPair、reduceByKey这种操作，都叫做transformation操作。一个Spark应用中，若只有transformation操作是不会执行的，必须要有action操作。<br><b>第八步：</b>关闭SparkContext对象。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">package com.ggstu.spark.core;</span><br><span class="line"></span><br><span class="line">import java.awt.List;</span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class WordCountLocal &#123;</span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">//第一步</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;WordCountLocal&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">//第二步</span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">//第三步</span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;);</span><br><span class="line"></span><br><span class="line">//第四步</span><br><span class="line">JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">//第五步</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; pairs = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;String, Integer&gt;(word, 1);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">//第六步</span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; wordCounts = pairs.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer v1, Integer v2) throws Exception &#123;</span><br><span class="line">return v1 + v2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">//第七步</span><br><span class="line">wordCounts.foreach(new VoidFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;String, Integer&gt; wordCount) throws Exception &#123;</span><br><span class="line">System.out.println(wordCount._1 + &quot;: &quot; + wordCount._2);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">//第八步</span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>最后在本地IDE运行程序，在控制台上看到如下所示的结果，即每个单词及其出现的频率。<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-7.png" width="70%" height="70%"><br><br></p><p><font size="4"><b>用Java开发wordcount程序提交到Spark集群上运行</b></font><br>如果要在spark集群上运行wordcount，代码与本地开发wordcount程序不同的只有两处：<br>一、将SparkConf的setMaster()方法删除掉，默认会去连接Spark集群。<br>二、文件路径不是本地文件，而是hadoop hdfs上的文件。</p><p><font size="4"><b>开发步骤：</b></font><br><b>1、创建数据源文件，并提交到hdfs上</b><br>在Linux上创建文件，并在文件中添加如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd /root/input/</span><br><span class="line">[root@ggstu input]# cat data.txt </span><br><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p>启动hadoop集群，将data.txt上传到hdfs上<br>接着启动spark集群<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu input]# hdfs dfs -put data.txt /wordcount.txt</span><br><span class="line">[root@ggstu input]# hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 root supergroup         27 2018-09-02 10:19 /wordcount.txt</span><br></pre></td></tr></table></figure></p><p><b>2、编写Java代码</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">package com.ggstu.spark.core;</span><br><span class="line"></span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class WordCountCluster &#123;</span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;WordCountCluster&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;hdfs://ggstu:9000/wordcount.txt&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; pairs = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;String, Integer&gt;(word, 1);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; wordCounts = pairs.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer v1, Integer v2) throws Exception &#123;</span><br><span class="line">return v1 + v2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">wordCounts.foreach(new VoidFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;String, Integer&gt; wordCount) throws Exception &#123;</span><br><span class="line">System.out.println(wordCount._1 + &quot;: &quot; + wordCount._2);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><b>3、将程序打包成jar包，并上传到Linux上</b><br>右击WordCountCluster.java-&gt;Export，选择JAR file，然后点击Next<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-8.png" width="60%" height="60%"><br>选择jar包保存路径及其名称<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-9.png" width="60%" height="60%"><br>直接点击Next<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-10.png" width="60%" height="60%"><br>选择主类，然后点击Finish<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-11.png" width="60%" height="60%"><br>这样，就可以在保存的路径下看到这个jar包</p><p>将jar包上传到事先在Linux创建好的目录中<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-12.png" width="100%" height="100%"><br><b>4、执行wordcount程序</b><br>使用vi编辑器创建执行脚本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jar]# vi wordcount.sh</span><br></pre></td></tr></table></figure></p><p>添加如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/root/software/spark-2.3.1-bin-hadoop2.7/bin/spark-submit \</span><br><span class="line">--class com.ggstu.spark.core.WordCountCluster \</span><br><span class="line">--num-executors 3 \</span><br><span class="line">--driver-memory 500m \</span><br><span class="line">--executor-memory 500m \</span><br><span class="line">--executor-cores 3 \</span><br><span class="line">/root/jar/WordCountCluster.jar \</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p>修改脚本的执行权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jar]# chmod 777 wordcount.sh</span><br></pre></td></tr></table></figure></p><p>执行脚本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jar]# ./wordcount.sh</span><br></pre></td></tr></table></figure></p><p>就可以在Linux机器上看到如下的打印结果，即每个单词及其出现的频率。<br><img src="http://pd8lpasbc.bkt.clouddn.com/45-13.png" width="60%" height="60%"></p><p>到这里，使用Java开发Spark的wordcount程序，在本地上运行和在Spark集群上运行都实现完成了。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在开发wordcount程序前，需要搭建Maven工程并添加开发spark程序所需的依赖。&lt;br&gt;&lt;b&gt;1、打开Eclipse，File-&amp;gt;New-&amp;gt;Project，然后找到如下图的Maven，创建Maven工程。&lt;/b&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>林润故居</title>
    <link href="https://www.ggstu.com/2018/09/02/%E6%9E%97%E6%B6%A6%E6%95%85%E5%B1%85/"/>
    <id>https://www.ggstu.com/2018/09/02/林润故居/</id>
    <published>2018-09-01T23:50:23.000Z</published>
    <updated>2018-09-01T14:57:39.213Z</updated>
    
    <content type="html"><![CDATA[<p>最近查看了下邮件，发现了我第一封发送的邮件。那是在高一，老师布置的语文作业，写一篇小几百字的描述校园一角的文章，写完后邮箱发给老师。就这样，它被存下来了。<br><a id="more"></a><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size="4"><b>温暖古朴的校园一角</b></font></p><p>&emsp;&emsp;从校外的栅栏往里望，看到了绿茵草地点缀着校园,看到了崭新建筑装饰着校园，看到了彩蝶扑粉美丽了校园。但是槛外人是不会发现，在那，在校园的一角，还有着一个“与世隔绝”的地方。那是古朴的林润故居。</p><p>&emsp;&emsp;首先吸引我眼球的不是它老式的建筑形式，而是从矮矮的墙头冒出的一簇花，“一枝红杏出墙来”，红色的花朵安静地守在故居的身边。即使红色是热烈奔放的象征，但它与古朴的小屋结合起来，又有与众不同的感觉。故居周围的树都不是很高大，或许是为了让这校园一角更多地展现在人们的眼前，以至于不被人们忽视。</p><p>&emsp;&emsp;推开木制的门，历史的气息扑面而来，脚下却被高高的门槛所绊到，门槛这么高，应该是为了防止雨天雨水涌入屋内所设计的。跨进屋内，首先映入眼帘的是大门对面一扇扇带孔的门，手从一扇扇门前划过，触摸到了历史的气息。从他们的建筑中我看到了那一代人的智慧，故居正门前的那个地方是被设计成低平的，角落处还有一个小洞通到屋外，这样无论雨下得多大，雨水都会流到这个低处，再顺着洞流出去。走进正屋，看着门前的几根木柱，有的掉了一小层皮，留下了岁月的痕迹。屋内的光线虽然不亮，但还是可以清楚地看到林润的雕像被放在了屋子的正中央，左右两侧还有一对金色的对联。环顾周围的一切，其实它的空间很小，大概只有一间教室的大小，但是小也有它的好处，能够营造一种温暖的感觉，家的味道。</p><p>&emsp;&emsp;踏出门槛，掩上木门，看着眼前一排排整齐的现代建筑，与身后的林润故居可是天壤之别，但是校园内有了这一角故居，更会给人温暖的感觉。门口的水泥道路，在林润故居的陪衬下，更是大家茶余饭后散步的绝佳地带。有了这古朴的一角，才更有温暖的感觉。</p><p>&emsp;&emsp;古朴的林润故居，你虽然占据的仅是校园的一角，带来的却是无穷的温暖。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近查看了下邮件，发现了我第一封发送的邮件。那是在高一，老师布置的语文作业，写一篇小几百字的描述校园一角的文章，写完后邮箱发给老师。就这样，它被存下来了。&lt;br&gt;
    
    </summary>
    
      <category term="随便说说" scheme="https://www.ggstu.com/categories/%E9%9A%8F%E4%BE%BF%E8%AF%B4%E8%AF%B4/"/>
    
    
      <category term="心情" scheme="https://www.ggstu.com/tags/%E5%BF%83%E6%83%85/"/>
    
  </entry>
  
  <entry>
    <title>Spark2.3.1版本全分布模式的安装与部署</title>
    <link href="https://www.ggstu.com/2018/09/01/Spark2-3-1%E7%89%88%E6%9C%AC%E5%85%A8%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2/"/>
    <id>https://www.ggstu.com/2018/09/01/Spark2-3-1版本全分布模式的安装与部署/</id>
    <published>2018-09-01T13:31:02.000Z</published>
    <updated>2018-09-01T14:25:16.968Z</updated>
    
    <content type="html"><![CDATA[<p>部署Spark的全分布模式，至少需要三台Linux机器，首先要有一些准备工作，就是配置Hadoop的全分布环境。我准备了三台机器，分别命名为Master，Worker1，Worker2<br>如果不清楚如何配置Hadoop的全分布环境，点击如下链接进行配置：<br><a href="https://www.ggstu.com/2018/08/31/Hadoop2-X%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%EF%BC%88%E5%9B%9B%EF%BC%89%E5%85%A8%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F/" target="_blank">Hadoop2.X的安装与配置（全分布模式）</a><br><a id="more"></a><br>配置完Hadoop全分布环境后，接下来开始安装配置Spark2.3.1版本全分布模式，这里部署的全分布模式是standalone模式。</p><p><font size="4"><b>接下来开始进行配置，在主节点Master上进行配置，然后把配置好的文件复制到其它机器</b></font><br><b>1、在Windows上下载Spark2.3.1安装包</b><br>点击如下链接进行下载：<br>由于配置的Hadoop版本是2.7.7,所以下载spark-2.3.1-bin-hadoop2.7.tgz这个压缩包<br><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">Spark官网下载地址</a></p><p><b>2、将安装包上传到Linux上，并解压缩</b><br>使用WinSCP将压缩包上传到事先创建好的tools目录下<br><img src="http://pd8lpasbc.bkt.clouddn.com/43-1.png" width="100%" height="100%"><br>使用SecureCRT远程连接上Linux，将压缩包解压到事先创建好的software目录下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# cd tools/</span><br><span class="line">[root@Master tools]# ls</span><br><span class="line">hadoop-2.7.7.tar.gz  jdk-8u131-linux-x64.tar.gz  spark-2.3.1-bin-hadoop2.7.tgz</span><br><span class="line">[root@Master tools]# tar -zxvf spark-2.3.1-bin-hadoop2.7.tgz -C ~/software/</span><br></pre></td></tr></table></figure></p><p><b>3、修改spark-env.sh配置文件</b><br>使用vi编辑器修改spark-env.sh配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Master tools]# cd ~/software/spark-2.3.1-bin-hadoop2.7/conf/</span><br><span class="line">[root@Master conf]# cp spark-env.sh.template spark-env.sh</span><br><span class="line">[root@Master conf]# vi spark-env.sh</span><br></pre></td></tr></table></figure></p><p>在spark-env.sh文件末尾添加如下内容：分别是JAVA_HOME，主机名和端口号<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/root/software/jdk1.8.0_131</span><br><span class="line">export SPARK_MASTER_HOST=Master</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure></p><p>这里添加的是你自己那台机器的JAVA_HOME路径，主机名，端口号不变，就是7077。<br>添加完成后，保存退出。</p><p><b>4、修改slaves配置文件</b><br>使用vi编辑器修改slaves配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@Master conf]# cp slaves.template slaves</span><br><span class="line">[root@Master conf]# vi slaves</span><br></pre></td></tr></table></figure></p><p>删除slaves文件末尾的localhost，并添加从节点的主机名:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Worker1</span><br><span class="line">Worker2</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p><b>5、把主节点上配置好的spark环境复制到从节点上</b><br>由于配置了SSH免密码登录，所以这里复制到其他机器上不需要输入密码。<br>复制到第一个从节点Worker1上<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# cd software/</span><br><span class="line">[root@Master software]# scp -r spark-2.3.1-bin-hadoop2.7/ root@Worker1:/root/software/</span><br></pre></td></tr></table></figure></p><p>复制到第二个从节点Worker2上<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master software]# scp -r spark-2.3.1-bin-hadoop2.7/ root@Worker2:/root/software/</span><br></pre></td></tr></table></figure></p><p>可以在从节点Worker1上看到复制过来的spark-2.3.1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# cd software/</span><br><span class="line">[root@Worker1 software]# ls</span><br><span class="line">hadoop-2.7.7  jdk1.8.0_131  spark-2.3.1-bin-hadoop2.7</span><br></pre></td></tr></table></figure></p><p>可以在从节点Worker2上看到复制过来的spark-2.3.1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker2 ~]# cd software/</span><br><span class="line">[root@Worker2 software]# ls</span><br><span class="line">hadoop-2.7.7  jdk1.8.0_131  spark-2.3.1-bin-hadoop2.7</span><br></pre></td></tr></table></figure></p><p>到这里Spark的全分布模式的配置就完成了。<br><br></p><p><font size="4"><b>启动Spark全分布集群：</b></font><br>由于hadoop和spark命令启动脚本start-all.sh有冲突，所以在系统的环境变量配置文件bash_profile中，不配置Spark的环境。<br>启动Spark，在主节点Master上，进入spark目录中进行启动。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# cd ~/software/spark-2.3.1-bin-hadoop2.7/</span><br><span class="line">[root@Master spark-2.3.1-bin-hadoop2.7]# sbin/start-all.sh </span><br><span class="line">starting org.apache.spark.deploy.master.Master, logging to /root/software/spark-2.3.1-bin-hadoop2.7/logs/spark-root-org.apache.spark.deploy.master.Master-1-Master.out</span><br><span class="line">Worker2: starting org.apache.spark.deploy.worker.Worker, logging to /root/software/spark-2.3.1-bin-hadoop2.7/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-Worker2.out</span><br><span class="line">Worker1: starting org.apache.spark.deploy.worker.Worker, logging to /root/software/spark-2.3.1-bin-hadoop2.7/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-Worker1.out</span><br></pre></td></tr></table></figure></p><p>从打印出的日志可以看到，在主节点上启动了Master，两个从节点上各自启动了一个Worker。<br>因为在系统的环境变量配置文件bash_profile中配置了Hadoop的环境，所以除了在spark-2.3.1-bin-hadoop2.7/sbin目录下使用start-all.sh命令启动的是Spark集群的全分布环境，其它任何位置使用start-all.sh命令启动的是Hadoop集群的全分布环境。</p><p>使用jps命令查看后台进程，也可以看到各个节点启动的进程。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# jps</span><br><span class="line">18576 Jps</span><br><span class="line">18510 Master</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# jps</span><br><span class="line">14097 Jps</span><br><span class="line">14026 Worker</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker2 ~]# jps</span><br><span class="line">13669 Worker</span><br><span class="line">13739 Jps</span><br></pre></td></tr></table></figure><p><br><br>若要查看集群的基本信息，可以在网页中输入主节点IP:8080端口进行访问。</p><p><font color="#f00">注意：</font>若使用购买的Linux服务器，这里的IP为公网IP。<br><img src="http://pd8lpasbc.bkt.clouddn.com/43-2.png" width="100%" height="100%"><br>在这个网页中也可以看到此时有两个从节点Worker<br><br><br>若要关闭Spark集群，在主节点使用如下命令关闭<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Master spark-2.3.1-bin-hadoop2.7]# sbin/stop-all.sh </span><br><span class="line">Worker2: stopping org.apache.spark.deploy.worker.Worker</span><br><span class="line">Worker1: stopping org.apache.spark.deploy.worker.Worker</span><br><span class="line">stopping org.apache.spark.deploy.master.Master</span><br></pre></td></tr></table></figure></p><p>到这里Spark2.3.1版本全分布模式的配置，以及启动关闭就介绍完了。<br><br>如果在安装过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;部署Spark的全分布模式，至少需要三台Linux机器，首先要有一些准备工作，就是配置Hadoop的全分布环境。我准备了三台机器，分别命名为Master，Worker1，Worker2&lt;br&gt;如果不清楚如何配置Hadoop的全分布环境，点击如下链接进行配置：&lt;br&gt;&lt;a href=&quot;https://www.ggstu.com/2018/08/31/Hadoop2-X%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%EF%BC%88%E5%9B%9B%EF%BC%89%E5%85%A8%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F/&quot; target=&quot;_blank&quot;&gt;Hadoop2.X的安装与配置（全分布模式）&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark2.3.1版本伪分布模式的安装与部署</title>
    <link href="https://www.ggstu.com/2018/09/01/Spark2-3-1%E7%89%88%E6%9C%AC%E4%BC%AA%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2/"/>
    <id>https://www.ggstu.com/2018/09/01/Spark2-3-1版本伪分布模式的安装与部署/</id>
    <published>2018-09-01T02:07:44.000Z</published>
    <updated>2018-09-01T03:28:02.803Z</updated>
    
    <content type="html"><![CDATA[<p><br><br>部署Spark的伪分布环境，需要一台Linux机器，首先要有一些准备工作，就是配置Hadoop的伪分布环境。<br>如果不清楚如何配置Hadoop的伪分布环境，点击如下链接进行配置：<br><a href="https://www.ggstu.com/2018/08/31/Hadoop2-X%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%EF%BC%88%E4%B8%89%EF%BC%89%E4%BC%AA%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F/" target="_blank">Hadoop2.X的安装与配置（伪分布模式）</a></p><p>配置完Hadoop伪分布环境后，接下来开始安装配置Spark2.3.1版本伪分布模式，这里部署的伪分布模式是standalone模式。<br><a id="more"></a></p><p><font size="4"><b>配置过程：</b></font><br><b>1、在Windows上下载Spark2.3.1安装包</b><br>点击如下链接进行下载：<br>由于配置的Hadoop版本是2.7.7,所以下载spark-2.3.1-bin-hadoop2.7.tgz这个压缩包<br><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">Spark官网下载地址</a></p><p><b>2、将安装包上传到Linux上，并解压缩</b><br>使用WinSCP将压缩包上传到事先创建好的tools目录下<br><img src="http://pd8lpasbc.bkt.clouddn.com/42-1.png" width="100%" height="100%"><br>使用SecureCRT远程连接上Linux，将压缩包解压到事先创建好的software目录下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd tools/</span><br><span class="line">[root@ggstu tools]# ls</span><br><span class="line">hadoop-2.7.7.tar.gz  jdk-8u131-linux-x64.tar.gz  spark-2.3.1-bin-hadoop2.7.tgz</span><br><span class="line">[root@ggstu tools]# tar -zxvf spark-2.3.1-bin-hadoop2.7.tgz -C ~/software/</span><br></pre></td></tr></table></figure></p><p><b>3、修改spark-env.sh配置文件</b><br>使用vi编辑器修改spark-env.sh配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu tools]# cd ~/software/spark-2.3.1-bin-hadoop2.7/conf/</span><br><span class="line">[root@ggstu conf]# cp spark-env.sh.template spark-env.sh</span><br><span class="line">[root@ggstu conf]# vi spark-env.sh</span><br></pre></td></tr></table></figure></p><p>在spark-env.sh文件末尾添加如下内容：分别是JAVA_HOME，主机名和端口号<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/root/software/jdk1.8.0_131</span><br><span class="line">export SPARK_MASTER_HOST=ggstu</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure></p><p>这里添加的是你自己那台机器的JAVA_HOME路径，主机名，端口号不变，就是7077。<br>添加完成后，保存退出。</p><p><b>4、修改slaves配置文件</b><br>使用vi编辑器修改slaves配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu conf]# cp slaves.template slaves</span><br><span class="line">[root@ggstu conf]# vi slaves</span><br></pre></td></tr></table></figure></p><p>删除slaves文件末尾的localhost，并添加这台主机的主机名:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ggstu</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p>到这里Spark的伪分布模式的配置就完成了。<br><br></p><p><font size="4"><b>启动Spark伪分布集群：</b></font><br>由于hadoop和spark命令启动脚本start-all.sh有冲突，所以在系统的环境变量配置文件bash_profile中，不配置Spark的环境。<br>启动Spark，进入spark目录中进行启动。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd ~/software/spark-2.3.1-bin-hadoop2.7/</span><br><span class="line">[root@ggstu spark-2.3.1-bin-hadoop2.7]# sbin/start-all.sh </span><br><span class="line">starting org.apache.spark.deploy.master.Master, logging to /root/software/spark-2.3.1-bin-hadoop2.7/logs/spark-root-org.apache.spark.deploy.master.Master-1-ggstu.out</span><br><span class="line">ggstu: starting org.apache.spark.deploy.worker.Worker, logging to /root/software/spark-2.3.1-bin-hadoop2.7/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-ggstu.out</span><br></pre></td></tr></table></figure></p><p>因为在系统的环境变量配置文件bash_profile中配置了Hadoop的环境，所以除了在spark-2.3.1-bin-hadoop2.7/sbin目录下使用start-all.sh命令启动的是Spark集群的伪分布环境，其它任何位置使用start-all.sh命令启动的是Hadoop集群的伪分布环境。</p><p>使用jps命令查看后台进程，看到启动了Spark的主节点Master和从节点Worker<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu spark-2.3.1-bin-hadoop2.7]# jps</span><br><span class="line">11858 Jps</span><br><span class="line">11736 Master</span><br><span class="line">11805 Worker</span><br></pre></td></tr></table></figure></p><p>若要查看集群的基本信息，可以在网页中输入IP:8080端口进行访问。</p><p><font color="#f00">注意：</font>若使用购买的Linux服务器，这里的IP为公网IP。<br><img src="http://pd8lpasbc.bkt.clouddn.com/42-2.png" width="100%" height="100%"><br>若要关闭Spark集群，使用如下命令关闭<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu spark-2.3.1-bin-hadoop2.7]# sbin/stop-all.sh </span><br><span class="line">ggstu: stopping org.apache.spark.deploy.worker.Worker</span><br><span class="line">stopping org.apache.spark.deploy.master.Master</span><br></pre></td></tr></table></figure></p><p>到这里Spark2.3.1版本伪分布模式的配置，以及启动关闭就介绍完了。</p><p>如果在安装过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;br&gt;&lt;br&gt;部署Spark的伪分布环境，需要一台Linux机器，首先要有一些准备工作，就是配置Hadoop的伪分布环境。&lt;br&gt;如果不清楚如何配置Hadoop的伪分布环境，点击如下链接进行配置：&lt;br&gt;&lt;a href=&quot;https://www.ggstu.com/2018/08/31/Hadoop2-X%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%EF%BC%88%E4%B8%89%EF%BC%89%E4%BC%AA%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F/&quot; target=&quot;_blank&quot;&gt;Hadoop2.X的安装与配置（伪分布模式）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;配置完Hadoop伪分布环境后，接下来开始安装配置Spark2.3.1版本伪分布模式，这里部署的伪分布模式是standalone模式。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Linux下NTP时间服务器的配置与搭建</title>
    <link href="https://www.ggstu.com/2018/08/31/Linux%E4%B8%8BNTP%E6%97%B6%E9%97%B4%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E6%90%AD%E5%BB%BA/"/>
    <id>https://www.ggstu.com/2018/08/31/Linux下NTP时间服务器的配置与搭建/</id>
    <published>2018-08-31T13:56:39.000Z</published>
    <updated>2018-08-31T14:11:12.915Z</updated>
    
    <content type="html"><![CDATA[<p>NTP服务[Network Time Protocol]是用来使计算机时间同步化的一种协议，它可以使计算机对其服务器或时钟源（如石英钟，GPS等等)做同步化，它可以提供高精准度的时间校正，且可介由加密确认的方式来防止恶毒的协议攻击。<br><a id="more"></a></p><p><font size="4"><b>在Linux上配置NTP服务器来实现多台机器间的时间同步步骤：</b></font><br>分别在每台机器上执行如下命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install ntp</span><br><span class="line">systemctl enable ntpd</span><br><span class="line">systemctl start ntpd</span><br><span class="line">systemctl is-enabled ntpd</span><br></pre></td></tr></table></figure></p><p><br></p><p><font size="4"><b>其中一台机器的执行过程如下：</b></font><br><b>1、安装NTP服务器</b><br>安装中出现会Is this ok [y/d/N]:     输入 y即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# yum install ntp</span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">base                                                     | 3.6 kB     00:00     </span><br><span class="line">epel                                                     | 3.2 kB     00:00     </span><br><span class="line">extras                                                   | 3.4 kB     00:00     </span><br><span class="line">updates                                                  | 3.4 kB     00:00     </span><br><span class="line">(1/7): base/7/x86_64/group_gz                              | 166 kB   00:00     </span><br><span class="line">(2/7): epel/x86_64/group_gz                                |  88 kB   00:00     </span><br><span class="line">(3/7): epel/x86_64/updateinfo                              | 938 kB   00:00     </span><br><span class="line">(4/7): extras/7/x86_64/primary_db                          | 187 kB   00:00     </span><br><span class="line">(5/7): epel/x86_64/primary                                 | 3.6 MB   00:00     </span><br><span class="line">(6/7): base/7/x86_64/primary_db                            | 5.9 MB   00:00     </span><br><span class="line">(7/7): updates/7/x86_64/primary_db                         | 5.2 MB   00:00     </span><br><span class="line">Determining fastest mirrors</span><br><span class="line">epel                                                                12662/12662</span><br><span class="line">Resolving Dependencies</span><br><span class="line">--&gt; Running transaction check</span><br><span class="line">---&gt; Package ntp.x86_64 0:4.2.6p5-25.el7.centos.2 will be updated</span><br><span class="line">---&gt; Package ntp.x86_64 0:4.2.6p5-28.el7.centos will be an update</span><br><span class="line">--&gt; Processing Dependency: ntpdate = 4.2.6p5-28.el7.centos for package: ntp-4.2.6p5-28.el7.centos.x86_64</span><br><span class="line">--&gt; Running transaction check</span><br><span class="line">---&gt; Package ntpdate.x86_64 0:4.2.6p5-25.el7.centos.2 will be updated</span><br><span class="line">---&gt; Package ntpdate.x86_64 0:4.2.6p5-28.el7.centos will be an update</span><br><span class="line">--&gt; Finished Dependency Resolution</span><br><span class="line"></span><br><span class="line">Dependencies Resolved</span><br><span class="line"></span><br><span class="line">================================================================================</span><br><span class="line"> Package        Arch          Version                         Repository   Size</span><br><span class="line">================================================================================</span><br><span class="line">Updating:</span><br><span class="line"> ntp            x86_64        4.2.6p5-28.el7.centos           base        549 k</span><br><span class="line">Updating for dependencies:</span><br><span class="line"> ntpdate        x86_64        4.2.6p5-28.el7.centos           base         86 k</span><br><span class="line"></span><br><span class="line">Transaction Summary</span><br><span class="line">================================================================================</span><br><span class="line">Upgrade  1 Package (+1 Dependent package)</span><br><span class="line"></span><br><span class="line">Total download size: 635 k</span><br><span class="line">Is this ok [y/d/N]: y</span><br><span class="line">Downloading packages:</span><br><span class="line">Delta RPMs disabled because /usr/bin/applydeltarpm not installed.</span><br><span class="line">(1/2): ntpdate-4.2.6p5-28.el7.centos.x86_64.rpm            |  86 kB   00:00     </span><br><span class="line">(2/2): ntp-4.2.6p5-28.el7.centos.x86_64.rpm                | 549 kB   00:00     </span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">Total                                              3.8 MB/s | 635 kB  00:00     </span><br><span class="line">Running transaction check</span><br><span class="line">Running transaction test</span><br><span class="line">Transaction test succeeded</span><br><span class="line">Running transaction</span><br><span class="line">  Updating   : ntpdate-4.2.6p5-28.el7.centos.x86_64                         1/4 </span><br><span class="line">  Updating   : ntp-4.2.6p5-28.el7.centos.x86_64                             2/4 </span><br><span class="line">  Cleanup    : ntp-4.2.6p5-25.el7.centos.2.x86_64                           3/4 </span><br><span class="line">  Cleanup    : ntpdate-4.2.6p5-25.el7.centos.2.x86_64                       4/4 </span><br><span class="line">  Verifying  : ntpdate-4.2.6p5-28.el7.centos.x86_64                         1/4 </span><br><span class="line">  Verifying  : ntp-4.2.6p5-28.el7.centos.x86_64                             2/4 </span><br><span class="line">  Verifying  : ntp-4.2.6p5-25.el7.centos.2.x86_64                           3/4 </span><br><span class="line">  Verifying  : ntpdate-4.2.6p5-25.el7.centos.2.x86_64                       4/4 </span><br><span class="line"></span><br><span class="line">Updated:</span><br><span class="line">  ntp.x86_64 0:4.2.6p5-28.el7.centos                                            </span><br><span class="line"></span><br><span class="line">Dependency Updated:</span><br><span class="line">  ntpdate.x86_64 0:4.2.6p5-28.el7.centos                                        </span><br><span class="line"></span><br><span class="line">Complete!</span><br></pre></td></tr></table></figure></p><p><b>2、开启NTP服务器</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# systemctl enable ntpd</span><br><span class="line">[root@Worker1 ~]# systemctl start ntpd</span><br><span class="line">[root@Worker1 ~]# systemctl is-enabled ntpd</span><br><span class="line">enabled</span><br></pre></td></tr></table></figure></p><p><br><br>这样，第一台机器上的NTP服务器就安装好了，在剩下的机器上安装NTP服务器，重复以上步骤即可。<br>在所有的机器上安装并配置好NTP服务器后，这些机器的时间就同步了。<br><br><br>如果在安装过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NTP服务[Network Time Protocol]是用来使计算机时间同步化的一种协议，它可以使计算机对其服务器或时钟源（如石英钟，GPS等等)做同步化，它可以提供高精准度的时间校正，且可介由加密确认的方式来防止恶毒的协议攻击。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Linux" scheme="https://www.ggstu.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop2.X的安装与配置（四）全分布模式</title>
    <link href="https://www.ggstu.com/2018/08/31/Hadoop2-X%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%EF%BC%88%E5%9B%9B%EF%BC%89%E5%85%A8%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F/"/>
    <id>https://www.ggstu.com/2018/08/31/Hadoop2-X的安装与配置（四）全分布模式/</id>
    <published>2018-08-31T11:54:45.000Z</published>
    <updated>2018-09-01T01:22:13.815Z</updated>
    
    <content type="html"><![CDATA[<p>配置全分布模式，至少需要三台Linux机器，我这里就以3台结点为例，配置全分布环境。<br>三台Linux机器，都完成准备阶段的配置后还需要如下的三个步骤，之后才开始配置全分布环境。<br>1、配置主机名<br>使用vi编辑器修改/etc/hosts文件，在配置文件中要添加三台Linux机器的IP地址和对应的主机名。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">172.20.77.47 Master</span><br><span class="line">172.20.77.48 Worker1</span><br><span class="line">172.20.77.49 Worker2</span><br></pre></td></tr></table></figure></p><a id="more"></a><p><font color="#f00">注意：</font>如果使用的是购买的Linux服务器，这里的IP是内网IP。<br>添加完成后，保存退出</p><p>2、对三台机器两两之间配置SSH免密码登录<br>如果不清楚如何配置免密码登录，点击如下链接进行配置：<br><a href="https://www.ggstu.com/2018/08/31/SSH%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E5%9C%A8Linux%E4%B8%8A%E7%9A%84%E9%85%8D%E7%BD%AE/" target="_blank">SSH免密码登录在Linux上的配置</a></p><p>3、保证每台机器的时间同步<br>在每台机器上配置NTP时间服务器保证多台机器间的时间同步，如果不清楚如何配置NTP时间服务器，点击如下链接进行配置：<br><a href="https://www.ggstu.com/2018/08/31/Linux%E4%B8%8BNTP%E6%97%B6%E9%97%B4%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E6%90%AD%E5%BB%BA/" target="_blank">Linux下NTP时间服务器的配置与搭建</a><br><br><br>完成准备工作完成后，接着就可以开始配置全分布模式。<br><img src="http://pd8lpasbc.bkt.clouddn.com/40-5.png" width="60%" height="60%"><br><br><br><b>全分布模式的配置需要在如下的配置文件中添加配置参数。</b><br><img src="http://pd8lpasbc.bkt.clouddn.com/40-1.png" width="100%" height="100%"><br>这些参数的作用，在伪分布模式的配置中介绍过了，这里唯一不同的就是多了个slaves配置文件，它的作用是配置从节点的地址。<br><br></p><p><font size="4"><b>接下来开始进行配置，在主节点Master上进行配置，然后把配置好的文件复制到其它机器</b></font><br><b>1、修改hadoop-env.sh配置文件</b><br>使用vi编辑器修改hadoop-env.sh配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# cd software/hadoop-2.7.7/etc/hadoop/</span><br><span class="line">[root@Master hadoop]# vi hadoop-env.sh</span><br></pre></td></tr></table></figure></p><p>:set number打开行号，在hadoop-env.sh配置文件中添加JAVA_HOME路径<br><img src="http://pd8lpasbc.bkt.clouddn.com/40-2.png" width="70%" height="70%"><br>添加完成后，保存退出。</p><p><b>2、修改hdfs-site.xml配置文件</b><br>使用vi编辑器修改hdfs-site.xml配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master hadoop]# vi hdfs-site.xml</span><br></pre></td></tr></table></figure></p><p>在configuration代码块中添加如下的两个property代码块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.permissions&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p><b>3、修改core-site.xml配置文件</b><br>创建hdfs数据保存的目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master hadoop]# mkdir /root/software/hadoop-2.7.7/tmp</span><br></pre></td></tr></table></figure></p><p>使用vi编辑器修改core-site.xml配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master hadoop]# vi core-site.xml</span><br></pre></td></tr></table></figure></p><p>在configuration代码块中添加如下的两个property代码块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://Master:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/root/software/hadoop-2.7.7/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p><b>4、修改mapred-site.xml配置文件</b><br>使用vi编辑器修改mapred-site.xml配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@Master hadoop]# cp mapred-site.xml.template mapred-site.xml</span><br><span class="line">[root@Master hadoop]# vi mapred-site.xml</span><br></pre></td></tr></table></figure></p><p>在configuration代码块中添加如下这个property代码块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p><b>5、修改yarn-site.xml配置文件</b><br>使用vi编辑器修改yarn-site.xml配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master hadoop]# vi yarn-site.xml</span><br></pre></td></tr></table></figure></p><p>在configuration代码块中添加如下的两个property代码块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;Master&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p><b>6、修改slaves配置文件</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master hadoop]# vi slaves</span><br></pre></td></tr></table></figure></p><p>删除文件中的内容，并将从节点添加到此文件中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Worker1</span><br><span class="line">Worker2</span><br></pre></td></tr></table></figure></p><p><b>7、对主节点NameNode格式化</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master hadoop]# hdfs namenode -format</span><br></pre></td></tr></table></figure></p><p>在打印的日志中，若看到如下这一条，说明格式化成功。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">18/08/31 23:48:47 INFO common.Storage: Storage directory /root/software/hadoop-2.7.7/tmp/dfs/name has been successfully formatted.</span><br></pre></td></tr></table></figure></p><p><b>8、把主节点上配置好的hadoop环境复制到从节点上</b><br>由于配置了SSH免密码登录，所以这里复制到其他机器上不需要输入密码。<br>复制到第一个从节点Worker1上<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master software]# scp -r hadoop-2.7.7/ root@Worker1:/root/software/</span><br></pre></td></tr></table></figure></p><p>复制到第二个从节点Worker2上<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master software]# scp -r hadoop-2.7.7/ root@Worker2:/root/software/</span><br></pre></td></tr></table></figure></p><p>可以在从节点Worker1上看到复制过来的hadoop-2.7.7<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# cd software/</span><br><span class="line">[root@Worker1 software]# ls</span><br><span class="line">hadoop-2.7.7  jdk1.8.0_131</span><br></pre></td></tr></table></figure></p><p>可以在从节点Worker2上看到复制过来的hadoop-2.7.7<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker2 ~]# cd software/</span><br><span class="line">[root@Worker2 software]# ls</span><br><span class="line">hadoop-2.7.7  jdk1.8.0_131</span><br></pre></td></tr></table></figure></p><p>到这里Hadoop的全分布模式的配置就完成了。<br><br></p><p><font size="4"><b>启动Hadoop全分布集群：</b></font><br>在主节点上启动集群，首次启动需要在启动的过程中输入一次yes<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# start-all.sh</span><br><span class="line">This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh</span><br><span class="line">Starting namenodes on [Master]</span><br><span class="line">Master: starting namenode, logging to /root/software/hadoop-2.7.7/logs/hadoop-root-namenode-Master.out</span><br><span class="line">Worker1: starting datanode, logging to /root/software/hadoop-2.7.7/logs/hadoop-root-datanode-Worker1.out</span><br><span class="line">Worker2: starting datanode, logging to /root/software/hadoop-2.7.7/logs/hadoop-root-datanode-Worker2.out</span><br><span class="line">Starting secondary namenodes [0.0.0.0]</span><br><span class="line">The authenticity of host &apos;0.0.0.0 (0.0.0.0)&apos; can&apos;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:ygoutFzn8lgE6ObOfMEpysiW7fzow7qA5I+32p3K0bU.</span><br><span class="line">ECDSA key fingerprint is MD5:92:5c:41:87:ba:b5:a3:83:f3:4a:86:87:61:0a:cc:5f.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">0.0.0.0: Warning: Permanently added &apos;0.0.0.0&apos; (ECDSA) to the list of known hosts.</span><br><span class="line">0.0.0.0: starting secondarynamenode, logging to /root/software/hadoop-2.7.7/logs/hadoop-root-secondarynamenode-Master.out</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /root/software/hadoop-2.7.7/logs/yarn-root-resourcemanager-Master.out</span><br><span class="line">Worker1: starting nodemanager, logging to /root/software/hadoop-2.7.7/logs/yarn-root-nodemanager-Worker1.out</span><br><span class="line">Worker2: starting nodemanager, logging to /root/software/hadoop-2.7.7/logs/yarn-root-nodemanager-Worker2.out</span><br></pre></td></tr></table></figure></p><p>从打印出的日志可以看到namenode、secondarynamenode、resourcemanager在主节点Master上，datanode、nodemanager在从节点上。<br>使用jps命令查看后台进程，也可以看到各个节点启动的进程。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# jps</span><br><span class="line">13139 ResourceManager</span><br><span class="line">12803 NameNode</span><br><span class="line">13429 Jps</span><br><span class="line">12989 SecondaryNameNode</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# jps</span><br><span class="line">11410 DataNode</span><br><span class="line">11655 Jps</span><br><span class="line">11514 NodeManager</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker2 ~]# jps</span><br><span class="line">11584 Jps</span><br><span class="line">11444 NodeManager</span><br><span class="line">11340 DataNode</span><br></pre></td></tr></table></figure><p><br></p><p>若要查看集群的基本信息，可以在网页中输入IP:50070端口进行访问。</p><p><font color="#f00">注意：</font>若使用购买的Linux服务器，这里的IP为公网IP。<br><img src="http://pd8lpasbc.bkt.clouddn.com/40-3.png" width="100%" height="100%"></p><p>若要查看运行的MapReduce程序，可以在网页中输入IP:8088端口进行访问。<br><img src="http://pd8lpasbc.bkt.clouddn.com/40-4.png" width="100%" height="100%"></p><p>若要关闭集群，在主节点使用如下命令关闭<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# stop-all.sh</span><br><span class="line">This script is Deprecated. Instead use stop-dfs.sh and stop-yarn.sh</span><br><span class="line">Stopping namenodes on [Master]</span><br><span class="line">Master: stopping namenode</span><br><span class="line">Worker2: stopping datanode</span><br><span class="line">Worker1: stopping datanode</span><br><span class="line">Stopping secondary namenodes [0.0.0.0]</span><br><span class="line">0.0.0.0: stopping secondarynamenode</span><br><span class="line">stopping yarn daemons</span><br><span class="line">stopping resourcemanager</span><br><span class="line">Worker2: stopping nodemanager</span><br><span class="line">Worker1: stopping nodemanager</span><br><span class="line">no proxyserver to stop</span><br></pre></td></tr></table></figure></p><p>到这里Hadoop的全分布模式的配置，以及启动关闭就介绍完了。</p><p>如果在安装过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;配置全分布模式，至少需要三台Linux机器，我这里就以3台结点为例，配置全分布环境。&lt;br&gt;三台Linux机器，都完成准备阶段的配置后还需要如下的三个步骤，之后才开始配置全分布环境。&lt;br&gt;1、配置主机名&lt;br&gt;使用vi编辑器修改/etc/hosts文件，在配置文件中要添加三台Linux机器的IP地址和对应的主机名。&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;172.20.77.47 Master&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;172.20.77.48 Worker1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;172.20.77.49 Worker2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Hadoop" scheme="https://www.ggstu.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>SSH免密码登录的原理及其在Linux上的配置</title>
    <link href="https://www.ggstu.com/2018/08/31/SSH%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E5%9C%A8Linux%E4%B8%8A%E7%9A%84%E9%85%8D%E7%BD%AE/"/>
    <id>https://www.ggstu.com/2018/08/31/SSH免密码登录的原理及其在Linux上的配置/</id>
    <published>2018-08-31T10:52:27.000Z</published>
    <updated>2018-08-31T13:39:38.697Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>基本原理：采用非对称加密算法</b></font><br>1、产生一个密钥对：公钥（锁）、私钥（钥匙）<br>2、公钥：给别人来加密<br>3、私钥：给自己来解密<br>4、RSA算法是一种非对称加密算法<br><a id="more"></a><br><img src="http://pd8lpasbc.bkt.clouddn.com/39-2.png" alt="原理图"></p><p>若没有配置SSH免密码登录，要想登录另外一台机器，如Worker1这台机器登录Worker2这台机器，需要输入Worker2的登录密码，如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# ssh Worker2</span><br><span class="line">The authenticity of host &apos;worker2 (172.20.77.49)&apos; can&apos;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:MsBRF2Dy1tNNcVbeLRa7x+dTTAh4Yz0P1eE6IK53SZQ.</span><br><span class="line">ECDSA key fingerprint is MD5:5f:f3:a0:18:13:68:84:76:6e:95:a5:18:a2:41:98:0f.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added &apos;worker2,172.20.77.49&apos; (ECDSA) to the list of known hosts.</span><br><span class="line">root@worker2&apos;s password: </span><br><span class="line">Last login: Fri Aug 31 21:11:02 2018 from 183.212.160.91</span><br><span class="line"></span><br><span class="line">Welcome to Alibaba Cloud Elastic Compute Service !</span><br><span class="line"></span><br><span class="line">[root@Worker2 ~]#</span><br></pre></td></tr></table></figure></p><p>显然若不配置SSH免密码登录，每次登录另外一台机器都要输入密码，这很麻烦。因此需要配置免密码登录，以后登录另外这台机器就不需要输入密码。<br><br></p><p><font size="4"><b>SSH免密码登录配置：</b></font><br><b>1、在/etc/hosts文件中添加对应的主机名与IP地址的映射</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# vi /etc/hosts</span><br></pre></td></tr></table></figure></p><p>例如我这里有三台机器，则要修改每台机器中的hosts文件，添加三台Linux机器的IP地址和对应的主机名。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">172.20.77.47 Master</span><br><span class="line">172.20.77.48 Worker1</span><br><span class="line">172.20.77.49 Worker2</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出</p><p><b>2、产生密钥对：输入ssh-keygen -t rsa，之后按回车</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# ssh-keygen -t rsa</span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/root/.ssh/id_rsa): </span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /root/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /root/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:asnaNLVI1Nvw0XBXF+u6L3aWbdqwRtIdb6ZVDg/+wPs root@Worker1</span><br><span class="line">The key&apos;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">|          . . .o+|</span><br><span class="line">|       .   + .  o|</span><br><span class="line">|      . o . .  . |</span><br><span class="line">|     .   = .  +..|</span><br><span class="line">|      . S o  + B+|</span><br><span class="line">|     o = .  . B O|</span><br><span class="line">|      O .    +.Oo|</span><br><span class="line">|     = .     o=B+|</span><br><span class="line">|    . .     .oB=E|</span><br><span class="line">+----[SHA256]-----+</span><br></pre></td></tr></table></figure></p><p><b>3、把公钥分别拷给三台机器</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# ssh-copy-id -i .ssh/id_rsa.pub root@Worker2</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;.ssh/id_rsa.pub&quot;</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys</span><br><span class="line">root@worker2&apos;s password: </span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &apos;root@Worker2&apos;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br></pre></td></tr></table></figure></p><p><b>4、查看公钥</b><br>可以在Worker2这台机器上看到从Worker1传过来的公钥<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker2 ~]# ls .ssh/</span><br><span class="line">authorized_keys  id_rsa  id_rsa.pub</span><br><span class="line">[root@Worker2 ~]# more .ssh/authorized_keys</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDV4JxjxfQ5+juCguh+tJqy0YonZyIK0eyecSXynqKD</span><br><span class="line">YwNfgJ6uvwpDHHNffUSMoM7TjZrpF3g/Bu+ERFYn+YdJeMpmsftHlhiJgZoxDfQD/mAqmVV7XrQOgf9t</span><br><span class="line">tjfCjJFVYGSN00PdiH9s9xHkdoOVpNR1+jYBpA22AbtlOdKaz5eTluYO6klaz9VsEJ0gOu29l/CLyCHb</span><br><span class="line">ggTAryB/XHj/CFnPju71uaFCWdYpeqNxuc5SyUY6X5Oo88jK2Z/fyrwajHVh1i/nYHfr5bG4TEh6UUdr</span><br><span class="line">7EPgBQSekFik1gHHfXEt6hUcadRGUE7uRW38/VFothFcmtPyUMOm7o06lhN7 root@Worker1</span><br></pre></td></tr></table></figure></p><p>其中ssh-rsa表示使用RSA算法，中间的一长串字符串就是公钥，root@Worker1表示公钥是由Worker1这台机器产生的。</p><p><b>4、Worker1免密码登录Worker2</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker1 ~]# ssh Worker2</span><br><span class="line">Last login: Fri Aug 31 21:32:08 2018 from 183.212.160.91</span><br><span class="line"></span><br><span class="line">Welcome to Alibaba Cloud Elastic Compute Service !</span><br></pre></td></tr></table></figure></p><p><b>5、若要退出Worker2，返回Worker1，使用exit命令</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Worker2 ~]# exit</span><br><span class="line">logout</span><br><span class="line">Connection to worker2 closed.</span><br><span class="line">[root@Worker1 ~]#</span><br></pre></td></tr></table></figure></p><p>同样的道理，我这里有三台机器，若Worker1想要登录另外一台机器，就得将它的公钥拷贝给那台机器，就可以实现免密码登录；若Worker2想要登录其他机器，同样的重复如上步骤，先产生密钥对，然后将公钥拷贝给其他机器。<br><br><br>如果在配置过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;基本原理：采用非对称加密算法&lt;/b&gt;&lt;/font&gt;&lt;br&gt;1、产生一个密钥对：公钥（锁）、私钥（钥匙）&lt;br&gt;2、公钥：给别人来加密&lt;br&gt;3、私钥：给自己来解密&lt;br&gt;4、RSA算法是一种非对称加密算法&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Linux" scheme="https://www.ggstu.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop2.X的安装与配置（三）伪分布模式</title>
    <link href="https://www.ggstu.com/2018/08/31/Hadoop2-X%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%EF%BC%88%E4%B8%89%EF%BC%89%E4%BC%AA%E5%88%86%E5%B8%83%E6%A8%A1%E5%BC%8F/"/>
    <id>https://www.ggstu.com/2018/08/31/Hadoop2-X的安装与配置（三）伪分布模式/</id>
    <published>2018-08-31T07:51:52.000Z</published>
    <updated>2018-08-31T10:39:02.714Z</updated>
    
    <content type="html"><![CDATA[<p>在完成了准备阶段的配置后，接着就可以配置伪分布模式了。<br>伪分布模式的配置需要在如下的配置文件中添加配置参数。<br><img src="http://pd8lpasbc.bkt.clouddn.com/38-1.png" width="100%" height="100%"><br><a id="more"></a></p><p><font size="4"><b>在配置之前，首先对这些配置参数做一下介绍：</b></font><br><b>hadoop-env.sh</b>：配置环境变量<br>&emsp;&emsp;JAVA_HOME：设置Java环境变量。</p><p><b>hdfs-site.xml</b>：配置hdfs的属性<br>&emsp;&emsp;dfs.replication：数据块的冗余度，如果不配置，默认值是3。这里配置伪分布模式，只有一个数据节点，因此设置为1。<br>&emsp;&emsp;dfs.permissions：权限检查，默认值是true。设置为false，不进行权限检查。</p><p><b>core-site.xml</b>：配置hdfs的属性<br>&emsp;&emsp;fs.defaultFS：配置主节点的地址。<br>&emsp;&emsp;hadoop.tmp.dir：配置hdfs数据保存的目录，默认值是Linux的/tmp目录。若Linux重启，tmp目录下的所有数据会被删除，因此需要修改保存数据的目录，并且此目录要事先存在。</p><p><b>mapred-site.xml</b>：配置yarn的属性，yarn是一个执行MapReduce程序的容器<br>&emsp;&emsp;mapreduce.framework.name：设置MapReduce程序运行的容器为yarn。</p><p><b>yarn-site.xml</b>：配置yarn的属性<br>&emsp;&emsp;yarn.resourcemanager.hostname：配置主节点地址。<br>&emsp;&emsp;yarn.nodemanager.aux-services：配置从节点运行MapReduce的方式。<br><br></p><p><font size="4"><b>接下来开始进行配置：</b></font><br><b>1、修改hadoop-env.sh配置文件</b><br>使用vi编辑器修改hadoop-env.sh配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd software/hadoop-2.7.7/etc/hadoop/</span><br><span class="line">[root@ggstu hadoop]# vi hadoop-env.sh</span><br></pre></td></tr></table></figure></p><p>:set number打开行号，在hadoop-env.sh配置文件中添加JAVA_HOME路径<br><img src="http://pd8lpasbc.bkt.clouddn.com/38-2.png" width="70%" height="70%"><br>添加完成后，保存退出。</p><p><b>2、修改hdfs-site.xml配置文件</b><br>使用vi编辑器修改hdfs-site.xml配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd ~/software/hadoop-2.7.7/etc/hadoop/</span><br><span class="line">[root@ggstu hadoop]# vi hdfs-site.xml</span><br></pre></td></tr></table></figure></p><p>在configuration代码块中添加如下的两个property代码块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.permissions&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p><b>3、修改core-site.xml配置文件</b><br>创建hdfs数据保存的目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu hadoop]# mkdir /root/software/hadoop-2.7.7/tmp</span><br></pre></td></tr></table></figure></p><p>使用vi编辑器修改core-site.xml配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu hadoop]# vi core-site.xml</span><br></pre></td></tr></table></figure></p><p>在configuration代码块中添加如下的两个property代码块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://ggstu:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/root/software/hadoop-2.7.7/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p><b>4、修改mapred-site.xml配置文件</b><br>使用vi编辑器修改mapred-site.xml配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu hadoop]# cp mapred-site.xml.template mapred-site.xml</span><br><span class="line">[root@ggstu hadoop]# vi mapred-site.xml</span><br></pre></td></tr></table></figure></p><p>在configuration代码块中添加如下这个property代码块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p><b>5、修改yarn-site.xml配置文件</b><br>使用vi编辑器修改yarn-site.xml配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu hadoop]# vi yarn-site.xml</span><br></pre></td></tr></table></figure></p><p>在configuration代码块中添加如下的两个property代码块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;ggstu&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出。</p><p><b>6、对主节点NameNode格式化</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu hadoop]# hdfs namenode -format</span><br></pre></td></tr></table></figure></p><p>在打印的日志中，若看到如下这一条，说明格式化成功。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">18/08/31 16:58:49 INFO common.Storage: Storage directory /root/software/hadoop-2.7.7/tmp/dfs/name has been successfully formatted.</span><br></pre></td></tr></table></figure></p><p>到这里Hadoop的伪分布模式的配置就完成了。<br><br></p><p><font size="4"><b>启动Hadoop伪分布集群：</b></font><br>启动命令：<br>start-all.sh：启动hdfs和yarn<br>start-dfs.sh：启动hdfs<br>start-yarn.sh：启动yarn<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu hadoop]# start-all.sh</span><br></pre></td></tr></table></figure></p><p>在启动的过程中要输入四次登录密码，这是因为没有配置SSH免密码登录。关于如何配置SSH免密码登录后面会介绍。<br>使用jps命令查看后台进程，可以看到hdfs的NameNode、DataNode、SecondaryNameNode以及Yarn的ResourceManager、NodeManager都启动了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu hadoop]# jps</span><br><span class="line">4016 SecondaryNameNode</span><br><span class="line">4170 ResourceManager</span><br><span class="line">4491 Jps</span><br><span class="line">4459 NodeManager</span><br><span class="line">3853 DataNode</span><br><span class="line">3725 NameNode</span><br></pre></td></tr></table></figure></p><p><font color="#f00">注意：</font>如果你使用的是购买的服务器搭建集群，/etc/hosts下配置的IP要为内网IP，否则NameNode和ResourceManager无法启动。</p><p>关闭命令<br>stop-al.sh：关闭hdfs和yarn<br>stop-dfs.sh：关闭hdfs<br>stop-yarn.sh：关闭yarn<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu hadoop]# stop-all.sh</span><br></pre></td></tr></table></figure></p><p>关闭hdfs集群同样要输入四次登录密码。</p><p>到这里Hadoop的伪分布模式的配置，以及启动关闭就介绍完了。美中不足的地方就是集群的启动和关闭需要输入多次密码，很不方便。所以就需要配置SSH免密码登录，关于免密码登录，在后面的文章中会做介绍。<br><br><br>如果在安装过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在完成了准备阶段的配置后，接着就可以配置伪分布模式了。&lt;br&gt;伪分布模式的配置需要在如下的配置文件中添加配置参数。&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/38-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Hadoop" scheme="https://www.ggstu.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop2.X的安装与配置（二）本地模式</title>
    <link href="https://www.ggstu.com/2018/08/31/Hadoop2-X%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%EF%BC%88%E4%BA%8C%EF%BC%89%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F/"/>
    <id>https://www.ggstu.com/2018/08/31/Hadoop2-X的安装与配置（二）本地模式/</id>
    <published>2018-08-31T06:31:32.000Z</published>
    <updated>2018-08-31T07:29:22.621Z</updated>
    
    <content type="html"><![CDATA[<p>在完成了准备阶段的配置后，接着就可以配置本地模式了。<br>本地模式的配置需要在如下的配置文件中添加JAVA_HOME的路径。<br><img src="http://pd8lpasbc.bkt.clouddn.com/37-1.png" width="100%" height="100%"><br><a id="more"></a><br>使用vi编辑器修改hadoop-env.sh配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd software/hadoop-2.7.7/etc/hadoop/</span><br><span class="line">[root@ggstu hadoop]# vi hadoop-env.sh</span><br></pre></td></tr></table></figure></p><p>:set number打开行号，在hadoop-env.sh配置文件中添加JAVA_HOME路径<br><img src="http://pd8lpasbc.bkt.clouddn.com/37-2.png" width="70%" height="70%"><br>添加完成后，保存退出。到这里本地模式就配置完成了。<br><br></p><p><font size="4"><b>下面在本地模式测试一下MapReduce程序</b></font><br><b>1、找到执行程序的jar包</b><br>hadoop-mapreduce-examples-2.7.7.jar就是执行程序的jar包，其下有很多例子<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd software/hadoop-2.7.7/share/hadoop/mapreduce/</span><br><span class="line">[root@ggstu mapreduce]# ls</span><br><span class="line">hadoop-mapreduce-client-app-2.7.7.jar         hadoop-mapreduce-client-jobclient-2.7.7-tests.jar</span><br><span class="line">hadoop-mapreduce-client-common-2.7.7.jar      hadoop-mapreduce-client-shuffle-2.7.7.jar</span><br><span class="line">hadoop-mapreduce-client-core-2.7.7.jar        hadoop-mapreduce-examples-2.7.7.jar</span><br><span class="line">hadoop-mapreduce-client-hs-2.7.7.jar          lib</span><br><span class="line">hadoop-mapreduce-client-hs-plugins-2.7.7.jar  lib-examples</span><br><span class="line">hadoop-mapreduce-client-jobclient-2.7.7.jar   sources</span><br></pre></td></tr></table></figure></p><p><b>2、查看示例jar包下的例子</b><br>这里使用经典的wordcount作为例子执行MapReduce程序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu mapreduce]# hadoop jar hadoop-mapreduce-examples-2.7.7.jar </span><br><span class="line">An example program must be given as the first argument.</span><br><span class="line">Valid program names are:</span><br><span class="line">  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.</span><br><span class="line">  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.</span><br><span class="line">  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.</span><br><span class="line">  dbcount: An example job that count the pageview counts from a database.</span><br><span class="line">  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.</span><br><span class="line">  grep: A map/reduce program that counts the matches of a regex in the input.</span><br><span class="line">  join: A job that effects a join over sorted, equally partitioned datasets</span><br><span class="line">  multifilewc: A job that counts words from several files.</span><br><span class="line">  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.</span><br><span class="line">  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.</span><br><span class="line">  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.</span><br><span class="line">  randomwriter: A map/reduce program that writes 10GB of random data per node.</span><br><span class="line">  secondarysort: An example defining a secondary sort to the reduce.</span><br><span class="line">  sort: A map/reduce program that sorts the data written by the random writer.</span><br><span class="line">  sudoku: A sudoku solver.</span><br><span class="line">  teragen: Generate data for the terasort</span><br><span class="line">  terasort: Run the terasort</span><br><span class="line">  teravalidate: Checking results of terasort</span><br><span class="line">  wordcount: A map/reduce program that counts the words in the input files.</span><br><span class="line">  wordmean: A map/reduce program that counts the average length of the words in the input files.</span><br><span class="line">  wordmedian: A map/reduce program that counts the median length of the words in the input files.</span><br><span class="line">  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.</span><br></pre></td></tr></table></figure></p><p><b>3、准备数据源</b><br>使用vi编辑器创建一个文件，在此文件中输入一些以空格间隔的单词作为数据输入源<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu mapreduce]# mkdir ~/input</span><br><span class="line">[root@ggstu mapreduce]# vi ~/input/data.txt</span><br></pre></td></tr></table></figure></p><p>在data.txt文件中输入一些单词，我输入的单词如下所示<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p><b>4、执行wordcount程序</b><br>使用命令： hadoop jar hadoop-mapreduce-examples-2.7.7.jar wordcount ~/input/data.txt ~/output/wordcount<br>参数：wordcount    表示执行jar包中的wordcount程序<br>参数：~/input/data.txt    表示输入数据<br>参数：~/output/wordcount    表示输出数据保存路径，这个路径事先不存在<br>这里打印的日志比较多，我就截取其中的一部分，看到日志中打印出map 100% reduce 100%，说明MapReduce程序执行完成。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu mapreduce]# hadoop jar hadoop-mapreduce-examples-2.7.7.jar wordcount ~/input/data.txt ~/output/wordcount</span><br><span class="line">18/08/31 15:19:08 INFO mapred.LocalJobRunner: Finishing task: attempt_local1075408480_0001_r_000000_0</span><br><span class="line">18/08/31 15:19:08 INFO mapred.LocalJobRunner: reduce task executor complete.</span><br><span class="line">18/08/31 15:19:08 INFO mapreduce.Job: Job job_local1075408480_0001 running in uber mode : false</span><br><span class="line">18/08/31 15:19:08 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">18/08/31 15:19:08 INFO mapreduce.Job: Job job_local1075408480_0001 completed successfully</span><br><span class="line">18/08/31 15:19:08 INFO mapreduce.Job: Counters: 30</span><br></pre></td></tr></table></figure></p><p><b>5、查看输出结果</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu mapreduce]# cd ~/output/wordcount/</span><br><span class="line">[root@ggstu wordcount]# ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r-- 1 root root 26 Aug 31 15:19 part-r-00000</span><br><span class="line">-rw-r--r-- 1 root root  0 Aug 31 15:19 _SUCCESS</span><br><span class="line">[root@ggstu wordcount]# cat part-r-00000 </span><br><span class="line">day     2</span><br><span class="line">good    2</span><br><span class="line">study   1</span><br><span class="line">up      1</span><br></pre></td></tr></table></figure></p><p>wordcount程序的执行结果，统计出每个单词出现的频率。<br><br><br>如果在安装过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在完成了准备阶段的配置后，接着就可以配置本地模式了。&lt;br&gt;本地模式的配置需要在如下的配置文件中添加JAVA_HOME的路径。&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/37-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Hadoop" scheme="https://www.ggstu.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop2.X的安装与配置（一）准备阶段</title>
    <link href="https://www.ggstu.com/2018/08/31/Hadoop2-X%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%EF%BC%88%E4%B8%80%EF%BC%89%E5%87%86%E5%A4%87%E9%98%B6%E6%AE%B5/"/>
    <id>https://www.ggstu.com/2018/08/31/Hadoop2-X的安装与配置（一）准备阶段/</id>
    <published>2018-08-31T01:51:17.000Z</published>
    <updated>2018-09-01T00:03:12.062Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop是一个由Apache基金会所开发的分布式系统基础架构。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。<br>Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。<br><a id="more"></a><br>Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，则MapReduce为海量的数据提供了计算。</p><p><font size="5"><b>Hadoop有三种安装模式：</b></font><br><b>1、本地模式：</b>一台Linux<br>特点：没有HDFS(Hadoop Distributed File System)，即没有数据的存储。<br>只能测试MapReduce程序，处理的数据是本地数据（Linux文件）</p><p><b>2、伪分布模式：</b> 一台Linux<br>特点：在单机上，模拟一个分布式的环境，具备Hadoop的所有功能</p><p><b>3、全分布模式：</b> 多台Linux（以3台为例）<br>特点：真正的分布式环境，用于生产环境</p><p>这里首先介绍安装配置这几种模式前的准备阶段</p><p><font size="5"><b>准备阶段：</b></font></p><p><font size="4"><b>一、准备Linux机器：</b></font><br>如果是搭建本地模式或者伪分布模式，使用一台Linux机器即可。<br>若是搭建全分布模式，至少需要三台机器。<br>可以购买三台Linux的服务器，也可以在虚拟机上搭建三台Linux机器。<br>如果不懂如何在虚拟机上搭建Linux，点击如下链接进行查看：<br><a href="https://www.ggstu.com/2018/08/13/%E5%A6%82%E4%BD%95%E5%9C%A8VMware-Workstation%E5%AE%89%E8%A3%85RedHat-Linux-7-4/" target="_blank">如何在VMware Workstation安装RedHat Linux 7.4</a></p><p><font size="4"><b>二、在Linux上安装JDK：</b></font><br>如果不懂如何安装JDK，点击如下链接进行安装：<br><a href="https://www.ggstu.com/2018/08/30/Linux%E4%B8%8B%EF%BC%88CentOS7%EF%BC%89%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AEJDK1-8%EF%BC%88wget%E5%91%BD%E4%BB%A4%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F%EF%BC%89/" target="_blank">Linux下（CentOS7）安装与配置JDK1.8</a></p><p><font size="4"><b>三、关闭防火墙：</b></font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# systemctl stop firewalld.service</span><br><span class="line">[root@ggstu ~]# systemctl status firewalld.service</span><br></pre></td></tr></table></figure></p><p><font size="4"><b>四、配置主机名：</b></font><br>使用vi编辑器修改/etc下的hosts配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# vi /etc/hosts</span><br></pre></td></tr></table></figure></p><p>添加这台Linux机器的IP地址和对应的主机名到此文件中，比如我这台Linux的IP和主机名如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">172.24.101.191 ggstu</span><br></pre></td></tr></table></figure></p><p><font color="#f00">注意：</font>如果使用的是购买的Linux服务器，这里的IP是内网IP。<br>添加完成后，保存退出</p><p><font size="4"><b>五、在Windows上下载Hadoop2.X的安装包：</b></font><br>点击如下链接进行下载，下载binary中的*.tar.gz压缩包<br><a href="http://hadoop.apache.org/releases.html#Download" target="_blank" rel="noopener">http://hadoop.apache.org/releases.html#Download</a></p><p><font size="4"><b>六、将安装包上传到Linux上，并解压缩：</b></font><br>使用WinSCP将压缩包上传到事先创建好的tools目录下<br><img src="http://pd8lpasbc.bkt.clouddn.com/36-1.png" width="100%" height="100%"><br>使用SecureCRT远程连接上Linux，将压缩包解压到事先创建好的software目录下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd tools/</span><br><span class="line">[root@ggstu tools]# ls</span><br><span class="line">hadoop-2.7.7.tar.gz  jdk-8u131-linux-x64.tar.gz</span><br><span class="line">[root@ggstu tools]# tar -zxvf hadoop-2.7.7.tar.gz -C ~/software/</span><br></pre></td></tr></table></figure></p><p><font size="4"><b>七、设置环境变量：</b></font><br>查看HADOOP_HOME所在位置，如下所示，/root/software/hadoop-2.7.7就是HADOOP_HOME的位置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu tools]# cd ~/software/hadoop-2.7.7/</span><br><span class="line">[root@ggstu hadoop-2.7.7]# pwd</span><br><span class="line">/root/software/hadoop-2.7.7</span><br></pre></td></tr></table></figure></p><p>使用vi编辑器修改bash_profile配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu hadoop-2.7.7]# vi ~/.bash_profile</span><br></pre></td></tr></table></figure></p><p>在bash_profile文件末尾添加如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=/root/software/hadoop-2.7.7</span><br><span class="line">export HADOOP_HOME</span><br><span class="line">PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</span><br><span class="line">export PATH</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出</p><p><font color="#f00">注意：</font>这里设置的HADOOP_HOME即为上面查看到的HADOOP_HOME位置</p><p><font size="4"><b>八、生效环境变量：</b></font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu hadoop-2.7.7]# source ~/.bash_profile</span><br></pre></td></tr></table></figure></p><p><font size="4"><b>九、查看是否配置成功：</b></font><br>输入start-，再双击Tab键，出现的就是Hadoop的启动命令<br>输入stop-，再双击Tab键，出现的就是Hadoop的关闭命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu hadoop-2.7.7]# start-</span><br><span class="line">start-all.cmd        start-dfs.cmd        start-yarn.cmd</span><br><span class="line">start-all.sh         start-dfs.sh         start-yarn.sh</span><br><span class="line">start-balancer.sh    start-secure-dns.sh  </span><br><span class="line">[root@ggstu hadoop-2.7.7]# stop-</span><br><span class="line">stop-all.cmd        stop-dfs.cmd        stop-yarn.cmd</span><br><span class="line">stop-all.sh         stop-dfs.sh         stop-yarn.sh</span><br><span class="line">stop-balancer.sh    stop-secure-dns.sh</span><br></pre></td></tr></table></figure></p><p><br><br>到此，Hadoop2.X安装配置的准备阶段就完成了。就下来就可以根据需要安装本地模式、伪分布模式或全分布模式。<br><br><br>如果在安装过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hadoop是一个由Apache基金会所开发的分布式系统基础架构。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。&lt;br&gt;Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Hadoop" scheme="https://www.ggstu.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Linux下（CentOS7）安装与配置JDK1.8（上传安装方式）</title>
    <link href="https://www.ggstu.com/2018/08/30/Linux%E4%B8%8B%EF%BC%88CentOS7%EF%BC%89%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AEJDK1-8%EF%BC%88%E4%B8%8A%E4%BC%A0%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/08/30/Linux下（CentOS7）安装与配置JDK1-8（上传安装方式）/</id>
    <published>2018-08-30T03:31:05.000Z</published>
    <updated>2018-08-31T06:25:21.572Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇文章中已经介绍了如果使用wget命令安装配置JDK环境，之所以再写这篇使用上传的方式安装配置JDK环境，是因为在Linux上下载JDK可能网速会比较慢，下载时间可能较长，所以可以在Windows本地下载后上传到Linux上。<br>个人认为使用wget命令下载配置JDK还是比较方便的，如果wget命令下载速度太慢或者失败可以使用这种方法。<br><a id="more"></a><br><br></p><p><font size="4"><b>一、本地下载JDK1.8安装包：</b></font><br>点击下方链接到官网下载JDK1.8<br><a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a><br>接受许可证协议，然后点击jdk-8u181-linux-x64.tar.gz进行下载<br><img src="http://pd8lpasbc.bkt.clouddn.com/35-1.png" width="100%" height="100%"></p><p><font size="4"><b>二、Linux上创建安装目录：</b></font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# mkdir tools</span><br></pre></td></tr></table></figure></p><p><font size="4"><b>三、使用WinSCP将下载好的JDK上传到上一步创建的目录中：</b></font><br>选择压缩包，按下F5快捷键进行上传<br><img src="http://pd8lpasbc.bkt.clouddn.com/35-2.png" width="100%" height="100%"><br>上传完成后，可以查看到tools目录下有了JDK的压缩包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Master ~]# cd tools/</span><br><span class="line">[root@Master tools]# ls</span><br><span class="line">jdk-8u181-linux-x64.tar.gz</span><br></pre></td></tr></table></figure></p><p><font size="4"><b>四、创建解压后目录并解压压缩包：</b></font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@Master tools]# mkdir ~/software</span><br><span class="line">[root@Master tools]# tar -zxvf jdk-8u181-linux-x64.tar.gz -C ~/software</span><br><span class="line">[root@Master tools]# cd ~/software/</span><br><span class="line">[root@Master software]# ls</span><br><span class="line">jdk1.8.0_181</span><br></pre></td></tr></table></figure></p><p>解压完成后查看JAVA_HOME所在位置，/root/software/jdk1.8.0_181就是JAVA_HOME所在位置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@Master software]# cd jdk1.8.0_181/</span><br><span class="line">[root@Master jdk1.8.0_181]# pwd</span><br><span class="line">/root/software/jdk1.8.0_181</span><br></pre></td></tr></table></figure></p><p><font size="4"><b>五、设置环境变量：</b></font><br>修改bash_profile配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master jdk1.8.0_181]# vi ~/.bash_profile</span><br></pre></td></tr></table></figure></p><p>在bash_profile文件末尾添加如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/root/software/jdk1.8.0_181</span><br><span class="line">export JAVA_HOME</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">export PATH</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出</p><p><font color="#f00">注意：</font>这里设置的JAVA_HOME即为上一步中查看到的JAVA_HOME</p><p><font size="4"><b>六、生效环境变量：</b></font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Master jdk1.8.0_181]# source ~/.bash_profile</span><br></pre></td></tr></table></figure></p><p><font size="4"><b>七、查看JDK版本：</b></font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Master jdk1.8.0_181]# java -version</span><br><span class="line">java version &quot;1.8.0_181&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_181-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode)</span><br></pre></td></tr></table></figure></p><p><br><br>如果在安装过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一篇文章中已经介绍了如果使用wget命令安装配置JDK环境，之所以再写这篇使用上传的方式安装配置JDK环境，是因为在Linux上下载JDK可能网速会比较慢，下载时间可能较长，所以可以在Windows本地下载后上传到Linux上。&lt;br&gt;个人认为使用wget命令下载配置JDK还是比较方便的，如果wget命令下载速度太慢或者失败可以使用这种方法。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Linux" scheme="https://www.ggstu.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux下（CentOS7）安装与配置JDK1.8（wget命令安装方式）</title>
    <link href="https://www.ggstu.com/2018/08/30/Linux%E4%B8%8B%EF%BC%88CentOS7%EF%BC%89%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AEJDK1-8%EF%BC%88wget%E5%91%BD%E4%BB%A4%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/08/30/Linux下（CentOS7）安装与配置JDK1-8（wget命令安装方式）/</id>
    <published>2018-08-30T02:13:31.000Z</published>
    <updated>2018-08-31T03:20:34.462Z</updated>
    
    <content type="html"><![CDATA[<p>下面我来介绍一下如何使用wget命令在linux中安装与配置JDK1.8。<br>写这篇博客的原因是因为我发现在网上许多使用wget命令下载jdk都会报错，所以我在下面提供了正确的wget命令下载jdk的语法。<br><a id="more"></a><br><br></p><p><font size="4"><b>一、创建目录并下载JDK：</b></font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# mkdir tools</span><br><span class="line">[root@ggstu ~]# cd tools</span><br><span class="line">[root@ggstu tools]# wget -c --header &quot;Cookie:oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz</span><br><span class="line">[root@ggstu tools]# ls</span><br><span class="line">jdk-8u131-linux-x64.tar.gz</span><br></pre></td></tr></table></figure></p><p><font size="4"><b>二、创建解压后目录并解压压缩包：</b></font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu tools]# mkdir ~/software</span><br><span class="line">[root@ggstu tools]# tar -zxvf jdk-8u131-linux-x64.tar.gz -C ~/software</span><br><span class="line">[root@ggstu tools]# cd ~/software/</span><br><span class="line">[root@ggstu software]# ls</span><br><span class="line">jdk1.8.0_131</span><br></pre></td></tr></table></figure></p><p>解压完成后查看JAVA_HOME所在位置，/root/software/jdk1.8.0_131就是JAVA_HOME所在位置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu software]# cd jdk1.8.0_131/</span><br><span class="line">[root@ggstu jdk1.8.0_131]# pwd</span><br><span class="line">/root/software/jdk1.8.0_131</span><br></pre></td></tr></table></figure></p><p><font size="4"><b>三、设置环境变量：</b></font><br>修改bash_profile配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jdk1.8.0_131]# vi ~/.bash_profile</span><br></pre></td></tr></table></figure></p><p>在bash_profile文件末尾添加如下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/root/software/jdk1.8.0_131</span><br><span class="line">export JAVA_HOME</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">export PATH</span><br></pre></td></tr></table></figure></p><p>添加完成后，保存退出</p><p><font color="#f00">注意：</font>这里设置的JAVA_HOME即为第二步中查看到的JAVA_HOME</p><p><font size="4"><b>四、生效环境变量：</b></font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jdk1.8.0_131]# source ~/.bash_profile</span><br></pre></td></tr></table></figure></p><p><font size="4"><b>五、查看JDK版本：</b></font><br>输入命令java -version查看JDK版本，打印出如下内容说明环境安装配置完成<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu jdk1.8.0_131]# java -version</span><br><span class="line">java version &quot;1.8.0_131&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_131-b11)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)</span><br></pre></td></tr></table></figure></p><p><br><br>如果在安装过程中遇到什么问题，欢迎评论。<br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;下面我来介绍一下如何使用wget命令在linux中安装与配置JDK1.8。&lt;br&gt;写这篇博客的原因是因为我发现在网上许多使用wget命令下载jdk都会报错，所以我在下面提供了正确的wget命令下载jdk的语法。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Linux" scheme="https://www.ggstu.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Scala中的Actor</title>
    <link href="https://www.ggstu.com/2018/08/26/Scala%E4%B8%AD%E7%9A%84Actor/"/>
    <id>https://www.ggstu.com/2018/08/26/Scala中的Actor/</id>
    <published>2018-08-26T01:41:44.000Z</published>
    <updated>2018-08-26T02:31:22.671Z</updated>
    
    <content type="html"><![CDATA[<p>Scala的Actor类似于Java中的多线程编程。但是Scala的Actor尽可能地避免锁和共享状态，从而避免多线程并发时出现资源争用的情况，进而提升多线程编程的性能。<br>Scala的Actor模型与Java的多线程模型之间很大的一个区别就是，Scala Actor支持线程之间的精准通信，即一个actor可以给其它actor直接发送消息。<br><a id="more"></a><br><strong>Actor的创建、启动和消息收发</strong><br>Scala提供了Actor trait来方便地进行actor多线程编程，Actor trait类似于Java中的Thread和Runnable，是基础的多线程基类和接口。只要重写Actor trait的act方法，即可实现自己的线程执行体，与Java中重写run方法类似。<br>使用start方法启动actor。使用!符号，向actor发送消息。actor内部使用receive和模式匹配接收消息。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; import scala.actors.Actor</span><br><span class="line"></span><br><span class="line">scala&gt; class HelloActor extends Actor&#123;</span><br><span class="line">     |     def act()&#123;</span><br><span class="line">     |         while(true)&#123;</span><br><span class="line">     |             receive&#123;</span><br><span class="line">     |                 case name:String =&gt; println(&quot;Hello,&quot; +name)</span><br><span class="line">     |             &#125;</span><br><span class="line">     |         &#125;</span><br><span class="line">     |     &#125;</span><br><span class="line">     | &#125;</span><br><span class="line"></span><br><span class="line">scala&gt; val p = new HelloActor</span><br><span class="line"></span><br><span class="line">scala&gt; p.start()</span><br><span class="line">res0: scala.actors.Actor = HelloActor@53dad875</span><br><span class="line"></span><br><span class="line">scala&gt; p ! &quot;Tom&quot;</span><br><span class="line">Hello,Tom</span><br></pre></td></tr></table></figure></p><p><br><br><strong>收发case class类型的消息</strong><br>在scala中，通常建议使用case class，即样本类作为消息进行发送。然后在actor接收消息之后，可以使用scala模式匹配功能来进行不同消息的处理。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; import scala.actors.Actor</span><br><span class="line"></span><br><span class="line">scala&gt; case class Person(name:String)</span><br><span class="line"></span><br><span class="line">scala&gt; case class Fruit(name:String)</span><br><span class="line"></span><br><span class="line">scala&gt; class HelloActor extends Actor&#123;</span><br><span class="line">     |     def act()&#123;</span><br><span class="line">     |         while(true)&#123;</span><br><span class="line">     |             receive&#123;</span><br><span class="line">     |                 case Person(name) =&gt; println(&quot;Hello,&quot; + name)</span><br><span class="line">     |                 case Fruit(name) =&gt; println(&quot;Hello,&quot; + name + &quot;.I want to eat you.&quot;)</span><br><span class="line">     |             &#125;</span><br><span class="line">     |         &#125;</span><br><span class="line">     |     &#125;</span><br><span class="line">     | &#125;</span><br><span class="line"></span><br><span class="line">scala&gt; val p = new HelloActor</span><br><span class="line"></span><br><span class="line">scala&gt; p.start()</span><br><span class="line">res0: scala.actors.Actor = HelloActor@59dc36d4</span><br><span class="line"></span><br><span class="line">scala&gt; p ! Person(&quot;Tom&quot;)</span><br><span class="line">Hello,Tom</span><br><span class="line"></span><br><span class="line">scala&gt; p ! Fruit(&quot;Apple&quot;)</span><br><span class="line">Hello,Apple.I want to eat you.</span><br></pre></td></tr></table></figure></p><p><br><br><strong>Actor之间互相收发消息</strong><br>如果两个Actor之间要互相收发消息，一个actor向另外一个actor发送消息，要带上自己的引用，其它actor收到消息时，直接通过发送消息的actor的引用，即可以给它回复消息。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; import scala.actors.Actor</span><br><span class="line"></span><br><span class="line">scala&gt; case class SendMessage(content:String,sender:Actor)</span><br><span class="line"></span><br><span class="line">scala&gt; class TomActor extends Actor&#123;</span><br><span class="line">     |     def act()&#123;</span><br><span class="line">     |         while(true)&#123;</span><br><span class="line">     |             receive&#123;</span><br><span class="line">     |                 case SendMessage(content,sender) =&gt; &#123;</span><br><span class="line">     |                     println(&quot;Send:Hello,I&apos;m Tom: &quot; + content)</span><br><span class="line">     |                     sender ! &quot;Receive:I&apos;m Tom,I&apos;m busy now.&quot;</span><br><span class="line">     |                 &#125;</span><br><span class="line">     |             &#125;</span><br><span class="line">     |         &#125;</span><br><span class="line">     |     &#125;</span><br><span class="line">     | &#125;</span><br><span class="line"></span><br><span class="line">scala&gt; class BobActor(val tomActor:Actor) extends Actor&#123;</span><br><span class="line">     |     def act()&#123;</span><br><span class="line">     |         tomActor ! SendMessage(&quot;Receive:Hello,Tom.I&apos;m Bob.&quot;,this)</span><br><span class="line">     |         while(true)&#123;</span><br><span class="line">     |             receive&#123;</span><br><span class="line">     |                 case response:String =&gt; println(&quot;Send:Hello,I&apos;m Bob: &quot; + response)</span><br><span class="line">     |             &#125;</span><br><span class="line">     |         &#125;</span><br><span class="line">     |     &#125;</span><br><span class="line">     | &#125;</span><br><span class="line"></span><br><span class="line">scala&gt; val tom = new TomActor</span><br><span class="line"></span><br><span class="line">scala&gt; val bob = new BobActor(tom)</span><br><span class="line"></span><br><span class="line">scala&gt; tom.start()</span><br><span class="line"></span><br><span class="line">scala&gt; bob.start()</span><br><span class="line"></span><br><span class="line">scala&gt; Send:Hello,I&apos;m Tom: Receive:Hello,Tom.I&apos;m Bob.</span><br><span class="line">Send:Hello,I&apos;m Bob: Receive:I&apos;m Tom,I&apos;m busy now.</span><br></pre></td></tr></table></figure></p><p><br><br><strong>同步消息和Future</strong><br>默认情况下，消息都是异步的，但是如果希望发送的消息是同步的，即对方接收后，一定要给自己返回结果，那么可以使用!?的方式发送消息。<br>val reply = actor !? message<br>如果要异步发送一个消息，但是在后续要获得消息的返回值，那么可以使用Future。即!!语法。<br>val future = actor !! message<br>val reply = future()<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scala的Actor类似于Java中的多线程编程。但是Scala的Actor尽可能地避免锁和共享状态，从而避免多线程并发时出现资源争用的情况，进而提升多线程编程的性能。&lt;br&gt;Scala的Actor模型与Java的多线程模型之间很大的一个区别就是，Scala Actor支持线程之间的精准通信，即一个actor可以给其它actor直接发送消息。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Scala的隐式转换函数、隐式参数和隐式类</title>
    <link href="https://www.ggstu.com/2018/08/25/Scala%E7%9A%84%E9%9A%90%E5%BC%8F%E8%BD%AC%E6%8D%A2%E5%87%BD%E6%95%B0%E3%80%81%E9%9A%90%E5%BC%8F%E5%8F%82%E6%95%B0%E5%92%8C%E9%9A%90%E5%BC%8F%E7%B1%BB/"/>
    <id>https://www.ggstu.com/2018/08/25/Scala的隐式转换函数、隐式参数和隐式类/</id>
    <published>2018-08-25T12:30:37.000Z</published>
    <updated>2018-08-25T13:44:52.223Z</updated>
    
    <content type="html"><![CDATA[<p><strong>隐式转换函数</strong><br>Scala的隐式转换函数是以implicit关键字声明的带有单个参数的函数。<br>Scala会根据隐式转换函数的签名，在程序中使用到隐式转换函数接收的参数类型定义的对象时，会自动将其传入隐式转换函数，转换为另外一种类型的对象并返回。这就是“隐式转换”。<br>通常建议将隐式转换函数的名称命名为“源类型2目标类型”的形式。<br><a id="more"></a><br>隐式转换的发生时机：<br>1、调用某个函数，但是给函数传入的参数的类型，与函数定义的接收参数类型不匹配。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; class Student(val name:String)</span><br><span class="line"></span><br><span class="line">scala&gt; class Teacher(val name:String)</span><br><span class="line"></span><br><span class="line">scala&gt; class Person(val name:String)</span><br><span class="line"></span><br><span class="line">scala&gt; class Fruit(val name:String)</span><br><span class="line"></span><br><span class="line">scala&gt; implicit def object2person(obj:Object):Person = &#123;</span><br><span class="line">     |     if(obj.getClass == classOf[Student])&#123;</span><br><span class="line">     |         val stu = obj.asInstanceOf[Student]</span><br><span class="line">     |         new Person(stu.name)</span><br><span class="line">     |     &#125;else if(obj.getClass == classOf[Teacher])&#123;</span><br><span class="line">     |         val t = obj.asInstanceOf[Teacher]</span><br><span class="line">     |         new Person(t.name)</span><br><span class="line">     |     &#125;else Nil</span><br><span class="line">     | &#125;</span><br><span class="line">                                                      ^</span><br><span class="line">scala&gt; def sayHello(p:Person) = println(&quot;Hello,I&apos;m &quot; + p.name)</span><br><span class="line">sayHello: (p: Person)Unit</span><br><span class="line"></span><br><span class="line">scala&gt; val s = new Student(&quot;Tom&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; sayHello(s)</span><br><span class="line">Hello,I&apos;m Tom</span><br><span class="line"></span><br><span class="line">scala&gt; val v = new Fruit(&quot;Apple&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; sayHello(v)</span><br></pre></td></tr></table></figure></p><p>2、使用某个类型的对象，调用某个方法，而这个方法并不存在于该类型时。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; class Man(val name:String)</span><br><span class="line"></span><br><span class="line">scala&gt; class Superman(val name:String)&#123;</span><br><span class="line">     |    def fly = println(&quot;Ha,ha,ha,I can fly.&quot;)</span><br><span class="line">     | &#125;</span><br><span class="line"></span><br><span class="line">scala&gt; implicit def man2superman(man:Man):Superman = new Superman(man.name)</span><br><span class="line">man2superman: (man: Man)Superman</span><br><span class="line"></span><br><span class="line">scala&gt; val m = new Man(&quot;Tom&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; m.fly</span><br><span class="line">Ha,ha,ha,I can fly.</span><br></pre></td></tr></table></figure></p><p>3、使用某个类型的对象，调用某个方法，虽然该类型有这个方法，但是给方法传入的参数类型，与方法定义的接收参数的类型不匹配。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; class Student(val name:String)</span><br><span class="line"></span><br><span class="line">scala&gt; class Teacher(val name:String)</span><br><span class="line"></span><br><span class="line">scala&gt; class Person(val name:String)</span><br><span class="line"></span><br><span class="line">scala&gt; implicit def object2person(obj:Object):Person = &#123;</span><br><span class="line">     |     if(obj.getClass == classOf[Student])&#123;</span><br><span class="line">     |         val stu = obj.asInstanceOf[Student]</span><br><span class="line">     |         new Person(stu.name)</span><br><span class="line">     |     &#125;else if(obj.getClass == classOf[Teacher])&#123;</span><br><span class="line">     |         val t = obj.asInstanceOf[Teacher]</span><br><span class="line">     |         new Person(t.name)</span><br><span class="line">     |     &#125;else Nil</span><br><span class="line">     | &#125;</span><br><span class="line">object2person: (obj: Object)Person</span><br><span class="line"></span><br><span class="line">scala&gt; class Hello&#123;</span><br><span class="line">     |     def sayHello(p:Person) = println(&quot;Hello,I&apos;m &quot; + p.name)</span><br><span class="line">     | &#125;</span><br><span class="line">defined class Hello</span><br><span class="line"></span><br><span class="line">scala&gt; val h = new Hello</span><br><span class="line"></span><br><span class="line">scala&gt; val s = new Student(&quot;Tom&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; h.sayHello(s)</span><br><span class="line">Hello,I&apos;m Tom</span><br></pre></td></tr></table></figure></p><p><br><br><strong>隐式参数</strong><br>使用implicit声明的函数参数叫做隐式参数。<br>Scala会在两个位置查找隐式参数：一种是当前作用域内可见的val或var定义的隐式变量；一种是隐式参数类型的伴生对象内的隐式值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; def sayHello(implicit name:String) = println(&quot;Hello,I&apos;m &quot; + name)</span><br><span class="line">sayHello: (implicit name: String)Unit</span><br><span class="line"></span><br><span class="line">scala&gt; implicit val name:String = &quot;Tom&quot;</span><br><span class="line">name: String = Tom</span><br><span class="line"></span><br><span class="line">scala&gt; sayHello</span><br><span class="line">Hello,I&apos;m Tom</span><br></pre></td></tr></table></figure></p><p><br><br><strong>隐式类</strong><br>隐式类就是对类增加implicit限定的类，其主要作用就是增强类的功能。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; 1.add(2)</span><br><span class="line">&lt;console&gt;:12: error: value add is not a member of Int</span><br><span class="line">       1.add(2)</span><br><span class="line">         ^</span><br><span class="line"></span><br><span class="line">scala&gt; implicit class Calc(x:Int)&#123;</span><br><span class="line">     |     def add(a:Int):Int = a + x</span><br><span class="line">     | &#125;</span><br><span class="line">defined class Calc</span><br><span class="line"></span><br><span class="line">scala&gt; 1.add(2)</span><br><span class="line">res0: Int = 3</span><br></pre></td></tr></table></figure></p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;隐式转换函数&lt;/strong&gt;&lt;br&gt;Scala的隐式转换函数是以implicit关键字声明的带有单个参数的函数。&lt;br&gt;Scala会根据隐式转换函数的签名，在程序中使用到隐式转换函数接收的参数类型定义的对象时，会自动将其传入隐式转换函数，转换为另外一种类型的对象并返回。这就是“隐式转换”。&lt;br&gt;通常建议将隐式转换函数的名称命名为“源类型2目标类型”的形式。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Scala的协变和逆变</title>
    <link href="https://www.ggstu.com/2018/08/25/Scala%E7%9A%84%E5%8D%8F%E5%8F%98%E5%92%8C%E9%80%86%E5%8F%98/"/>
    <id>https://www.ggstu.com/2018/08/25/Scala的协变和逆变/</id>
    <published>2018-08-25T11:33:06.000Z</published>
    <updated>2018-08-25T12:24:11.558Z</updated>
    
    <content type="html"><![CDATA[<p>在Java中，如果定义父类A，子类B，那么根据多态性，A a = new B是可以编译通过的。<br>但是如果泛型B的集合直接赋给父类A的集合，即List&lt;A> aList = new ArrayList&lt;B>();会报错。<br>Scala的协变和逆变就是解决这一问题的存在。</p><p><strong>Scala的协变</strong><br><a id="more"></a><br>泛型变量的值可以是本身类型或者其子类的类型。<br>Scala的class或trait的泛型定义中，如果在类型参数前面加上+符号，就可以使类或特质变为协变。<br>例如：有了协变就可以把一个子类Bird的EatSomething[Bird]赋值给父类Animal的EatSomething[Animal]。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; class Animal</span><br><span class="line">defined class Animal</span><br><span class="line"></span><br><span class="line">scala&gt; class Bird extends Animal</span><br><span class="line">defined class Bird</span><br><span class="line"></span><br><span class="line">scala&gt; class EatSomething[+T](t:T)</span><br><span class="line">defined class EatSomething</span><br><span class="line"></span><br><span class="line">scala&gt; val s1:EatSomething[Bird] = new EatSomething[Bird](new Bird)</span><br><span class="line">s1: EatSomething[Bird] = EatSomething@59e505b2</span><br><span class="line"></span><br><span class="line">scala&gt; val s2:EatSomething[Animal] = s1</span><br><span class="line">s2: EatSomething[Animal] = EatSomething@59e505b2</span><br></pre></td></tr></table></figure></p><p><br><br><strong>Scala的逆变</strong><br><br><br>泛型变量的值可以是本身类型或者其父类的类型。<br>Scala的class或trait的泛型定义中，如果在类型参数前面加上-符号，就可以使类或特质变为逆变。<br>例如：有了逆变就可以把一个父类Animal的EatSomething[Animal]赋值给子类Bird的EatSomething[Bird]。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; class Animal</span><br><span class="line">defined class Animal</span><br><span class="line"></span><br><span class="line">scala&gt; class Bird extends Animal</span><br><span class="line">defined class Bird</span><br><span class="line"></span><br><span class="line">scala&gt; class EatSomething[-T](t:T)</span><br><span class="line">defined class EatSomething</span><br><span class="line"></span><br><span class="line">scala&gt; val s1:EatSomething[Animal] = new EatSomething[Animal](new Animal)</span><br><span class="line">s1: EatSomething[Animal] = EatSomething@82ea68c</span><br><span class="line"></span><br><span class="line">scala&gt; val s2:EatSomething[Bird] = s1</span><br><span class="line">s2: EatSomething[Bird] = EatSomething@82ea68c</span><br></pre></td></tr></table></figure></p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Java中，如果定义父类A，子类B，那么根据多态性，A a = new B是可以编译通过的。&lt;br&gt;但是如果泛型B的集合直接赋给父类A的集合，即List&amp;lt;A&gt; aList = new ArrayList&amp;lt;B&gt;();会报错。&lt;br&gt;Scala的协变和逆变就是解决这一问题的存在。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Scala的协变&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Scala泛型类型的视图界定和上下文界定</title>
    <link href="https://www.ggstu.com/2018/08/25/Scala%E6%B3%9B%E5%9E%8B%E7%B1%BB%E5%9E%8B%E7%9A%84%E8%A7%86%E5%9B%BE%E7%95%8C%E5%AE%9A%E5%92%8C%E4%B8%8A%E4%B8%8B%E6%96%87%E7%95%8C%E5%AE%9A/"/>
    <id>https://www.ggstu.com/2018/08/25/Scala泛型类型的视图界定和上下文界定/</id>
    <published>2018-08-25T08:34:29.000Z</published>
    <updated>2018-08-25T11:18:37.947Z</updated>
    
    <content type="html"><![CDATA[<p><strong>视图界定View Bounds</strong><br>视图界定比上界&nbsp;&lt;:&nbsp;适用的范围更广，除了所有的子类型，还允许隐式转换过去的类型。用&nbsp;&lt;%&nbsp;表示。<br>例如：拼接字符串。这里由于T的上界是String，当参数传入整数100和200时，就会出现类型不匹配。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; def addString[T&lt;:String](x:T,y:T) = println(x+&quot; &quot;+y)</span><br><span class="line">addString: [T &lt;: String](x: T, y: T)Unit</span><br><span class="line"></span><br><span class="line">scala&gt; addString(&quot;Hello&quot;,&quot;World&quot;)</span><br><span class="line">Hello World</span><br><span class="line"></span><br><span class="line">scala&gt; addString(100,200)</span><br><span class="line">&lt;console&gt;:13: error: inferred type arguments [Int] do not conform to method addString&apos;s type parameter bounds [T &lt;: String]</span><br><span class="line">       addString(100,200)</span><br><span class="line">       ^</span><br><span class="line">&lt;console&gt;:13: error: type mismatch;</span><br><span class="line"> found   : Int(100)</span><br><span class="line"> required: T</span><br><span class="line">       addString(100,200)</span><br><span class="line">                 ^</span><br><span class="line">&lt;console&gt;:13: error: type mismatch;</span><br><span class="line"> found   : Int(200)</span><br><span class="line"> required: T</span><br><span class="line">       addString(100,200)</span><br><span class="line">                     ^</span><br></pre></td></tr></table></figure></p><p>但是整型数字是可以转换成字符串的，所以可以使用视图界定让addString方法接收更广泛的数据类型。<br>即：1、字符串及其子类；2、可以转换成字符串的类型。</p><p><font color="#f00">注意：</font>使用的是&nbsp;<font color="#f00">&lt;%</font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; def addString[T&lt;%String](x:T,y:T) = println(x+&quot; &quot;+y)</span><br><span class="line">addString: [T](x: T, y: T)(implicit evidence$1: T =&gt; String)Unit</span><br><span class="line"></span><br><span class="line">scala&gt; addString(&quot;Hello&quot;,&quot;World&quot;)</span><br><span class="line">Hello World</span><br><span class="line"></span><br><span class="line">scala&gt; addString(100,200)</span><br><span class="line">&lt;console&gt;:13: error: No implicit view available from Int =&gt; String.</span><br><span class="line">       addString(100,200)</span><br><span class="line">                ^</span><br></pre></td></tr></table></figure></p><p>实际运行时发生了错误：<font color="#f00">No implicit view available from Int =&gt; String.</font><br>这是因为Scala并没有定义如何将Int转换成String的规则，所以要使用视图界定，就必须手动创建转换规则。<br>如下所示，创建了转换规则后，运行成功。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; implicit def int2String(n:Int):String = n.toString</span><br><span class="line">warning: there was one feature warning; re-run with -feature for details</span><br><span class="line">int2String: (n: Int)String</span><br><span class="line"></span><br><span class="line">scala&gt; addString(100,200)</span><br><span class="line">100 200</span><br></pre></td></tr></table></figure></p><p><br><br><strong>上下文界定Context Bounds</strong><br>上下文界定的类型参数形式为T:M的形式，其中M是一个泛型，这种形式要求存在一个M[T]类型的隐式值。<br>例如：使用Scala内置的比较器比较大小<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; class Calculator[T:Ordering](val number1:T,val number2:T)&#123;</span><br><span class="line">     |     def max(implicit order:Ordering[T]) = if(order.compare(number1,number2) &gt; 0) number1 else number2</span><br><span class="line">     | &#125;</span><br><span class="line">defined class Calculator</span><br><span class="line"></span><br><span class="line">scala&gt; val cal = new Calculator(3,6)</span><br><span class="line">cal: Calculator[Int] = Calculator@3700ec9c</span><br><span class="line"></span><br><span class="line">scala&gt; cal.max</span><br><span class="line">res0: Int = 6</span><br></pre></td></tr></table></figure></p><p><br><br><strong>Manifest Context Bounds</strong><br>在Scala中，数组必须是有类型的，如果直接是泛型的话会报错，这时就必须使用Manifest上下文界定。<br>也就是说，如果数组元素类型为T的话，要实例化Array[T]这种泛型数组，需要为类或者函数定义[T:Manifest]泛型类型。<br>案例：将大人分成一组，将小孩分成一组<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; class Child(val name:String)</span><br><span class="line"></span><br><span class="line">scala&gt; class Adult(val name:String)</span><br><span class="line"></span><br><span class="line">scala&gt; def toGroup[T:Manifest](p:T*) = &#123;</span><br><span class="line">     |     val group = new Array[T](p.length)</span><br><span class="line">     |     for(i &lt;- 0 until p.length) group(i) = p(i)</span><br><span class="line">     |     group</span><br><span class="line">     | &#125;</span><br><span class="line">toGroup: [T](p: T*)(implicit evidence$1: Manifest[T])Array[T]</span><br><span class="line"></span><br><span class="line">scala&gt; val child1 = new Child(&quot;Tom&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; val child2 = new Child(&quot;Bob&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; val child3 = new Child(&quot;Alice&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; val childGroup = toGroup(child1,child2,child3)</span><br><span class="line">childGroup: Array[Child] = Array(Child@7b22ec89, Child@4a1e3ac1, Child@4f449e8f)</span><br><span class="line"></span><br><span class="line">scala&gt; val adult1 = new Adult(&quot;Jack&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; val adult2 = new Adult(&quot;Jerry&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; val adult3 = new Adult(&quot;Peter&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; val adultGroup = toGroup(adult1,adult2,adult3)</span><br><span class="line">adultGroup: Array[Adult] = Array(Adult@1816e24a, Adult@4fad6218, Adult@112d1c8e)</span><br></pre></td></tr></table></figure></p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;视图界定View Bounds&lt;/strong&gt;&lt;br&gt;视图界定比上界&amp;nbsp;&amp;lt;:&amp;nbsp;适用的范围更广，除了所有的子类型，还允许隐式转换过去的类型。用&amp;nbsp;&amp;lt;%&amp;nbsp;表示。&lt;br&gt;例如：拼接字符串。这里由于T的上界是String，当参数传入整数100和200时，就会出现类型不匹配。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Scala泛型类型的上界(Upper Bounds)和下界(Lower Bounds)</title>
    <link href="https://www.ggstu.com/2018/08/25/Scala%E6%B3%9B%E5%9E%8B%E7%B1%BB%E5%9E%8B%E7%9A%84%E4%B8%8A%E7%95%8C-Upper-Bounds-%E5%92%8C%E4%B8%8B%E7%95%8C-Lower-Bounds/"/>
    <id>https://www.ggstu.com/2018/08/25/Scala泛型类型的上界-Upper-Bounds-和下界-Lower-Bounds/</id>
    <published>2018-08-25T07:18:08.000Z</published>
    <updated>2018-08-25T08:21:53.868Z</updated>
    
    <content type="html"><![CDATA[<p>在指定泛型类型的时候，有时需要对泛型类型的范围进行界定，而不是可以是任意的类型。<br>比如，要求某个泛型类型，它必须是某个类的子类，这样在程序中就可以放心的调用满足泛型条件的方法，程序才能正常的使用和运行。<br>Scala的上下边界特性允许泛型类型必须是某个类的子类，或者必须是某个类的父类。<br><a id="more"></a><br><strong>上界Upper Bounds</strong><br>定义：S&lt;:T，也就是类型S必须是类型T的子类（或本身，自己也可以是自己的子类。）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; :paste</span><br><span class="line">// Entering paste mode (ctrl-D to finish)</span><br><span class="line"></span><br><span class="line">class Vehicle&#123;</span><br><span class="line">    def drive() = println(&quot;Driving&quot;)</span><br><span class="line">&#125;</span><br><span class="line">class Car extends Vehicle&#123;</span><br><span class="line">    override def drive() = println(&quot;Car driving&quot;)</span><br><span class="line">&#125;</span><br><span class="line">class Person&#123;</span><br><span class="line">    def drive() = println(&quot;I&apos;m driving the car.&quot;)</span><br><span class="line">&#125;</span><br><span class="line">def takeVehicle[T &lt;: Vehicle](v:T) = v.drive()</span><br><span class="line"></span><br><span class="line">// Exiting paste mode, now interpreting.</span><br><span class="line"></span><br><span class="line">scala&gt; val v:Vehicle = new Vehicle</span><br><span class="line">scala&gt; takeVehicle(v)</span><br><span class="line">Driving</span><br><span class="line"></span><br><span class="line">scala&gt; val c:Car = new Car</span><br><span class="line">scala&gt; takeVehicle(c)</span><br><span class="line">Car driving</span><br><span class="line"></span><br><span class="line">scala&gt; val p = new Person</span><br><span class="line">scala&gt; takeVehicle(p)</span><br><span class="line">&lt;console&gt;:16: error: inferred type arguments [Person] do not conform to method takeVehicle&apos;s type parameter bounds [T &lt;: Vehicle]</span><br><span class="line">       takeVehicle(p)</span><br><span class="line">       ^</span><br><span class="line">&lt;console&gt;:16: error: type mismatch;</span><br><span class="line"> found   : Person</span><br><span class="line"> required: T</span><br><span class="line">       takeVehicle(p)</span><br><span class="line">                   ^</span><br></pre></td></tr></table></figure></p><p><br><br><strong>下界Lower Bounds</strong><br>定义：U&gt;:T，也就是类型U必须是类型T的父类（或本身，自己也可以是自己的父类。）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; class Father</span><br><span class="line">defined class Father</span><br><span class="line"></span><br><span class="line">scala&gt; class Child extends Father</span><br><span class="line">defined class Child</span><br><span class="line"></span><br><span class="line">scala&gt; class Teacher</span><br><span class="line">defined class Teacher</span><br><span class="line"></span><br><span class="line">scala&gt; def getName[T &gt;: Child](person:T)&#123;</span><br><span class="line">     |     if(person.getClass == classOf[Child]) println(&quot;I&apos;m child.&quot;)</span><br><span class="line">     |     else if(person.getClass == classOf[Father]) println(&quot;I&apos;m father.&quot;)</span><br><span class="line">     |     else println(&quot;Sorry,don&apos;t know who you are.&quot;)</span><br><span class="line">     | &#125;</span><br><span class="line">getName: [T &gt;: Child](person: T)Unit</span><br><span class="line"></span><br><span class="line">scala&gt; val p1 = new Father</span><br><span class="line">p1: Father = Father@3688eb5b</span><br><span class="line"></span><br><span class="line">scala&gt; getName(p1)</span><br><span class="line">I&apos;m father.</span><br><span class="line"></span><br><span class="line">scala&gt; val p2 = new Child</span><br><span class="line">p2: Child = Child@7f0d96f2</span><br><span class="line"></span><br><span class="line">scala&gt; getName(p2)</span><br><span class="line">I&apos;m child.</span><br><span class="line"></span><br><span class="line">scala&gt; val p3 = new Teacher</span><br><span class="line">p3: Teacher = Teacher@5143c662</span><br><span class="line"></span><br><span class="line">scala&gt; getName(p3)</span><br><span class="line">Sorry,don&apos;t know who you are.</span><br></pre></td></tr></table></figure></p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在指定泛型类型的时候，有时需要对泛型类型的范围进行界定，而不是可以是任意的类型。&lt;br&gt;比如，要求某个泛型类型，它必须是某个类的子类，这样在程序中就可以放心的调用满足泛型条件的方法，程序才能正常的使用和运行。&lt;br&gt;Scala的上下边界特性允许泛型类型必须是某个类的子类，或者必须是某个类的父类。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Scala的泛型类和泛型函数</title>
    <link href="https://www.ggstu.com/2018/08/25/Scala%E7%9A%84%E6%B3%9B%E5%9E%8B%E7%B1%BB%E5%92%8C%E6%B3%9B%E5%9E%8B%E5%87%BD%E6%95%B0/"/>
    <id>https://www.ggstu.com/2018/08/25/Scala的泛型类和泛型函数/</id>
    <published>2018-08-25T06:25:35.000Z</published>
    <updated>2018-08-25T07:06:36.829Z</updated>
    
    <content type="html"><![CDATA[<p>在Scala中可以使用类型参数来实现类和函数，这样的类和函数可以用于多种类型。比如Array[T]可以存放任意指定类型T的数据。</p><p><strong>泛型类</strong>就是在类的声明中，定义一些泛型类型，然后在类内部，就可以使用这些泛型类型。直接给使用了泛型类型的字段赋值时，Scala会自动进行类型推断。使用泛型类，通常是需要对类中的某些成员，比如某些字段和方法中的参数或变量进行统一的类型限制，这样可以保证程序的健壮性和稳定性。</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; class Student[T](val name:T)&#123;</span><br><span class="line">     |     def getAge(age:T) = name + &quot;:&quot; + age + &quot; years old&quot;</span><br><span class="line">     | &#125;</span><br><span class="line">defined class Student</span><br><span class="line"></span><br><span class="line">scala&gt; val s = new Student(&quot;Tom&quot;)</span><br><span class="line">s: Student[String] = Student@366ac49b</span><br><span class="line"></span><br><span class="line">scala&gt; s.getAge(&quot;21&quot;)</span><br><span class="line">res0: String = Tom:21 years old</span><br><span class="line"></span><br><span class="line">scala&gt; s.getAge(21)</span><br><span class="line">&lt;console&gt;:14: error: type mismatch;</span><br><span class="line"> found   : Int(21)</span><br><span class="line"> required: String</span><br><span class="line">       s.getAge(21)</span><br><span class="line">                ^</span><br></pre></td></tr></table></figure><p><br><br><strong>泛型函数</strong>，与泛型类类似，可以给某个函数在声明时指定泛型类型。<br>与泛型类一样，可以通过给使用了泛型类型的变量传递值来让Scala自动推断泛型的实际类型，也可以在调用函数时，手动指定泛型类型。<br>例如：创建一个函数，既能创建Int类型的数据，也能创建String类型的数组<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; import scala.reflect.ClassTag</span><br><span class="line">import scala.reflect.ClassTag</span><br><span class="line"></span><br><span class="line">scala&gt; def mkArray[T:ClassTag](elems:T*) = Array[T](elems:_*)</span><br><span class="line">mkArray: [T](elems: T*)(implicit evidence$1: scala.reflect.ClassTag[T])Array[T]</span><br><span class="line"></span><br><span class="line">scala&gt; mkArray(1,2,3,4,5)</span><br><span class="line">res0: Array[Int] = Array(1, 2, 3, 4, 5)</span><br><span class="line"></span><br><span class="line">scala&gt; mkArray(&quot;Tom&quot;,&quot;Bob&quot;,&quot;Alice&quot;)</span><br><span class="line">res1: Array[String] = Array(Tom, Bob, Alice)</span><br></pre></td></tr></table></figure></p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Scala中可以使用类型参数来实现类和函数，这样的类和函数可以用于多种类型。比如Array[T]可以存放任意指定类型T的数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;泛型类&lt;/strong&gt;就是在类的声明中，定义一些泛型类型，然后在类内部，就可以使用这些泛型类型。直接给使用了泛型类型的字段赋值时，Scala会自动进行类型推断。使用泛型类，通常是需要对类中的某些成员，比如某些字段和方法中的参数或变量进行统一的类型限制，这样可以保证程序的健壮性和稳定性。&lt;/p&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
  </entry>
  
</feed>
