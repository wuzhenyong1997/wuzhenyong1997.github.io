<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>GGSTU</title>
  
  <subtitle>Good Good Study</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.ggstu.com/"/>
  <updated>2018-09-10T03:32:10.908Z</updated>
  <id>https://www.ggstu.com/</id>
  
  <author>
    <name>Wu Zhenyong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark中transformation的cogroup算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84cogroup%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/10/Spark中transformation的cogroup算子的使用（Scala代码）/</id>
    <published>2018-09-10T03:02:43.000Z</published>
    <updated>2018-09-10T03:32:10.908Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84cogroup%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的cogroup算子的使用（Java代码）</a>这篇文章中用Java代码实现了cogroup算子打印出每个学生姓名对应的多门成绩，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object CogroupTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;CogroupTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf);</span><br><span class="line">    </span><br><span class="line">    val nameList = Array(</span><br><span class="line">      Tuple2(1, &quot;Tom&quot;),</span><br><span class="line">      Tuple2(2, &quot;Bob&quot;),</span><br><span class="line">      Tuple2(3, &quot;Alice&quot;)</span><br><span class="line">    )</span><br><span class="line">    val scoreList = Array(</span><br><span class="line">      Tuple2(1, 80),</span><br><span class="line">      Tuple2(1, 81),</span><br><span class="line">      Tuple2(1, 82),</span><br><span class="line">      Tuple2(2, 90),</span><br><span class="line">      Tuple2(2, 92),</span><br><span class="line">      Tuple2(2, 94),</span><br><span class="line">      Tuple2(3, 60),</span><br><span class="line">      Tuple2(3, 70),</span><br><span class="line">      Tuple2(3, 80)    </span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    val names = sc.parallelize(nameList)</span><br><span class="line">    val scores = sc.parallelize(scoreList)</span><br><span class="line">    </span><br><span class="line">    val studentScore = names.cogroup(scores)</span><br><span class="line">    </span><br><span class="line">    studentScore.foreach&#123;t =&gt; </span><br><span class="line">      println(&quot;ID:&quot;+t._1+ &quot;\t&quot; +&quot;Name:&quot;+t._2._1.mkString(&quot;&quot;)+ &quot;\t&quot; +&quot;Score:&quot;+t._2._2.mkString(&quot;,&quot;))</span><br><span class="line">      println(&quot;*****************************************&quot;)  </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用cogroup打印出每个学生姓名对应的多门成绩<br><img src="http://pd8lpasbc.bkt.clouddn.com/66-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84cogroup%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的cogroup算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了cogroup算子打印出每个学生姓名对应的多门成绩，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的cogroup算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84cogroup%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/10/Spark中transformation的cogroup算子的使用（Java代码）/</id>
    <published>2018-09-10T02:13:04.000Z</published>
    <updated>2018-09-10T02:55:31.515Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对cogroup算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/65-1.png" width="100%" height="100%"><br>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable&lt;V>,Iterable&lt;W>))类型的RDD<br><a id="more"></a><br>例如：已知两个序列，(学号,姓名)，(学号,成绩)，这里同一个学生有多门成绩，打印出每个学生姓名对应的成绩<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class CogroupTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;CogroupTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;Integer, String&gt;&gt; nameList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(1, &quot;Tom&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(2, &quot;Bob&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(3, &quot;Alice&quot;)</span><br><span class="line">);</span><br><span class="line">List&lt;Tuple2&lt;Integer, Integer&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(1, 80),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(1, 81),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(1, 82),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(2, 90),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(2, 92),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(2, 94),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(3, 60),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(3, 70),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(3, 80)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; names = sc.parallelizePairs(nameList);</span><br><span class="line">JavaPairRDD&lt;Integer, Integer&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, Tuple2&lt;Iterable&lt;String&gt;, Iterable&lt;Integer&gt;&gt;&gt; studentScore = names.cogroup(scores);</span><br><span class="line"></span><br><span class="line">studentScore.foreach(new VoidFunction&lt;Tuple2&lt;Integer,Tuple2&lt;Iterable&lt;String&gt;,Iterable&lt;Integer&gt;&gt;&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;Integer, Tuple2&lt;Iterable&lt;String&gt;, Iterable&lt;Integer&gt;&gt;&gt; t) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;ID:&quot;+t._1+ &quot;\t&quot; +&quot;Name:&quot;+t._2._1+ &quot;\t&quot; +&quot;Score:&quot;+t._2._2);</span><br><span class="line">System.out.println(&quot;*****************************************&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用cogroup打印出每个学生姓名对应的多门成绩<br><img src="http://pd8lpasbc.bkt.clouddn.com/65-2.png" width="60%" height="60%"><br>同样的例子，如果使用join算子进行操作，得到如下结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/65-3.png" width="60%" height="60%"><br>这里就能明显的看出join算子和cogroup算子的区别<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对cogroup算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/65-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable&amp;lt;V&gt;,Iterable&amp;lt;W&gt;))类型的RDD&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的join算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84join%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/10/Spark中transformation的join算子的使用（Scala代码）/</id>
    <published>2018-09-10T01:41:04.000Z</published>
    <updated>2018-09-10T01:58:12.196Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84join%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的join算子的使用（Java代码）</a>这篇文章中用Java代码实现了join算子打印出学生成绩，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object JoinTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;JoinTest&quot;)</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line">      </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">  </span><br><span class="line">    val nameList = Array(</span><br><span class="line">      Tuple2(1, &quot;Tom&quot;),</span><br><span class="line">      Tuple2(2, &quot;Bob&quot;),</span><br><span class="line">      Tuple2(3, &quot;Alice&quot;),</span><br><span class="line">      Tuple2(4, &quot;Jack&quot;),</span><br><span class="line">      Tuple2(5, &quot;Jerry&quot;)</span><br><span class="line">    )</span><br><span class="line">    val scoreList = Array(</span><br><span class="line">      Tuple2(1, 80),</span><br><span class="line">      Tuple2(2, 75),</span><br><span class="line">      Tuple2(3, 92),</span><br><span class="line">      Tuple2(4, 85),</span><br><span class="line">      Tuple2(5, 60)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    val names = sc.parallelize(nameList)</span><br><span class="line">    val scores = sc.parallelize(scoreList)</span><br><span class="line">    </span><br><span class="line">    val studentScore = names.join(scores)</span><br><span class="line">    </span><br><span class="line">    studentScore.foreach&#123;t =&gt;</span><br><span class="line">      println(&quot;ID:&quot;+t._1+ &quot;\t&quot; +&quot;Name:&quot;+t._2._1+ &quot;\t&quot; +&quot;Score:&quot;+t._2._2)</span><br><span class="line">      println(&quot;********************************&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用join打印出学生成绩<br><img src="http://pd8lpasbc.bkt.clouddn.com/64-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84join%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的join算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了join算子打印出学生成绩，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的join算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84join%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/10/Spark中transformation的join算子的使用（Java代码）/</id>
    <published>2018-09-10T01:10:01.000Z</published>
    <updated>2018-09-10T01:34:47.895Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对join算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/63-1.png" width="100%" height="100%"><br>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD<br><a id="more"></a><br>例如：已知两个序列，(学号,姓名)，(学号,成绩)，打印出每个学生姓名对应的成绩<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class JoinTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;JoinTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;Integer, String&gt;&gt; nameList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(1, &quot;Tom&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(2, &quot;Bob&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(3, &quot;Alice&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(4, &quot;Jack&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(5, &quot;Jerry&quot;)</span><br><span class="line">);</span><br><span class="line">List&lt;Tuple2&lt;Integer, Integer&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(1, 80),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(2, 75),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(3, 92),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(4, 85),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(5, 60)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; names = sc.parallelizePairs(nameList);</span><br><span class="line">JavaPairRDD&lt;Integer, Integer&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, Tuple2&lt;String, Integer&gt;&gt; studentScore = names.join(scores);</span><br><span class="line"></span><br><span class="line">studentScore.foreach(new VoidFunction&lt;Tuple2&lt;Integer,Tuple2&lt;String,Integer&gt;&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;Integer, Tuple2&lt;String, Integer&gt;&gt; t) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;ID:&quot;+t._1+ &quot;\t&quot; +&quot;Name:&quot;+t._2._1+ &quot;\t&quot; +&quot;Score:&quot;+t._2._2);</span><br><span class="line">System.out.println(&quot;************************************&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用join打印出学生成绩<br><img src="http://pd8lpasbc.bkt.clouddn.com/63-2.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对join算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/63-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的sortByKey算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84sortByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/10/Spark中transformation的sortByKey算子的使用（Scala代码）/</id>
    <published>2018-09-10T00:10:40.000Z</published>
    <updated>2018-09-10T00:59:15.173Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/09/Spark%E4%B8%ADtransformation%E7%9A%84sortByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的sortByKey算子的使用（Java代码）</a>这篇文章中用Java代码实现了sortByKey算子对学生成绩进行了升序和降序排序，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object SortByKeyTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">          .setAppName(&quot;SortByKeyTest&quot;)</span><br><span class="line">          .setMaster(&quot;local&quot;)</span><br><span class="line">          </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val scoreList = Array(</span><br><span class="line">      Tuple2(80, &quot;Tom&quot;),</span><br><span class="line">      Tuple2(75, &quot;Bob&quot;),</span><br><span class="line">      Tuple2(92, &quot;Alice&quot;),</span><br><span class="line">      Tuple2(85, &quot;Jack&quot;),</span><br><span class="line">      Tuple2(60, &quot;Jerry&quot;)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    val scores = sc.parallelize(scoreList)</span><br><span class="line">    </span><br><span class="line">    val sortedScore = scores.sortByKey()</span><br><span class="line">    </span><br><span class="line">    sortedScore.foreach&#123;score =&gt;</span><br><span class="line">      println(&quot;Name: &quot; + score._2)</span><br><span class="line">      println(&quot;Score: &quot; + score._1)</span><br><span class="line">      println(&quot;********************&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">        </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用sortByKey算子对学生成绩进行了升序排序<br><img src="http://pd8lpasbc.bkt.clouddn.com/62-1.png" width="60%" height="60%"><br>按成绩降序排序，将sortByKey()改成sortByKey(false)即可，修改后结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/62-2.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/09/Spark%E4%B8%ADtransformation%E7%9A%84sortByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的sortByKey算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了sortByKey算子对学生成绩进行了升序和降序排序，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的sortByKey算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/09/Spark%E4%B8%ADtransformation%E7%9A%84sortByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/09/Spark中transformation的sortByKey算子的使用（Java代码）/</id>
    <published>2018-09-09T00:01:31.000Z</published>
    <updated>2018-09-09T00:29:46.285Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对sortByKey算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/61-1.png" width="100%" height="100%"><br>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD<br><a id="more"></a><br>例如：对学生成绩进行排序（升序、降序）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class SortByKeyTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;SortByKeyTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;Integer, String&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(80, &quot;Tom&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(75, &quot;Bob&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(92, &quot;Alice&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(85, &quot;Jack&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(60, &quot;Jerry&quot;)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; sortedScore = scores.sortByKey();</span><br><span class="line"></span><br><span class="line">sortedScore.foreach(new VoidFunction&lt;Tuple2&lt;Integer,String&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;Integer, String&gt; score) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;Name: &quot; + score._2);</span><br><span class="line">System.out.println(&quot;Score: &quot; + score._1);</span><br><span class="line">System.out.println(&quot;*************************&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用sortByKey对学生成绩进行升序排序<br><img src="http://pd8lpasbc.bkt.clouddn.com/61-2.png" width="60%" height="60%"></p><p>如果要降序排序，将sortByKey()改成sortByKey(false)即可，修改后结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/61-3.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对sortByKey算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/61-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的reduceByKey算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/08/Spark%E4%B8%ADtransformation%E7%9A%84reduceByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/08/Spark中transformation的reduceByKey算子的使用（Scala代码）/</id>
    <published>2018-09-08T12:00:22.000Z</published>
    <updated>2018-09-08T12:28:36.739Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/08/Spark%E4%B8%ADtransformation%E7%9A%84reduceByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的reduceByKey算子的使用（Java代码）</a>这篇文章中用Java代码实现了reduceByKey算子统计每个学生的成绩总分，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object ReduceByKeyTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;ReduceByKeyTest&quot;)</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line">      </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">  </span><br><span class="line">    val scoreList = Array(</span><br><span class="line">     Tuple2(&quot;Tom&quot;, 85),</span><br><span class="line">     Tuple2(&quot;Bob&quot;, 75),</span><br><span class="line">     Tuple2(&quot;Alice&quot;, 92),</span><br><span class="line">     Tuple2(&quot;Tom&quot;, 70),</span><br><span class="line">     Tuple2(&quot;Bob&quot;, 95),</span><br><span class="line">     Tuple2(&quot;Alice&quot;, 80)</span><br><span class="line">    )</span><br><span class="line">  </span><br><span class="line">    val scores = sc.parallelize(scoreList)</span><br><span class="line">  </span><br><span class="line">    val totalScore = scores.reduceByKey(_ + _)</span><br><span class="line">  </span><br><span class="line">    totalScore.foreach&#123;score =&gt;</span><br><span class="line">      println(&quot;Name: &quot; + score._1)</span><br><span class="line">      println(&quot;Total Score: &quot; + score._2)</span><br><span class="line">      println(&quot;************************&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用reduceByKey算子统计出每个学生的成绩总分<br><img src="http://pd8lpasbc.bkt.clouddn.com/60-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/08/Spark%E4%B8%ADtransformation%E7%9A%84reduceByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的reduceByKey算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了reduceByKey算子统计每个学生的成绩总分，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的reduceByKey算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/08/Spark%E4%B8%ADtransformation%E7%9A%84reduceByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/08/Spark中transformation的reduceByKey算子的使用（Java代码）/</id>
    <published>2018-09-08T05:25:40.000Z</published>
    <updated>2018-09-08T11:49:54.517Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对reduceByKey算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/59-1.png" width="100%" height="100%"><br>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置<br><a id="more"></a><br>例如：统计每个学生的成绩总分<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class ReduceByKeyTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;ReduceByKeyTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;String,Integer&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Tom&quot;, 85),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Bob&quot;, 75),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Alice&quot;, 92),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Tom&quot;, 70),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Bob&quot;, 95),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Alice&quot;, 80)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; totalScore = scores.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer score1, Integer score2) throws Exception &#123;</span><br><span class="line">return score1 + score2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">totalScore.foreach(new VoidFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;String, Integer&gt; score) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;Name: &quot; + score._1);</span><br><span class="line">System.out.println(&quot;Total Score: &quot; + score._2);</span><br><span class="line">System.out.println(&quot;**************************&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用reduceByKey统计出每个学生的成绩总分<br><img src="http://pd8lpasbc.bkt.clouddn.com/59-2.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对reduceByKey算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/59-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的groupByKey算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/08/Spark%E4%B8%ADtransformation%E7%9A%84groupByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/08/Spark中transformation的groupByKey算子的使用（Scala代码）/</id>
    <published>2018-09-07T16:39:17.000Z</published>
    <updated>2018-09-08T12:01:12.107Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/07/Spark%E4%B8%ADtransformation%E7%9A%84groupByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的groupByKey算子的使用（Java代码）</a>这篇文章中用Java代码实现了groupByKey算子对每个学生的成绩进行分组，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object GroupByKeyTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;GroupByKeyTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val scoreList = Array(</span><br><span class="line">        Tuple2(&quot;Tom&quot;, 85),</span><br><span class="line">        Tuple2(&quot;Bob&quot;, 75),</span><br><span class="line">        Tuple2(&quot;Alice&quot;, 92),</span><br><span class="line">        Tuple2(&quot;Tom&quot;, 70),</span><br><span class="line">        Tuple2(&quot;Bob&quot;, 95),</span><br><span class="line">        Tuple2(&quot;Alice&quot;, 80)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    val scores = sc.parallelize(scoreList)</span><br><span class="line">    </span><br><span class="line">    val groupedScores = scores.groupByKey()</span><br><span class="line">    </span><br><span class="line">    groupedScores.foreach(score =&gt; &#123;</span><br><span class="line">      println(&quot;Name: &quot; + score._1)</span><br><span class="line">      score._2.foreach(subjectScore =&gt; println(subjectScore))</span><br><span class="line">      println(&quot;******************&quot;)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用groupByKey算子对每个学生的成绩进行分组<br><img src="http://pd8lpasbc.bkt.clouddn.com/58-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/07/Spark%E4%B8%ADtransformation%E7%9A%84groupByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的groupByKey算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了groupByKey算子对每个学生的成绩进行分组，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的groupByKey算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/07/Spark%E4%B8%ADtransformation%E7%9A%84groupByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/07/Spark中transformation的groupByKey算子的使用（Java代码）/</id>
    <published>2018-09-07T15:38:42.000Z</published>
    <updated>2018-09-07T16:27:10.629Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对groupByKey算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/57-1.png" width="100%" height="100%"><br>在一个(K,V)的RDD上调用，返回一个(K, Iterable<v>)的RDD<br><a id="more"></a><br>例如：对每个学生的成绩进行分组<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class GroupByKeyTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;GroupByKeyTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;String,Integer&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Tom&quot;, 85),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Bob&quot;, 75),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Alice&quot;, 92),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Tom&quot;, 70),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Bob&quot;, 95),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Alice&quot;, 80)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Iterable&lt;Integer&gt;&gt; groupedScores = scores.groupByKey();</span><br><span class="line"></span><br><span class="line">groupedScores.foreach(new VoidFunction&lt;Tuple2&lt;String,Iterable&lt;Integer&gt;&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;String, Iterable&lt;Integer&gt;&gt; score) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;Name: &quot; + score._1);</span><br><span class="line">Iterator&lt;Integer&gt; ite = score._2.iterator();</span><br><span class="line">while(ite.hasNext()) &#123;</span><br><span class="line">System.out.println(ite.next());</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(&quot;******************************&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></v></p><p>在本地IDE执行后，得到如下结果，即，使用groupByKey操作对每个学生的成绩进行分组<br><img src="http://pd8lpasbc.bkt.clouddn.com/57-2.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对groupByKey算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/57-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在一个(K,V)的RDD上调用，返回一个(K, Iterable&lt;v&gt;)的RDD&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的flatMap算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/07/Spark%E4%B8%ADtransformation%E7%9A%84flatMap%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/07/Spark中transformation的flatMap算子的使用（Scala代码）/</id>
    <published>2018-09-07T00:16:22.000Z</published>
    <updated>2018-09-07T00:36:58.399Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84flatMap%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的flatMap算子的使用（Java代码）</a>这篇文章中用Java代码实现了flatMap算子将文本行拆分成一个个单词，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br>同样，在桌面创建了个文件test.txt，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object FlatMapTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;FlatMapTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    words.foreach(word =&gt; println(word))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用flatMap算子将文本行拆分成一个个单词<br><img src="http://pd8lpasbc.bkt.clouddn.com/56-1.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84flatMap%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的flatMap算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了flatMap算子将文本行拆分成一个个单词，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的flatMap算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84flatMap%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/06/Spark中transformation的flatMap算子的使用（Java代码）/</id>
    <published>2018-09-06T14:46:23.000Z</published>
    <updated>2018-09-06T15:06:06.941Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对flatMap算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/55-1.png" width="100%" height="100%"><br>flatMap类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）<br><a id="more"></a><br>例如：将文本行拆分成一个个单词<br>我在桌面创建了个文件test.txt，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">public class FlatMapTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;FlatMapTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">    JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line">    </span><br><span class="line">    JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;);</span><br><span class="line">    </span><br><span class="line">    JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">    </span><br><span class="line">    words.foreach(new VoidFunction&lt;String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(String word) throws Exception &#123;</span><br><span class="line">System.out.println(word);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">    </span><br><span class="line">    sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用flatMap操作将文本行拆分成一个个单词<br><img src="http://pd8lpasbc.bkt.clouddn.com/55-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对flatMap算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/55-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;flatMap类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的filter算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84filter%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/06/Spark中transformation的filter算子的使用（Scala代码）/</id>
    <published>2018-09-06T11:09:57.000Z</published>
    <updated>2018-09-06T11:27:09.626Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84filter%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的filter算子的使用（Java代码）</a>这篇文章中用Java代码实现了filter算子过滤出集合中的所有偶数，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object FilterTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;FilterTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numbers = Array(1,2,3,4,5,6,7,8,9,10)</span><br><span class="line">    </span><br><span class="line">    val numberRDD = sc.parallelize(numbers)</span><br><span class="line">    </span><br><span class="line">    val resultRDD = numberRDD.filter(num =&gt; num%2 == 0)</span><br><span class="line">    </span><br><span class="line">    resultRDD.foreach(res =&gt; println(res))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用filter算子过滤出集合中的所有偶数<br><img src="http://pd8lpasbc.bkt.clouddn.com/54-1.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84filter%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的filter算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了filter算子过滤出集合中的所有偶数，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的filter算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84filter%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/06/Spark中transformation的filter算子的使用（Java代码）/</id>
    <published>2018-09-06T02:54:46.000Z</published>
    <updated>2018-09-06T03:21:00.283Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对filter算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/53-1.png" width="100%" height="100%"><br>意思就是，返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成<br><a id="more"></a><br>例如：过滤出集合中的所有偶数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">public class FilterTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;FilterTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numbers = Arrays.asList(1,2,3,4,5,6,7,8,9,10);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numberRDD = sc.parallelize(numbers);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; resultRDD = numberRDD.filter(new Function&lt;Integer, Boolean&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Boolean call(Integer num) throws Exception &#123;</span><br><span class="line">return num%2 == 0;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">resultRDD.foreach(new VoidFunction&lt;Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Integer res) throws Exception &#123;</span><br><span class="line">System.out.println(res);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用filter操作过滤出一个集合中的所有偶数<br><img src="http://pd8lpasbc.bkt.clouddn.com/53-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对filter算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/53-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;意思就是，返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的map算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84map%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/06/Spark中transformation的map算子的使用（Scala代码）/</id>
    <published>2018-09-05T23:56:05.000Z</published>
    <updated>2018-09-06T00:20:08.177Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/05/Spark%E4%B8%ADtransformation%E7%9A%84map%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的map算子的使用（Java代码）</a>这篇文章中用Java代码实现了map算子对集合中的每个元素开平方，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object MapTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;MapTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numbers = Array(1,2,3,4,5,6,7,8,9,10)</span><br><span class="line">    </span><br><span class="line">    val numberRDD = sc.parallelize(numbers)</span><br><span class="line">    </span><br><span class="line">    val resultRDD = numberRDD.map(num =&gt; num*num)</span><br><span class="line">    </span><br><span class="line">    resultRDD.foreach(res =&gt; println(res))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用map操作将一个集合中的每个元素开平方<br><img src="http://pd8lpasbc.bkt.clouddn.com/52-1.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/05/Spark%E4%B8%ADtransformation%E7%9A%84map%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的map算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了map算子对集合中的每个元素开平方，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的map算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/05/Spark%E4%B8%ADtransformation%E7%9A%84map%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/05/Spark中transformation的map算子的使用（Java代码）/</id>
    <published>2018-09-05T15:14:42.000Z</published>
    <updated>2018-09-06T00:20:06.748Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对map算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/51-1.png" width="100%" height="100%"><br>意思就是，返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成。<br><a id="more"></a><br>例如：将集合中的每个元素开平方<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">public class MapTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;MapTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numbers = Arrays.asList(1,2,3,4,5,6,7,8,9,10);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numberRDD = sc.parallelize(numbers);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; resultRDD = numberRDD.map(new Function&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer num) throws Exception &#123;</span><br><span class="line">return num * num;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">resultRDD.foreach(new VoidFunction&lt;Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Integer res) throws Exception &#123;</span><br><span class="line">System.out.println(res);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用map操作将一个集合中的每个元素开平方<br><img src="http://pd8lpasbc.bkt.clouddn.com/51-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对map算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/51-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;意思就是，返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中RDD的transformation操作和action操作介绍</title>
    <link href="https://www.ggstu.com/2018/09/05/Spark%E4%B8%ADRDD%E7%9A%84transformation%E6%93%8D%E4%BD%9C%E5%92%8Caction%E6%93%8D%E4%BD%9C%E4%BB%8B%E7%BB%8D/"/>
    <id>https://www.ggstu.com/2018/09/05/Spark中RDD的transformation操作和action操作介绍/</id>
    <published>2018-09-05T07:48:35.000Z</published>
    <updated>2018-09-05T12:10:25.878Z</updated>
    
    <content type="html"><![CDATA[<p><b>Spark支持两种RDD操作，transformation和action</b></p><p><b>transformation</b>：针对已有的RDD创建一个新的RDD。比如map就是一种transformation操作，它用于将已有RDD的每个元素传入一个自定义的函数，并获取一个新的元素，然后将所有的新元素组成一个新的RDD。</p><p><b>action</b>：主要是对RDD进行最后的操作，比如遍历、reduce、保存到文件等，并将结果返回给Driver程序。</p><a id="more"></a><font size="4"><b>特点</b></font><br>transformation的特点就是<b>延迟加载</b>特性。如果一个spark应用中只定义了transformation操作，那么即使执行了该应用，这些操作也不会执行。也就是说，transformation不会触发spark程序的执行，它们只是记录了对RDD所做的操作，但是不会自发的执行。只有当transformation之后接着执行了一个action操作，那么所有的transformation才会执行。Spark通过这种延迟加载特性，避免产生过多的中间结果，从而让Spark更加高效的运行。<br>action操作执行，会触发一个spark job的运行，从而触发这个action之前所有的transformation的执行。<br><br><br>例如以之前写的Scala版本的Spark的wordcount程序为例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object WordCountLocal &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;WordCountLocal&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    val pairs = words.map(word =&gt; (word,1))</span><br><span class="line">    </span><br><span class="line">    val wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line">    </span><br><span class="line">    wordCounts.foreach(wordCount =&gt; println(wordCount._1 + &quot;: &quot; + wordCount._2))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><br>使用textFile创建一个RDD后，其后的flatMap、map、reduceByKey都是transformation操作，不会触发程序的执行，只有执行了最后的foreach操作，前面的transformation操作才开始执行。<br><br><br><font size="4"><b>transformation介绍</b></font><table><thead><tr><th style="text-align:left">transformation</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:left">map(func)</td><td style="text-align:left">返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</td></tr><tr><td style="text-align:left">filter(func)</td><td style="text-align:left">返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成</td></tr><tr><td style="text-align:left">flatMap(func)</td><td style="text-align:left">类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）</td></tr><tr><td style="text-align:left">mapPartitions(func)</td><td style="text-align:left">类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator<t> =&gt; Iterator<u></u></t></td></tr><tr><td style="text-align:left">mapPartitionsWithIndex(func)</td><td style="text-align:left">类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Iterator<t>) =&gt; Iterator<u></u></t></td></tr><tr><td style="text-align:left">sample(withReplacement,fraction,seed)</td><td style="text-align:left">根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子</td></tr><tr><td style="text-align:left">union(otherDataset)</td><td style="text-align:left">对源RDD和参数RDD求并集后返回一个新的RDD</td></tr><tr><td style="text-align:left">intersection(otherDataset)</td><td style="text-align:left">对源RDD和参数RDD求交集后返回一个新的RDD</td></tr><tr><td style="text-align:left">distinct([numPartitions]))</td><td style="text-align:left">对源RDD进行去重后返回一个新的RDD</td></tr><tr><td style="text-align:left">groupByKey([numPartitions])</td><td style="text-align:left">在一个(K,V)的RDD上调用，返回一个(K, Iterable&lt;V>)的RDD</td></tr><tr><td style="text-align:left">reduceByKey(func, [numPartitions])</td><td style="text-align:left">在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置</td></tr><tr><td style="text-align:left">sortByKey([ascending], [numPartitions])</td><td style="text-align:left">在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</td></tr><tr><td style="text-align:left">join(otherDataset, [numPartitions])</td><td style="text-align:left">在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD</td></tr><tr><td style="text-align:left">cogroup(otherDataset, [numPartitions])</td><td style="text-align:left">在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable&lt;V>,Iterable&lt;W>))类型的RDD</td></tr></tbody></table><p><br></p><font size="4"><b>action介绍</b></font><table><thead><tr><th style="text-align:left">action</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:left">reduce(func)</td><td style="text-align:left">通过func函数聚集RDD中的所有元素，这个函数必须是可交换且可并联的</td></tr><tr><td style="text-align:left">collect(func)</td><td style="text-align:left">在驱动程序中，以数组的形式返回数据集的所有元素</td></tr><tr><td style="text-align:left">count()</td><td style="text-align:left">返回RDD的元素个数</td></tr><tr><td style="text-align:left">first()</td><td style="text-align:left">返回RDD的第一个元素（类似于take(1)）</td></tr><tr><td style="text-align:left">take(n)</td><td style="text-align:left">返回一个由数据集的前n个元素组成的数组</td></tr><tr><td style="text-align:left">takeSample(withReplacement, num, [seed])</td><td style="text-align:left">返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</td></tr><tr><td style="text-align:left">takeOrdered(n, [ordering])</td><td style="text-align:left">返回使用自然顺序或自定义比较器的RDD的前n个元素</td></tr><tr><td style="text-align:left">saveAsTextFile(path)</td><td style="text-align:left">将数据集的元素以text file的形式保存到HDFS文件系统或者其它支持的文件系统，对于每个元素，Spark会调用toString方法，将它转换为文件中的文本</td></tr><tr><td style="text-align:left">saveAsSequenceFile(path)</td><td style="text-align:left">将数据集中的元素以Hadoop SequenceFile的格式保存到指定的目录下，可以是HDFS或者其它Hadoop支持的文件系统</td></tr><tr><td style="text-align:left">countByKey()</td><td style="text-align:left">针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数</td></tr><tr><td style="text-align:left">foreach(func)</td><td style="text-align:left">在数据集的每个元素上，运行func函数进行更新</td></tr></tbody></table><p><br><br>之后的文章会分别使用Java代码和Scala代码来示范常用的transformation算子和action算子的使用。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;b&gt;Spark支持两种RDD操作，transformation和action&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;transformation&lt;/b&gt;：针对已有的RDD创建一个新的RDD。比如map就是一种transformation操作，它用于将已有RDD的每个元素传入一个自定义的函数，并获取一个新的元素，然后将所有的新元素组成一个新的RDD。&lt;/p&gt;
&lt;p&gt;&lt;b&gt;action&lt;/b&gt;：主要是对RDD进行最后的操作，比如遍历、reduce、保存到文件等，并将结果返回给Driver程序。&lt;/p&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark弹性分布式数据集RDD介绍</title>
    <link href="https://www.ggstu.com/2018/09/05/Spark%E5%BC%B9%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86RDD%E4%BB%8B%E7%BB%8D/"/>
    <id>https://www.ggstu.com/2018/09/05/Spark弹性分布式数据集RDD介绍/</id>
    <published>2018-09-05T02:47:09.000Z</published>
    <updated>2018-09-05T05:40:44.753Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>什么是RDD?</b></font><br>RDD(Resilient Distributed Dataset)，叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表了一个不可变、可分区、其中的元素可并行计算的集合。RDD既有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作机缓存在内存中，后续的查询操作能够重用工作机，因此能够大大提高查询速度。<br><a id="more"></a><br><br></p><p><font size="4"><b>RDD的属性</b></font><br>在Spark源码中有RDD属性的描述<br><img src="http://pd8lpasbc.bkt.clouddn.com/49-1.png" width="100%" height="100%"><br><b>1、一组分片（Partition）</b>，即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p><p><b>2、一个计算每个分区的函数</b>。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的，compute函数会对迭代器进行复合，不需要保存每次计算的结果。</p><p><b>3、RDD之间的依赖关系</b>。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p><p><b>4、一个Partitioner（即RDD的分片函数）</b>。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于key-value的RDD，才会有Partitioner，非key-value的RDD的Partitioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p><p><b>5、一个列表</b>，存储每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。<br><br></p><p><font size="4"><b>RDD的基本原理</b></font><br>创建一个RDD：val rdd = sc.parallelize(Array(1,2,3,4,5,6,7,8),3)<br>这个RDD创建了3个分区，一个分区运行在一个Worker节点上，但是实际上一个Worker上可以运行多个分区。<br>下图的红框标注的就是一个RDD。<br><img src="http://pd8lpasbc.bkt.clouddn.com/49-2.png" width="100%" height="100%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;什么是RDD?&lt;/b&gt;&lt;/font&gt;&lt;br&gt;RDD(Resilient Distributed Dataset)，叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表了一个不可变、可分区、其中的元素可并行计算的集合。RDD既有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作机缓存在内存中，后续的查询操作能够重用工作机，因此能够大大提高查询速度。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark创建RDD的几种方式（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/03/Spark%E5%88%9B%E5%BB%BARDD%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/03/Spark创建RDD的几种方式（Scala代码）/</id>
    <published>2018-09-03T11:01:43.000Z</published>
    <updated>2018-09-03T12:54:21.008Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/03/Spark%E5%88%9B%E5%BB%BARDD%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark创建RDD的几种方式（Java代码）</a>这篇文章中介绍了Spark创建RDD的三种方式的Java代码。<br>下面介绍Scala版本的创建RDD，直接上代码。</p><p><font size="4"><b>并行化集合创建RDD</b></font><br>例如：累加1到10<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object ParallelizeCollection &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">          .setAppName(&quot;ParallelizeCollection&quot;)</span><br><span class="line">          .setMaster(&quot;local&quot;)</span><br><span class="line">          </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numbers = Array(1,2,3,4,5,6,7,8,9,10)</span><br><span class="line">    </span><br><span class="line">    val numberRDD = sc.parallelize(numbers)</span><br><span class="line">    </span><br><span class="line">    val sum = numberRDD.reduce(_ + _)</span><br><span class="line">    </span><br><span class="line">    println(&quot;1+2+3+......+10 = &quot; + sum)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行程序，在控制台上看到如下所示的结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/48-1.png" width="60%" height="60%"><br><br></p><p><font size="4"><b>使用本地文件创建RDD</b></font><br>例如：累加1到10<br>我在本地桌面上创建了个文本文件，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 2 3 4 5 6 7 8 9 10</span><br></pre></td></tr></table></figure></p><p>代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object LocalFile &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">          .setAppName(&quot;LocalFile&quot;)</span><br><span class="line">          .setMaster(&quot;local&quot;)</span><br><span class="line">          </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val nums = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    val numsToInt = nums.map(num =&gt; num.toInt)</span><br><span class="line">    </span><br><span class="line">    val sum = numsToInt.reduce(_ + _)</span><br><span class="line">    </span><br><span class="line">    println(&quot;1+2+3+4+......+10 = &quot; + sum)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行程序，在控制台上看到如下所示的结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/48-2.png" width="60%" height="60%"><br><br></p><p><font size="4"><b>使用HDFS文件创建RDD</b></font><br>例如：累加1到10<br>在Linux上创建文本文件，文件内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd input/</span><br><span class="line">[root@ggstu input]# cat test.txt </span><br><span class="line">1 2 3 4 5 6 7 8 9 10</span><br></pre></td></tr></table></figure></p><p>启动hadoop集群，将test.txt上传到hdfs上<br>接着启动spark集群<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu input]# hdfs dfs -put test.txt /test.txt</span><br><span class="line">[root@ggstu input]# hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 root supergroup         21 2018-09-03 20:43 /test.txt</span><br></pre></td></tr></table></figure></p><p>代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object HDFSFile &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">          .setAppName(&quot;HDFSFile&quot;)</span><br><span class="line">          </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;hdfs://ggstu:9000/test.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val nums = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    val numsToInt = nums.map(num =&gt; num.toInt)</span><br><span class="line">    </span><br><span class="line">    val sum = numsToInt.reduce(_ + _)</span><br><span class="line">    </span><br><span class="line">    println(&quot;1+2+3+4+5+......+10 = &quot; + sum)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>将程序打包成jar包，并上传到Linux上，之后执行执行脚本。<br>如果这几步不懂，在<a href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Scala%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/" target="_blank">使用Scala开发Spark的wordcount程序</a>这篇文章中有详细过程。<br>执行脚本结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/48-3.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/03/Spark%E5%88%9B%E5%BB%BARDD%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark创建RDD的几种方式（Java代码）&lt;/a&gt;这篇文章中介绍了Spark创建RDD的三种方式的Java代码。&lt;br&gt;下面介绍Scala版本的创建RDD，直接上代码。&lt;/p&gt;
&lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;并行化集合创建RDD&lt;/b&gt;&lt;/font&gt;&lt;br&gt;例如：累加1到10&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark创建RDD的几种方式（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/03/Spark%E5%88%9B%E5%BB%BARDD%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/03/Spark创建RDD的几种方式（Java代码）/</id>
    <published>2018-09-03T05:02:48.000Z</published>
    <updated>2018-09-03T07:20:36.594Z</updated>
    
    <content type="html"><![CDATA[<p>Spark核心编程，首先要做的是创建一个初始的RDD。该RDD通常包含了Spark应用程序的输入源数据。只有在创建了初始的RDD之后，才可以使用transformation算子，对该RDD进行转换，得到其它的RDD。<br>Spark Core提供了三种创建RDD的方式：<br>1、使用程序中的集合创建RDD<br>2、使用本地文件创建RDD<br>3、使用HDFS文件创建RDD<br><a id="more"></a></p><p><font size="4"><b>并行化集合创建RDD</b></font><br>使用并行化集合来创建RDD，需要对集合调用SparkContext的parallelize()方法。Spark会将集合中的数据拷贝到集群上去，形成一个分布式的数据集，也就是一个RDD。<br>例如：累加1到10<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line"></span><br><span class="line">public class ParallelizeCollection &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;ParallelizeCollection&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numbers = Arrays.asList(1,2,3,4,5,6,7,8,9,10);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numberRDD = sc.parallelize(numbers,3);</span><br><span class="line"></span><br><span class="line">int sum = numberRDD.reduce(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer num1, Integer num2) throws Exception &#123;</span><br><span class="line">return num1 + num2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;1+2+3+......+10 = &quot; + sum);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行程序，在控制台上看到如下所示的结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-1.png" width="50%" height="50%"><br>调用parallelize()时，可以指定一个参数将集合切分成多少个partition。Spark会为每一个partition运行一个task。<br>Spark官方的建议是，为集群中的每个CPU创建2~4个partition。Spark会根据集群的情况来设置partition的数量。也可以在调用parallelize()方法时，传入第二个参数，设置RDD的partition数量，比如parallelize(numbers,3)。<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-2.png" width="100%" height="100%"><br><br></p><p><font size="4"><b>使用本地文件创建RDD</b></font><br>Spark支持使用任何Hadoop支持的存储系统上的文件创建RDD的，比如HDFS、Cassandra、HBase以及本地文件。<br>通过调用SparkContext的textFile()方法，对本地文件创建RDD。<br>例如：累加1到10<br>我在本地桌面上创建了个文本文件，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 2 3 4 5 6 7 8 9 10</span><br></pre></td></tr></table></figure></p><p>代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line"></span><br><span class="line">public class LocalFile &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;LocalFile&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus/Desktop//test.txt&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; nums = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">String sum = nums.reduce(new Function2&lt;String, String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public String call(String num1, String num2) throws Exception &#123;</span><br><span class="line">return (Integer.parseInt(num1) + Integer.parseInt(num2))+&quot;&quot;;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;1+2+3+4+......+10 = &quot; + sum);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行程序，在控制台上看到如下所示的结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-3.png" width="60%" height="60%"><br><br></p><p><font size="4"><b>使用HDFS文件创建RDD</b></font><br>通过调用SparkContext的textFile()方法，对HDFS文件创建RDD。<br>例如：累加1到10<br>在Linux上创建文本文件，文件内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# cd input/</span><br><span class="line">[root@ggstu input]# cat test.txt </span><br><span class="line">1 2 3 4 5 6 7 8 9 10</span><br></pre></td></tr></table></figure></p><p>启动hadoop集群，将test.txt上传到hdfs上<br>接着启动spark集群<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu input]# hdfs dfs -put test.txt /test.txt</span><br><span class="line">[root@ggstu input]# hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 root supergroup         21 2018-09-03 14:26 /test.txt</span><br></pre></td></tr></table></figure></p><p>代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line"></span><br><span class="line">public class HDFSFile &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;HDFSFile&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;hdfs://ggstu:9000/test.txt&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; nums = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">String sum = nums.reduce(new Function2&lt;String, String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public String call(String num1, String num2) throws Exception &#123;</span><br><span class="line">return (Integer.parseInt(num1)+Integer.parseInt(num2))+&quot;&quot;;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;1+2+3+4+5+......+10 = &quot; + sum);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>将程序打包成jar包，并上传到Linux上，之后执行执行脚本。<br>如果这几步不懂，在<a href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Java%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/" target="_blank">使用Java开发Spark的wordcount程序</a>这篇文章中有详细过程。<br>执行脚本结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-4.png" width="50%" height="50%"><br><br></p><p><font size="4"><b>其它创建RDD方式</b></font><br>Spark的官方文档中可以看到除了通过使用textFile()方法创建RDD，还有如下几种方式：<br><img src="http://pd8lpasbc.bkt.clouddn.com/47-5.png" width="100%" height="100%"><br>1、SparkContext.wholeTextFiles方法，可以针对一个目录中大量的小文件，返回&lt;filename,content&gt;组成的pair，作为一个PairRDD，而不是普通的RDD。普通的textFile()返回的RDD是文件中的一行文本。<br>2、SparkContext.sequenceFile[K,V]方法，可以针对SequenceFile创建RDD，K和V泛型类型就是SequenceFile的key和value的类型。K和V要求必须是Hadoop的序列化类型，如IntWritable、Text等。<br>3、SparkContext.hadoopRDD方法，对于Hadoop的自定义输入类型，可以创建RDD。该方法接收JobConf、InputFormatClass、Key和Value的class。<br>4、SparkContext.objectFile方法，可以针对之前调用RDD.saveAsObjectFile创建的对象序列化的文件，反序列化文件中的数据，并创建一个RDD。</p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spark核心编程，首先要做的是创建一个初始的RDD。该RDD通常包含了Spark应用程序的输入源数据。只有在创建了初始的RDD之后，才可以使用transformation算子，对该RDD进行转换，得到其它的RDD。&lt;br&gt;Spark Core提供了三种创建RDD的方式：&lt;br&gt;1、使用程序中的集合创建RDD&lt;br&gt;2、使用本地文件创建RDD&lt;br&gt;3、使用HDFS文件创建RDD&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
</feed>
