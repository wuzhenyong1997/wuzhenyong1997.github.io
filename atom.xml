<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>GGSTU</title>
  
  <subtitle>Good Good Study</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.ggstu.com/"/>
  <updated>2018-09-12T05:24:11.954Z</updated>
  <id>https://www.ggstu.com/</id>
  
  <author>
    <name>Wu Zhenyong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark中action的take算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84take%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的take算子的使用（Java代码）/</id>
    <published>2018-09-12T05:11:36.000Z</published>
    <updated>2018-09-12T05:24:11.954Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对take算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/73-1.png" width="90%" height="90%"><br>返回一个由数据集的前n个元素组成的数组<br><a id="more"></a><br>例如：获取集合中的前3个元素<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">public class TakeTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;TakeTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;String&gt; nameList = Arrays.asList(&quot;Tom&quot;,&quot;Bob&quot;,&quot;Alice&quot;,&quot;Jack&quot;,&quot;Jerry&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; names = sc.parallelize(nameList);</span><br><span class="line"></span><br><span class="line">List&lt;String&gt; top3Names = names.take(3);</span><br><span class="line"></span><br><span class="line">for(String name : top3Names) &#123;</span><br><span class="line">System.out.println(name);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用take算子获取集合中的前3个元素<br><img src="http://pd8lpasbc.bkt.clouddn.com/73-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对take算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/73-1.png&quot; width=&quot;90%&quot; height=&quot;90%&quot;&gt;&lt;br&gt;返回一个由数据集的前n个元素组成的数组&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的count算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84count%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的count算子的使用（Scala代码）/</id>
    <published>2018-09-12T04:45:10.000Z</published>
    <updated>2018-09-12T05:06:15.549Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84count%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中action的count算子的使用（Java代码）</a>这篇文章中用Java代码实现了count算子获取集合元素总数，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object CountTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;CountTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5,6,7,8,9,10)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    val count = numbers.count()</span><br><span class="line">    </span><br><span class="line">    println(&quot;There are &quot; + count + &quot; numbers.&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用count算子获取集合元素总数<br><img src="http://pd8lpasbc.bkt.clouddn.com/72-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84count%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中action的count算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了count算子获取集合元素总数，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的count算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84count%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的count算子的使用（Java代码）/</id>
    <published>2018-09-12T04:24:15.000Z</published>
    <updated>2018-09-12T04:38:58.256Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对count算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/71-1.png" width="90%" height="90%"><br>返回RDD的元素个数<br><a id="more"></a><br>例如：获取集合元素总数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">public class CountTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;CountTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5,6,7,8,9,10);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">long count = numbers.count();</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;There are &quot; + count + &quot; numbers.&quot;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用count算子获取集合元素总数<br><img src="http://pd8lpasbc.bkt.clouddn.com/71-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对count算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/71-1.png&quot; width=&quot;90%&quot; height=&quot;90%&quot;&gt;&lt;br&gt;返回RDD的元素个数&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的collect算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84collect%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的collect算子的使用（Scala代码）/</id>
    <published>2018-09-12T01:19:32.000Z</published>
    <updated>2018-09-12T01:52:32.466Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84collect%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中action的collect算子的使用（Java代码）</a>这篇文章中用Java代码实现了collect算子打印集合中的元素，这里使用同样的例子，用Scala来开发。</p><a id="more"></a><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object CollectTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;CollectTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    val numList = numbers.collect()</span><br><span class="line"></span><br><span class="line">    for(num &lt;- numList)&#123;</span><br><span class="line">      println(num)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在本地Scala IDE执行后，得到如下结果，即，使用collect算子打印集合中的元素<br><img src="http://pd8lpasbc.bkt.clouddn.com/70-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84collect%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中action的collect算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了collect算子打印集合中的元素，这里使用同样的例子，用Scala来开发。&lt;/p&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的collect算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84collect%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的collect算子的使用（Java代码）/</id>
    <published>2018-09-12T00:32:16.000Z</published>
    <updated>2018-09-12T00:59:12.152Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对collect算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/69-1.png" width="100%" height="100%"><br>在驱动程序中，以数组的形式返回数据集的所有元素<br><a id="more"></a><br>例如：打印集合中的元素<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">public class CollectTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;CollectTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numList = numbers.collect();</span><br><span class="line"></span><br><span class="line">System.out.println(numList);</span><br><span class="line">for(Integer num : numList) &#123;</span><br><span class="line">System.out.println(num);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用collect算子打印出集合中的元素<br><img src="http://pd8lpasbc.bkt.clouddn.com/69-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对collect算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/69-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在驱动程序中，以数组的形式返回数据集的所有元素&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的reduce算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84reduce%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的reduce算子的使用（Scala代码）/</id>
    <published>2018-09-11T23:57:59.000Z</published>
    <updated>2018-09-12T00:25:13.616Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/11/Spark%E4%B8%ADaction%E7%9A%84reduce%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中action的reduce算子的使用（Java代码）</a>这篇文章中用Java代码实现了reduce算子累加1到100，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">object ReduceTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;ReduceTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    var numberArray = ArrayBuffer[Int]()</span><br><span class="line">    for(i &lt;- 1 to 100)&#123;</span><br><span class="line">      numberArray += i </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    val sum = numbers.reduce(_ + _)</span><br><span class="line">    </span><br><span class="line">    println(&quot;1+2+3+...+100 = &quot; + sum)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用reduce算子累加1到100<br><img src="http://pd8lpasbc.bkt.clouddn.com/68-1.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/11/Spark%E4%B8%ADaction%E7%9A%84reduce%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中action的reduce算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了reduce算子累加1到100，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的reduce算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/11/Spark%E4%B8%ADaction%E7%9A%84reduce%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/11/Spark中action的reduce算子的使用（Java代码）/</id>
    <published>2018-09-11T15:02:10.000Z</published>
    <updated>2018-09-11T15:19:28.592Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对reduce算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/67-1.png" width="100%" height="100%"><br>通过func函数聚集RDD中的所有元素，这个函数必须是可交换且可并联的<br><a id="more"></a><br>例如：累加1到100<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line"></span><br><span class="line">public class ReduceTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;ReduceTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = new ArrayList&lt;Integer&gt;();</span><br><span class="line">for(int i=1; i&lt;=100; i++) &#123;</span><br><span class="line">numberList.add(i);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">int sum = numbers.reduce(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer num1, Integer num2) throws Exception &#123;</span><br><span class="line">return num1 + num2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;1+2+3+...+100=&quot; + sum);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用reduce算子累加1到100<br><img src="http://pd8lpasbc.bkt.clouddn.com/67-2.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对reduce算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/67-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;通过func函数聚集RDD中的所有元素，这个函数必须是可交换且可并联的&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的cogroup算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84cogroup%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/10/Spark中transformation的cogroup算子的使用（Scala代码）/</id>
    <published>2018-09-10T03:02:43.000Z</published>
    <updated>2018-09-10T03:32:10.908Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84cogroup%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的cogroup算子的使用（Java代码）</a>这篇文章中用Java代码实现了cogroup算子打印出每个学生姓名对应的多门成绩，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object CogroupTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;CogroupTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf);</span><br><span class="line">    </span><br><span class="line">    val nameList = Array(</span><br><span class="line">      Tuple2(1, &quot;Tom&quot;),</span><br><span class="line">      Tuple2(2, &quot;Bob&quot;),</span><br><span class="line">      Tuple2(3, &quot;Alice&quot;)</span><br><span class="line">    )</span><br><span class="line">    val scoreList = Array(</span><br><span class="line">      Tuple2(1, 80),</span><br><span class="line">      Tuple2(1, 81),</span><br><span class="line">      Tuple2(1, 82),</span><br><span class="line">      Tuple2(2, 90),</span><br><span class="line">      Tuple2(2, 92),</span><br><span class="line">      Tuple2(2, 94),</span><br><span class="line">      Tuple2(3, 60),</span><br><span class="line">      Tuple2(3, 70),</span><br><span class="line">      Tuple2(3, 80)    </span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    val names = sc.parallelize(nameList)</span><br><span class="line">    val scores = sc.parallelize(scoreList)</span><br><span class="line">    </span><br><span class="line">    val studentScore = names.cogroup(scores)</span><br><span class="line">    </span><br><span class="line">    studentScore.foreach&#123;t =&gt; </span><br><span class="line">      println(&quot;ID:&quot;+t._1+ &quot;\t&quot; +&quot;Name:&quot;+t._2._1.mkString(&quot;&quot;)+ &quot;\t&quot; +&quot;Score:&quot;+t._2._2.mkString(&quot;,&quot;))</span><br><span class="line">      println(&quot;*****************************************&quot;)  </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用cogroup打印出每个学生姓名对应的多门成绩<br><img src="http://pd8lpasbc.bkt.clouddn.com/66-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84cogroup%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的cogroup算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了cogroup算子打印出每个学生姓名对应的多门成绩，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的cogroup算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84cogroup%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/10/Spark中transformation的cogroup算子的使用（Java代码）/</id>
    <published>2018-09-10T02:13:04.000Z</published>
    <updated>2018-09-10T02:55:31.515Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对cogroup算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/65-1.png" width="100%" height="100%"><br>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable&lt;V>,Iterable&lt;W>))类型的RDD<br><a id="more"></a><br>例如：已知两个序列，(学号,姓名)，(学号,成绩)，这里同一个学生有多门成绩，打印出每个学生姓名对应的成绩<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class CogroupTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;CogroupTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;Integer, String&gt;&gt; nameList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(1, &quot;Tom&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(2, &quot;Bob&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(3, &quot;Alice&quot;)</span><br><span class="line">);</span><br><span class="line">List&lt;Tuple2&lt;Integer, Integer&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(1, 80),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(1, 81),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(1, 82),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(2, 90),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(2, 92),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(2, 94),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(3, 60),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(3, 70),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(3, 80)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; names = sc.parallelizePairs(nameList);</span><br><span class="line">JavaPairRDD&lt;Integer, Integer&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, Tuple2&lt;Iterable&lt;String&gt;, Iterable&lt;Integer&gt;&gt;&gt; studentScore = names.cogroup(scores);</span><br><span class="line"></span><br><span class="line">studentScore.foreach(new VoidFunction&lt;Tuple2&lt;Integer,Tuple2&lt;Iterable&lt;String&gt;,Iterable&lt;Integer&gt;&gt;&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;Integer, Tuple2&lt;Iterable&lt;String&gt;, Iterable&lt;Integer&gt;&gt;&gt; t) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;ID:&quot;+t._1+ &quot;\t&quot; +&quot;Name:&quot;+t._2._1+ &quot;\t&quot; +&quot;Score:&quot;+t._2._2);</span><br><span class="line">System.out.println(&quot;*****************************************&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用cogroup打印出每个学生姓名对应的多门成绩<br><img src="http://pd8lpasbc.bkt.clouddn.com/65-2.png" width="60%" height="60%"><br>同样的例子，如果使用join算子进行操作，得到如下结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/65-3.png" width="60%" height="60%"><br>这里就能明显的看出join算子和cogroup算子的区别<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对cogroup算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/65-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable&amp;lt;V&gt;,Iterable&amp;lt;W&gt;))类型的RDD&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的join算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84join%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/10/Spark中transformation的join算子的使用（Scala代码）/</id>
    <published>2018-09-10T01:41:04.000Z</published>
    <updated>2018-09-10T01:58:12.196Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84join%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的join算子的使用（Java代码）</a>这篇文章中用Java代码实现了join算子打印出学生成绩，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object JoinTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;JoinTest&quot;)</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line">      </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">  </span><br><span class="line">    val nameList = Array(</span><br><span class="line">      Tuple2(1, &quot;Tom&quot;),</span><br><span class="line">      Tuple2(2, &quot;Bob&quot;),</span><br><span class="line">      Tuple2(3, &quot;Alice&quot;),</span><br><span class="line">      Tuple2(4, &quot;Jack&quot;),</span><br><span class="line">      Tuple2(5, &quot;Jerry&quot;)</span><br><span class="line">    )</span><br><span class="line">    val scoreList = Array(</span><br><span class="line">      Tuple2(1, 80),</span><br><span class="line">      Tuple2(2, 75),</span><br><span class="line">      Tuple2(3, 92),</span><br><span class="line">      Tuple2(4, 85),</span><br><span class="line">      Tuple2(5, 60)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    val names = sc.parallelize(nameList)</span><br><span class="line">    val scores = sc.parallelize(scoreList)</span><br><span class="line">    </span><br><span class="line">    val studentScore = names.join(scores)</span><br><span class="line">    </span><br><span class="line">    studentScore.foreach&#123;t =&gt;</span><br><span class="line">      println(&quot;ID:&quot;+t._1+ &quot;\t&quot; +&quot;Name:&quot;+t._2._1+ &quot;\t&quot; +&quot;Score:&quot;+t._2._2)</span><br><span class="line">      println(&quot;********************************&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用join打印出学生成绩<br><img src="http://pd8lpasbc.bkt.clouddn.com/64-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84join%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的join算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了join算子打印出学生成绩，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的join算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84join%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/10/Spark中transformation的join算子的使用（Java代码）/</id>
    <published>2018-09-10T01:10:01.000Z</published>
    <updated>2018-09-10T01:34:47.895Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对join算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/63-1.png" width="100%" height="100%"><br>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD<br><a id="more"></a><br>例如：已知两个序列，(学号,姓名)，(学号,成绩)，打印出每个学生姓名对应的成绩<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class JoinTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;JoinTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;Integer, String&gt;&gt; nameList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(1, &quot;Tom&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(2, &quot;Bob&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(3, &quot;Alice&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(4, &quot;Jack&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(5, &quot;Jerry&quot;)</span><br><span class="line">);</span><br><span class="line">List&lt;Tuple2&lt;Integer, Integer&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(1, 80),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(2, 75),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(3, 92),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(4, 85),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(5, 60)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; names = sc.parallelizePairs(nameList);</span><br><span class="line">JavaPairRDD&lt;Integer, Integer&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, Tuple2&lt;String, Integer&gt;&gt; studentScore = names.join(scores);</span><br><span class="line"></span><br><span class="line">studentScore.foreach(new VoidFunction&lt;Tuple2&lt;Integer,Tuple2&lt;String,Integer&gt;&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;Integer, Tuple2&lt;String, Integer&gt;&gt; t) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;ID:&quot;+t._1+ &quot;\t&quot; +&quot;Name:&quot;+t._2._1+ &quot;\t&quot; +&quot;Score:&quot;+t._2._2);</span><br><span class="line">System.out.println(&quot;************************************&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用join打印出学生成绩<br><img src="http://pd8lpasbc.bkt.clouddn.com/63-2.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对join算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/63-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的sortByKey算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84sortByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/10/Spark中transformation的sortByKey算子的使用（Scala代码）/</id>
    <published>2018-09-10T00:10:40.000Z</published>
    <updated>2018-09-10T00:59:15.173Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/09/Spark%E4%B8%ADtransformation%E7%9A%84sortByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的sortByKey算子的使用（Java代码）</a>这篇文章中用Java代码实现了sortByKey算子对学生成绩进行了升序和降序排序，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object SortByKeyTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">          .setAppName(&quot;SortByKeyTest&quot;)</span><br><span class="line">          .setMaster(&quot;local&quot;)</span><br><span class="line">          </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val scoreList = Array(</span><br><span class="line">      Tuple2(80, &quot;Tom&quot;),</span><br><span class="line">      Tuple2(75, &quot;Bob&quot;),</span><br><span class="line">      Tuple2(92, &quot;Alice&quot;),</span><br><span class="line">      Tuple2(85, &quot;Jack&quot;),</span><br><span class="line">      Tuple2(60, &quot;Jerry&quot;)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    val scores = sc.parallelize(scoreList)</span><br><span class="line">    </span><br><span class="line">    val sortedScore = scores.sortByKey()</span><br><span class="line">    </span><br><span class="line">    sortedScore.foreach&#123;score =&gt;</span><br><span class="line">      println(&quot;Name: &quot; + score._2)</span><br><span class="line">      println(&quot;Score: &quot; + score._1)</span><br><span class="line">      println(&quot;********************&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">        </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用sortByKey算子对学生成绩进行了升序排序<br><img src="http://pd8lpasbc.bkt.clouddn.com/62-1.png" width="60%" height="60%"><br>按成绩降序排序，将sortByKey()改成sortByKey(false)即可，修改后结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/62-2.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/09/Spark%E4%B8%ADtransformation%E7%9A%84sortByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的sortByKey算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了sortByKey算子对学生成绩进行了升序和降序排序，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的sortByKey算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/09/Spark%E4%B8%ADtransformation%E7%9A%84sortByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/09/Spark中transformation的sortByKey算子的使用（Java代码）/</id>
    <published>2018-09-09T00:01:31.000Z</published>
    <updated>2018-09-09T00:29:46.285Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对sortByKey算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/61-1.png" width="100%" height="100%"><br>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD<br><a id="more"></a><br>例如：对学生成绩进行排序（升序、降序）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class SortByKeyTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;SortByKeyTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;Integer, String&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(80, &quot;Tom&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(75, &quot;Bob&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(92, &quot;Alice&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(85, &quot;Jack&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(60, &quot;Jerry&quot;)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; sortedScore = scores.sortByKey();</span><br><span class="line"></span><br><span class="line">sortedScore.foreach(new VoidFunction&lt;Tuple2&lt;Integer,String&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;Integer, String&gt; score) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;Name: &quot; + score._2);</span><br><span class="line">System.out.println(&quot;Score: &quot; + score._1);</span><br><span class="line">System.out.println(&quot;*************************&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用sortByKey对学生成绩进行升序排序<br><img src="http://pd8lpasbc.bkt.clouddn.com/61-2.png" width="60%" height="60%"></p><p>如果要降序排序，将sortByKey()改成sortByKey(false)即可，修改后结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/61-3.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对sortByKey算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/61-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的reduceByKey算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/08/Spark%E4%B8%ADtransformation%E7%9A%84reduceByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/08/Spark中transformation的reduceByKey算子的使用（Scala代码）/</id>
    <published>2018-09-08T12:00:22.000Z</published>
    <updated>2018-09-08T12:28:36.739Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/08/Spark%E4%B8%ADtransformation%E7%9A%84reduceByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的reduceByKey算子的使用（Java代码）</a>这篇文章中用Java代码实现了reduceByKey算子统计每个学生的成绩总分，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object ReduceByKeyTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;ReduceByKeyTest&quot;)</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line">      </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">  </span><br><span class="line">    val scoreList = Array(</span><br><span class="line">     Tuple2(&quot;Tom&quot;, 85),</span><br><span class="line">     Tuple2(&quot;Bob&quot;, 75),</span><br><span class="line">     Tuple2(&quot;Alice&quot;, 92),</span><br><span class="line">     Tuple2(&quot;Tom&quot;, 70),</span><br><span class="line">     Tuple2(&quot;Bob&quot;, 95),</span><br><span class="line">     Tuple2(&quot;Alice&quot;, 80)</span><br><span class="line">    )</span><br><span class="line">  </span><br><span class="line">    val scores = sc.parallelize(scoreList)</span><br><span class="line">  </span><br><span class="line">    val totalScore = scores.reduceByKey(_ + _)</span><br><span class="line">  </span><br><span class="line">    totalScore.foreach&#123;score =&gt;</span><br><span class="line">      println(&quot;Name: &quot; + score._1)</span><br><span class="line">      println(&quot;Total Score: &quot; + score._2)</span><br><span class="line">      println(&quot;************************&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用reduceByKey算子统计出每个学生的成绩总分<br><img src="http://pd8lpasbc.bkt.clouddn.com/60-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/08/Spark%E4%B8%ADtransformation%E7%9A%84reduceByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的reduceByKey算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了reduceByKey算子统计每个学生的成绩总分，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的reduceByKey算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/08/Spark%E4%B8%ADtransformation%E7%9A%84reduceByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/08/Spark中transformation的reduceByKey算子的使用（Java代码）/</id>
    <published>2018-09-08T05:25:40.000Z</published>
    <updated>2018-09-08T11:49:54.517Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对reduceByKey算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/59-1.png" width="100%" height="100%"><br>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置<br><a id="more"></a><br>例如：统计每个学生的成绩总分<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class ReduceByKeyTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;ReduceByKeyTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;String,Integer&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Tom&quot;, 85),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Bob&quot;, 75),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Alice&quot;, 92),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Tom&quot;, 70),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Bob&quot;, 95),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Alice&quot;, 80)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; totalScore = scores.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer score1, Integer score2) throws Exception &#123;</span><br><span class="line">return score1 + score2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">totalScore.foreach(new VoidFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;String, Integer&gt; score) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;Name: &quot; + score._1);</span><br><span class="line">System.out.println(&quot;Total Score: &quot; + score._2);</span><br><span class="line">System.out.println(&quot;**************************&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用reduceByKey统计出每个学生的成绩总分<br><img src="http://pd8lpasbc.bkt.clouddn.com/59-2.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对reduceByKey算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/59-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的groupByKey算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/08/Spark%E4%B8%ADtransformation%E7%9A%84groupByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/08/Spark中transformation的groupByKey算子的使用（Scala代码）/</id>
    <published>2018-09-07T16:39:17.000Z</published>
    <updated>2018-09-08T12:01:12.107Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/07/Spark%E4%B8%ADtransformation%E7%9A%84groupByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的groupByKey算子的使用（Java代码）</a>这篇文章中用Java代码实现了groupByKey算子对每个学生的成绩进行分组，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object GroupByKeyTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;GroupByKeyTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val scoreList = Array(</span><br><span class="line">        Tuple2(&quot;Tom&quot;, 85),</span><br><span class="line">        Tuple2(&quot;Bob&quot;, 75),</span><br><span class="line">        Tuple2(&quot;Alice&quot;, 92),</span><br><span class="line">        Tuple2(&quot;Tom&quot;, 70),</span><br><span class="line">        Tuple2(&quot;Bob&quot;, 95),</span><br><span class="line">        Tuple2(&quot;Alice&quot;, 80)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    val scores = sc.parallelize(scoreList)</span><br><span class="line">    </span><br><span class="line">    val groupedScores = scores.groupByKey()</span><br><span class="line">    </span><br><span class="line">    groupedScores.foreach(score =&gt; &#123;</span><br><span class="line">      println(&quot;Name: &quot; + score._1)</span><br><span class="line">      score._2.foreach(subjectScore =&gt; println(subjectScore))</span><br><span class="line">      println(&quot;******************&quot;)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用groupByKey算子对每个学生的成绩进行分组<br><img src="http://pd8lpasbc.bkt.clouddn.com/58-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/07/Spark%E4%B8%ADtransformation%E7%9A%84groupByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的groupByKey算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了groupByKey算子对每个学生的成绩进行分组，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的groupByKey算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/07/Spark%E4%B8%ADtransformation%E7%9A%84groupByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/07/Spark中transformation的groupByKey算子的使用（Java代码）/</id>
    <published>2018-09-07T15:38:42.000Z</published>
    <updated>2018-09-07T16:27:10.629Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对groupByKey算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/57-1.png" width="100%" height="100%"><br>在一个(K,V)的RDD上调用，返回一个(K, Iterable<v>)的RDD<br><a id="more"></a><br>例如：对每个学生的成绩进行分组<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class GroupByKeyTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;GroupByKeyTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;String,Integer&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Tom&quot;, 85),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Bob&quot;, 75),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Alice&quot;, 92),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Tom&quot;, 70),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Bob&quot;, 95),</span><br><span class="line">new Tuple2&lt;String,Integer&gt;(&quot;Alice&quot;, 80)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Iterable&lt;Integer&gt;&gt; groupedScores = scores.groupByKey();</span><br><span class="line"></span><br><span class="line">groupedScores.foreach(new VoidFunction&lt;Tuple2&lt;String,Iterable&lt;Integer&gt;&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;String, Iterable&lt;Integer&gt;&gt; score) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;Name: &quot; + score._1);</span><br><span class="line">Iterator&lt;Integer&gt; ite = score._2.iterator();</span><br><span class="line">while(ite.hasNext()) &#123;</span><br><span class="line">System.out.println(ite.next());</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(&quot;******************************&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></v></p><p>在本地IDE执行后，得到如下结果，即，使用groupByKey操作对每个学生的成绩进行分组<br><img src="http://pd8lpasbc.bkt.clouddn.com/57-2.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对groupByKey算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/57-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在一个(K,V)的RDD上调用，返回一个(K, Iterable&lt;v&gt;)的RDD&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的flatMap算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/07/Spark%E4%B8%ADtransformation%E7%9A%84flatMap%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/07/Spark中transformation的flatMap算子的使用（Scala代码）/</id>
    <published>2018-09-07T00:16:22.000Z</published>
    <updated>2018-09-07T00:36:58.399Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84flatMap%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的flatMap算子的使用（Java代码）</a>这篇文章中用Java代码实现了flatMap算子将文本行拆分成一个个单词，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br>同样，在桌面创建了个文件test.txt，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object FlatMapTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;FlatMapTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    words.foreach(word =&gt; println(word))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用flatMap算子将文本行拆分成一个个单词<br><img src="http://pd8lpasbc.bkt.clouddn.com/56-1.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84flatMap%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的flatMap算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了flatMap算子将文本行拆分成一个个单词，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的flatMap算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84flatMap%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/06/Spark中transformation的flatMap算子的使用（Java代码）/</id>
    <published>2018-09-06T14:46:23.000Z</published>
    <updated>2018-09-06T15:06:06.941Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对flatMap算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/55-1.png" width="100%" height="100%"><br>flatMap类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）<br><a id="more"></a><br>例如：将文本行拆分成一个个单词<br>我在桌面创建了个文件test.txt，文件内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">good good study</span><br><span class="line">day day up</span><br></pre></td></tr></table></figure></p><p>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">public class FlatMapTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;FlatMapTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">    JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line">    </span><br><span class="line">    JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;);</span><br><span class="line">    </span><br><span class="line">    JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">    </span><br><span class="line">    words.foreach(new VoidFunction&lt;String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(String word) throws Exception &#123;</span><br><span class="line">System.out.println(word);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">    </span><br><span class="line">    sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用flatMap操作将文本行拆分成一个个单词<br><img src="http://pd8lpasbc.bkt.clouddn.com/55-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对flatMap算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/55-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;flatMap类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的filter算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84filter%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/06/Spark中transformation的filter算子的使用（Scala代码）/</id>
    <published>2018-09-06T11:09:57.000Z</published>
    <updated>2018-09-06T11:27:09.626Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84filter%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的filter算子的使用（Java代码）</a>这篇文章中用Java代码实现了filter算子过滤出集合中的所有偶数，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object FilterTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;FilterTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numbers = Array(1,2,3,4,5,6,7,8,9,10)</span><br><span class="line">    </span><br><span class="line">    val numberRDD = sc.parallelize(numbers)</span><br><span class="line">    </span><br><span class="line">    val resultRDD = numberRDD.filter(num =&gt; num%2 == 0)</span><br><span class="line">    </span><br><span class="line">    resultRDD.foreach(res =&gt; println(res))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用filter算子过滤出集合中的所有偶数<br><img src="http://pd8lpasbc.bkt.clouddn.com/54-1.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/06/Spark%E4%B8%ADtransformation%E7%9A%84filter%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的filter算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了filter算子过滤出集合中的所有偶数，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
</feed>
