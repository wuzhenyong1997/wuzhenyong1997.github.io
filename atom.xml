<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>GGSTU</title>
  
  <subtitle>Good Good Study</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.ggstu.com/"/>
  <updated>2018-09-16T01:30:25.551Z</updated>
  <id>https://www.ggstu.com/</id>
  
  <author>
    <name>Wu Zhenyong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark基于排序机制的wordcount程序（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/16/Spark%E5%9F%BA%E4%BA%8E%E6%8E%92%E5%BA%8F%E6%9C%BA%E5%88%B6%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/16/Spark基于排序机制的wordcount程序（Java代码）/</id>
    <published>2018-09-16T00:41:39.000Z</published>
    <updated>2018-09-16T01:30:25.551Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Java%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/" target="_blank">使用Java开发Spark的wordcount程序</a>这篇文章中开发了Java版本的wordcount程序，得到了如下所示的结果。可以发现，并没有按照单词的总数进行排序，而是乱序的。<br><img src="http://pd8lpasbc.bkt.clouddn.com/84-1.png" width="70%" height="70%"><br><a id="more"></a><br>下面就来使用Java开发基于排序机制的Spark的wordcount程序<br>按照单词出现总数降序排序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class SortWordCount &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;SortWordCount&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus/Desktop//test.txt&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; pairs = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;String, Integer&gt;(word, 1);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; wordCount = pairs.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer v1, Integer v2) throws Exception &#123;</span><br><span class="line">return v1 + v2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; countWord = wordCount.mapToPair(new PairFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;Integer, String&gt; call(Tuple2&lt;String, Integer&gt; t) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;Integer, String&gt;(t._2, t._1);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; sortCountWord = countWord.sortByKey(false);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; sortWordCount = sortCountWord.mapToPair(new PairFunction&lt;Tuple2&lt;Integer,String&gt;, String, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Integer, String&gt; t) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;String, Integer&gt;(t._2, t._1);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sortWordCount.foreach(new VoidFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;String, Integer&gt; t) throws Exception &#123;</span><br><span class="line">System.out.println(t._1 + &quot;: &quot; + t._2);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后，得到如下结果，即按照单词出现总数降序排序<br><img src="http://pd8lpasbc.bkt.clouddn.com/84-2.png" width="60%" height="60%"></p><p>这个程序的关键步骤就是在使用reduceByKey统计出每个单词和其出现的总次数后，进行了key-value的转换，此时的key是单词总数，value是单词。然后使用sortByKey(false)按照key降序排序。排序完成后，再次进行key-value转换，转换为原来的key是单词，value是单词总数的情况。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Java%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/&quot; target=&quot;_blank&quot;&gt;使用Java开发Spark的wordcount程序&lt;/a&gt;这篇文章中开发了Java版本的wordcount程序，得到了如下所示的结果。可以发现，并没有按照单词的总数进行排序，而是乱序的。&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/84-1.png&quot; width=&quot;70%&quot; height=&quot;70%&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中的共享变量（Shared Variables）</title>
    <link href="https://www.ggstu.com/2018/09/15/Spark%E4%B8%AD%E7%9A%84%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F%EF%BC%88Shared-Variables%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/15/Spark中的共享变量（Shared-Variables）/</id>
    <published>2018-09-15T06:57:41.000Z</published>
    <updated>2018-09-15T14:56:30.315Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>共享变量工作原理：</b></font><br>默认情况下，如果在一个算子的函数中使用到了某个外部的变量，那么这个变量的值会被拷贝到每个task中。此时每个task操作的是拷贝的那份变量副本，而不是原变量。因此，这种方式无法共享此变量。</p><p>Spark为此提供了两种共享变量，一种是Broadcast Variables(广播变量)，另一种是Accumulators(累加器)。Broadcast Variables会将使用到的变量，仅仅为每个节点(机器)拷贝一份，因此可以优化性能，减少网络传输以及内存消耗。Accumulators则可以让多个task共同操作一份变量，主要可以进行累加操作。<br><a id="more"></a><br><b>Broadcast Variables：</b><br>广播变量可以在每台机器上只保留一个变量，并且这个变量是只读的，而不会为每个task都拷贝一份副本。因此其可以减少变量到各个节点的网络传输消耗，以及在各个节点上的内存消耗。此外，spark内部也使用了高效的广播算法来分发广播变量，以减少通信成本。</p><p>可以通过调用SparkContext的broadcast()方法，来针对某个变量创建广播变量。然后在算子的函数内，使用到广播变量时，每个节点只会拷贝一份副本。每个节点可以使用广播变量的value()方法获取值。</p><p>例如：将集合中的每个元素乘以10，即把10当成广播变量<br>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line">import org.apache.spark.broadcast.Broadcast;</span><br><span class="line"></span><br><span class="line">public class BroadcastTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;BroadcastTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">int factor = 10;</span><br><span class="line"></span><br><span class="line">Broadcast&lt;Integer&gt; factorBroadcast  = sc.broadcast(factor);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; result = numbers.map(new Function&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer num) throws Exception &#123;</span><br><span class="line">int factor = factorBroadcast.value();</span><br><span class="line">return num * factor;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">result.foreach(new VoidFunction&lt;Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Integer res) throws Exception &#123;</span><br><span class="line">System.out.println(res);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE后，运行结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/83-1.png" width="60%" height="60%"></p><p>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object BroadcastTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;BroadcastTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"> </span><br><span class="line">    val factor = 10</span><br><span class="line">    </span><br><span class="line">    val factorBroadcast = sc.broadcast(factor)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    val result = numbers.map(num =&gt; num*factorBroadcast.value)</span><br><span class="line"> </span><br><span class="line">    result.foreach(res =&gt; println(res))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE上运行，同样得到如上结果。<br><br><br><b>Accumulators:</b><br>Spark提供的Accumulator，主要用于多个节点对一个变量进行共享性的操作。Accumulator只提供了累加的功能，可以多个task对一个变量并行操作。<br>task只能对Accumulator进行累加操作，不能读取它的值。只有Driver程序可以读取Accumulator的值。</p><p>例如：累加1到5<br>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line">import org.apache.spark.util.LongAccumulator;</span><br><span class="line"></span><br><span class="line">public class AccumulatorTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;AccumulatorTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">LongAccumulator sum  = sc.sc().longAccumulator();</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">numbers.foreach(new VoidFunction&lt;Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Integer num) throws Exception &#123;</span><br><span class="line">sum.add(num);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;1+2+3+4+5 = &quot; + sum.value());</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE后，运行结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/83-2.png" width="60%" height="60%"></p><p>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object AccumulatorTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;AccumulatorTest&quot;)</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line">      </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val sum = sc.longAccumulator(&quot;My Accumulator&quot;)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    numbers.foreach(num =&gt; sum.add(num))</span><br><span class="line">    </span><br><span class="line">    println(&quot;1+2+3+4+5 = &quot; + sum.value)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE上运行，同样得到如上结果。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;共享变量工作原理：&lt;/b&gt;&lt;/font&gt;&lt;br&gt;默认情况下，如果在一个算子的函数中使用到了某个外部的变量，那么这个变量的值会被拷贝到每个task中。此时每个task操作的是拷贝的那份变量副本，而不是原变量。因此，这种方式无法共享此变量。&lt;/p&gt;
&lt;p&gt;Spark为此提供了两种共享变量，一种是Broadcast Variables(广播变量)，另一种是Accumulators(累加器)。Broadcast Variables会将使用到的变量，仅仅为每个节点(机器)拷贝一份，因此可以优化性能，减少网络传输以及内存消耗。Accumulators则可以让多个task共同操作一份变量，主要可以进行累加操作。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中RDD的持久化</title>
    <link href="https://www.ggstu.com/2018/09/14/Spark%E4%B8%ADRDD%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96/"/>
    <id>https://www.ggstu.com/2018/09/14/Spark中RDD的持久化/</id>
    <published>2018-09-14T06:20:41.000Z</published>
    <updated>2018-09-14T15:38:02.773Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>RDD持久化原理</b></font><br>Spark非常重要的一个功能特性就是可以将RDD持久化到内存中。当对RDD执行持久化操作时，每个节点都会将自己操作的RDD的partition持久化到内存中，并且在之后对该RDD的反复使用中，可以直接使用内存缓存的partition。这样的话，若对于一个RDD反复执行多个操作，就只要对RDD计算一次即可，后面直接使用该RDD，而不需要多次计算该RDD。<br><a id="more"></a><br>要持久化一个RDD，只要调用其cache()或者persist()方法即可。在该RDD第一次被计算出来时，就会直接缓存每个节点。而且Spark的持久化机制还是自动容错的，如果持久化的RDD的任何partition丢失了，那么Spark会自动通过其源RDD，使用transformation操作重新计算该partition。</p><p>通过查看Spark源码可以发现，cache()的底层其实调用的persist()的无参版本，亦即调用persist(StorageLevel.MEMORY_ONLY)将数据持久化到内存中。<br><img src="http://pd8lpasbc.bkt.clouddn.com/82-1.png" width="100%" height="100%"></p><p>Spark自己会在shuffle操作时，进行数据持久化，比如写入磁盘，主要是为了在结点失败时，避免需要重新计算整个过程。</p><p>cache()或persist()的使用必须在transformation或者textFile等创建了一个RDD后直接调用cache()或persist()，单独用一条语句执行cache()或persist()方法会报错。如果需要从内存中清除缓存，那么可以使用unpersist()方法。<br><br><br>例如：使用持久化统计文件中单词总数<br>我在桌面创建了个文件，名为test.txt，文件中的内容是以空格隔开的单词<br><b>Java代码：</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"></span><br><span class="line">public class PersistTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;PersistTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;).cache();</span><br><span class="line"></span><br><span class="line">long beginTime = System.currentTimeMillis();</span><br><span class="line">JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">long count = words.count();</span><br><span class="line">System.out.println(&quot;Count: &quot; + count);</span><br><span class="line">long endTime = System.currentTimeMillis();</span><br><span class="line">System.out.println(&quot;Cost: &quot; + (endTime-beginTime) + &quot; milliseconds.&quot;);</span><br><span class="line"></span><br><span class="line">beginTime = System.currentTimeMillis();</span><br><span class="line">words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">count = words.count();</span><br><span class="line">System.out.println(&quot;Count: &quot; + count);</span><br><span class="line">endTime = System.currentTimeMillis();</span><br><span class="line">System.out.println(&quot;Cost: &quot; + (endTime-beginTime) + &quot; milliseconds.&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后得到如下结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/82-3.png" width="60%" height="60%"><br><img src="http://pd8lpasbc.bkt.clouddn.com/82-4.png" width="60%" height="60%"><br>如果没有使用持久化，即没有在代码中添加.cache()，结果如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/82-5.png" width="60%" height="60%"><br><img src="http://pd8lpasbc.bkt.clouddn.com/82-6.png" width="60%" height="60%"><br>可以发现使用持久化后，第二次执行同样操作，其执行的时间缩短了。<br><br><br><b>Scala代码：</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object PersistTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;PersistTest&quot;)</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line">      </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;).cache()</span><br><span class="line">    </span><br><span class="line">    var beginTime = System.currentTimeMillis()</span><br><span class="line">    var words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    var count = words.count()</span><br><span class="line">    println(&quot;Count: &quot; + count)</span><br><span class="line">    var endTime = System.currentTimeMillis()</span><br><span class="line">    println(&quot;Cost: &quot; + (endTime-beginTime) + &quot; milliseconds.&quot;)</span><br><span class="line">    </span><br><span class="line">    beginTime = System.currentTimeMillis()</span><br><span class="line">    words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    count = words.count()</span><br><span class="line">    println(&quot;Count: &quot; + count)</span><br><span class="line">    endTime = System.currentTimeMillis()</span><br><span class="line">    println(&quot;Cost: &quot; + (endTime-beginTime) + &quot; milliseconds.&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，同样得到如上结果<br><br></p><p><font size="4"><b>RDD持久化策略</b></font><br>RDD持久化可以手动选择不同的策略。比如可以将RDD持久化到内存中、持久化到磁盘上、使用序列化的方式持久化、多持久化的数据进行多路复用。只要在调用persist()时传入对应的StorageLevel即可。</p><p><b>StorageLevel（持久化级别）：</b><br>DISK_ONLY：使用非序列化Java对象的方式持久化，完全存储到磁盘上。</p><p>MEMORY_ONLY：以非序列化的Java对象的方式持久化在JVM内存中。如果内存无法完全存储RDD所有的partition，那些没有持久化的partition就会在下一次需要使用它的时候，重新被计算。</p><p>MEMORY_ONLY_SER：同MEMORY_ONLY，但是会使用Java序列化方式，将Java对象序列化后进行持久化。可以减少内存开销，但是需要进行反序列化，因此会加大CPU开销。</p><p>MEMORY_AND_DISK：同上，但是当某些partition无法存储在内存中时，会持久化到磁盘中。下次需要使用这些partition时，需要从磁盘上读取。</p><p>MEMORY_AND_DISK_SER：同MEMORY_AND_DISK。但是使用序列化方式持久化Java对象。</p><p>DISK_ONLY_2、MEMORY_ONLY_2、MEMORY_ONLY_SER_2、<br>MEMORY_AND_DISK_2、MEMORY_AND_DISK_SER_2：如果是尾部加了2的持久化级别，表示会将持久化数据复用一份保存到其它节点，从而在数据丢失时不需要再次计算，只需要使用备份数据即可。</p><p>查看Spark的文档，可以看到这些持久化级别，如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/82-2.png" width="100%" height="100%"><br><br></p><p><font size="4"><b>如何选择RDD持久化策略？</b></font><br>Spark提供的多种持久化级别，主要是为了在CPU和内存消耗之间进行取舍。下面是一些通用的持久化级别的选择建议：<br>1、优先使用MEMORY_ONLY，如果可以缓存所有数据，就使用这种策略。因为内存速度最快，而且没有序列化，不需要消耗CPU进行反序列化操作。<br>2、如果MEMORY_ONLY策略，无法存储的下所有数据的话，那么使用MEMORY_ONLY_SER，将数据进行序列化进行存储，但是要消耗CPU进行反序列化。<br>3、如果需要进行快速的失败恢复，那么就选择带后缀为_2的策略，进行数据的备份，这样在失败时，就不需要重新计算了。<br>4、尽量不使用DISK相关的策略，因为从磁盘读取数据的时间可能要比重新计算一次的时间长。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;RDD持久化原理&lt;/b&gt;&lt;/font&gt;&lt;br&gt;Spark非常重要的一个功能特性就是可以将RDD持久化到内存中。当对RDD执行持久化操作时，每个节点都会将自己操作的RDD的partition持久化到内存中，并且在之后对该RDD的反复使用中，可以直接使用内存缓存的partition。这样的话，若对于一个RDD反复执行多个操作，就只要对RDD计算一次即可，后面直接使用该RDD，而不需要多次计算该RDD。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark的体系架构</title>
    <link href="https://www.ggstu.com/2018/09/13/Spark%E7%9A%84%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/"/>
    <id>https://www.ggstu.com/2018/09/13/Spark的体系架构/</id>
    <published>2018-09-13T14:47:28.000Z</published>
    <updated>2018-09-14T04:13:38.960Z</updated>
    
    <content type="html"><![CDATA[<p>Spark应用程序作为集群上的独立进程集运行，由SparkContext主程序中的对象（称为驱动程序）协调。</p><p>具体来说，要在集群上运行，SparkContext可以连接到几种类型的集群管理器（Spark的standalone集群管理器，Mesos或YARN），它们跨应用程序分配资源。连接后，Spark会在集群中的节点上获取执行程序，这些节点是应用程序运行计算和存储数据的进程。接下来，它将应用程序的代码发送给执行程序。最后SparkContext将任务发送给执行程序来运行。<br><a id="more"></a><br>在Spark的官方文档中，Spark的架构图如下所示，我在图上标记了数字，表示执行的顺序<br><img src="http://pd8lpasbc.bkt.clouddn.com/81-2.png" width="100%" height="100%"></p><p><font size="4"><b>各个组件介绍：</b></font><br><b>Driver：</b><br>1、编写的Spark程序就在Driver上，由spark-submit执行；<br>2、运行应用程序的main方法，并创建SparkContext的进程。<br><b>Master(Cluster Manager)：</b><br>1、负责全局的资源管理和分配，包括内存、CPU、网络等等；<br>2、接收客户端Driver提交的任务请求，会发送请求给Worker，进行资源分配；<br>3、支持的类型：Standalone、Apache Mesos、Hadoop YARN。<br><b>Worker：</b><br>每个子节点上的资源管理者，可以由多个Executor组成。<br><b>Executor：</b><br>Worker上应用程序启动的进程，Executor运行任务，并将数据保存在内存或磁盘中。<br><b>Task：</b><br>发送给Executor上的的任务运行的单位。<br><br></p><p><font size="4"><b>执行步骤：</b></font><br>第一步：Driver进程启动，提交任务请求到Master上；<br>第二步：Master接收到客户端Driver提交的任务请求，会将请求发送给Worker，进行资源分配；Worker接收到Master的请求后，为Spark应用程序启动Executor；<br>第三步：Executor启动后，会向Driver进行反注册，这样Driver就知道哪些Executor是为它服务的；<br>第四步：Driver会根据RDD定义的操作，提交task到Executor上；Executor接收到task后，会启动多个线程来执行task。<br><br></p><p><font size="4"><b>监控</b></font><br>每个Driver Program都有一个Web UI，通常在端口4040上显示有关运行任务，执行程序和存储使用情况的信息。只需访问http://&lt;driver-node>:4040，Web浏览器即可访问此UI。<br><br></p><p><font size="4"><b>关于这种架构有几点注意事项：</b></font><br>1、每个应用程序都有自己的执行程序的进程，这些进程在整个应用程序的持续时间内保持不变并在多个线程中运行任务。这样可以在调度方（每个驱动程序调度自己的任务）和执行方（在不同JVM中运行的不同应用程序中的任务）之间隔离应用程序。但是，这也意味着只有将Spark应用程序写入外部存储系统的情况下才能共享数据。</p><p>2、Spark与底层集群管理器无关。只要它可以获取执行程序进程，并且这些进程相互通信，即使在Mesos、YARN等集群管理器上运行它也相对容易。</p><p>3、驱动程序必须在其生命周期内监听并接收来自其执行程序的传入连接。因此，驱动程序必须是Worker节点网络可寻址的。</p><p>4、因为驱动程序在集群上调度任务，所以它应该靠近Worker节点运行，最好是在同一局域网上。如果要远程向集群发送请求，最好让驱动程序打开RPC并让它从附近提交操作，而不是远离Worker节点运行驱动程序。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spark应用程序作为集群上的独立进程集运行，由SparkContext主程序中的对象（称为驱动程序）协调。&lt;/p&gt;
&lt;p&gt;具体来说，要在集群上运行，SparkContext可以连接到几种类型的集群管理器（Spark的standalone集群管理器，Mesos或YARN），它们跨应用程序分配资源。连接后，Spark会在集群中的节点上获取执行程序，这些节点是应用程序运行计算和存储数据的进程。接下来，它将应用程序的代码发送给执行程序。最后SparkContext将任务发送给执行程序来运行。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的foreach算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84foreach%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/13/Spark中action的foreach算子的使用（Scala代码）/</id>
    <published>2018-09-13T12:38:43.000Z</published>
    <updated>2018-09-13T13:07:50.316Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84foreach%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中action的foreach算子的使用（Java代码）</a>这篇文章中用Java代码实现了foreach算子遍历集合中的每个元素，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object ForeachTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;ForeachTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    numbers.foreach(num =&gt; println(num))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用foreach算子遍历集合中的每个元素<br><img src="http://pd8lpasbc.bkt.clouddn.com/80-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84foreach%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中action的foreach算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了foreach算子遍历集合中的每个元素，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的foreach算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84foreach%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/13/Spark中action的foreach算子的使用（Java代码）/</id>
    <published>2018-09-13T05:32:19.000Z</published>
    <updated>2018-09-13T06:14:26.125Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对foreach算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/79-1.png" width="100%" height="100%"><br>在数据集的每个元素上，运行func函数进行更新<br><a id="more"></a><br>例如：遍历集合中的每个元素<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">public class ForeachTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;ForeachTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">numbers.foreach(new VoidFunction&lt;Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Integer num) throws Exception &#123;</span><br><span class="line">System.out.println(num);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用foreach算子遍历集合中的每个元素<br><img src="http://pd8lpasbc.bkt.clouddn.com/79-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对foreach算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/79-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在数据集的每个元素上，运行func函数进行更新&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的countByKey算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84countByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/13/Spark中action的countByKey算子的使用（Scala代码）/</id>
    <published>2018-09-13T00:51:34.000Z</published>
    <updated>2018-09-13T01:16:24.720Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84countByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中action的countByKey算子的使用（Java代码）</a>这篇文章中用Java代码实现了countByKey算子统计出每个学生有几门课程，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object CountByKeyTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;CountByKeyTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val courseArray = Array(</span><br><span class="line">      Tuple2(&quot;Tom&quot;, &quot;Physics&quot;),</span><br><span class="line">      Tuple2(&quot;Tom&quot;, &quot;Maths&quot;),</span><br><span class="line">      Tuple2(&quot;Bob&quot;, &quot;English&quot;),</span><br><span class="line">      Tuple2(&quot;Bob&quot;, &quot;Science&quot;),</span><br><span class="line">      Tuple2(&quot;Bob&quot;, &quot;Chemistry&quot;)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    val courses = sc.parallelize(courseArray)</span><br><span class="line">    </span><br><span class="line">    val courseCounts = courses.countByKey()</span><br><span class="line">    </span><br><span class="line">    println(courseCounts)</span><br><span class="line">    for(count &lt;- courseCounts)&#123;</span><br><span class="line">      println(&quot;Name: &quot; + count._1 + &quot;\t&quot; + &quot;Course: &quot; + count._2)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用countByKey算子统计出每个学生有几门课程<br><img src="http://pd8lpasbc.bkt.clouddn.com/78-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84countByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中action的countByKey算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了countByKey算子统计出每个学生有几门课程，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的countByKey算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84countByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/13/Spark中action的countByKey算子的使用（Java代码）/</id>
    <published>2018-09-13T00:09:19.000Z</published>
    <updated>2018-09-13T00:42:00.445Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对countByKey算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/77-1.png" width="100%" height="100%"><br>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数<br><a id="more"></a><br>例如：统计每个学生有几门课程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Map;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class CountByKeyTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;CountByKeyTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;String, String&gt;&gt; courseList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;String, String&gt;(&quot;Tom&quot;, &quot;Physics&quot;),</span><br><span class="line">new Tuple2&lt;String, String&gt;(&quot;Tom&quot;, &quot;Maths&quot;),</span><br><span class="line">new Tuple2&lt;String, String&gt;(&quot;Bob&quot;, &quot;English&quot;),</span><br><span class="line">new Tuple2&lt;String, String&gt;(&quot;Bob&quot;, &quot;Science&quot;),</span><br><span class="line">new Tuple2&lt;String, String&gt;(&quot;Bob&quot;, &quot;Chemistry&quot;)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, String&gt; courses = sc.parallelizePairs(courseList);</span><br><span class="line"></span><br><span class="line">Map&lt;String, Long&gt; courseCounts = courses.countByKey();</span><br><span class="line"></span><br><span class="line">System.out.println(courseCounts);</span><br><span class="line">for(Map.Entry&lt;String, Long&gt; count : courseCounts.entrySet()) &#123;</span><br><span class="line">System.out.println(&quot;Name: &quot; + count.getKey() + &quot;\t&quot; + &quot;Course: &quot; + count.getValue());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用countByKey算子统计每个学生有几门课程<br><img src="http://pd8lpasbc.bkt.clouddn.com/77-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对countByKey算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/77-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的saveAsTextFile算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84saveAsTextFile%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的saveAsTextFile算子的使用（Scala代码）/</id>
    <published>2018-09-12T11:03:54.000Z</published>
    <updated>2018-09-12T12:00:44.323Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84saveAsTextFile%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中action的saveAsTextFile算子的使用（Java代码）</a>这篇文章中用Java代码实现了saveAsTextFile算子将RDD元素保存到本地文件和HDFS文件中，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br>将RDD元素保存到本地文件中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object SaveToLocalFile &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;SaveToLocalFile&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    numbers.saveAsTextFile(&quot;C://Users//asus/Desktop/number&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，就可以在本地保存的路径中看到一个名为number的文件夹，文件夹中内容如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/76-1.png" width="100%" height="100%"><br>在part-00000文件中就保存了具体内容，如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/76-2.png" width="60%" height="60%"><br><br><br>将RDD元素保存到HDFS文件中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object SaveToHDFS &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;SaveToHDFS&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    numbers.saveAsTextFile(&quot;hdfs://ggstu:9000/number&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>代码完成后，将代码打包成jar包，上传到Linux上。接着启动hadoop集群和spark集群，最后执行脚本文件。<br>以上步骤在<a href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Scala%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/" target="_blank">使用Scala开发Spark的wordcount程序</a>这篇文章中介绍过了。<br>执行完成，在hdfs的保存路径可以看到保存的number目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2018-09-12 19:57 /number</span><br></pre></td></tr></table></figure></p><p>在part-00000文件中就保存了具体内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# hdfs dfs -ls /number</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 root supergroup          0 2018-09-12 19:57 /number/_SUCCESS</span><br><span class="line">-rw-r--r--   3 root supergroup         10 2018-09-12 19:57 /number/part-00000</span><br><span class="line">[root@ggstu ~]# hdfs dfs -cat /number/part-00000</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td></tr></table></figure></p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84saveAsTextFile%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中action的saveAsTextFile算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了saveAsTextFile算子将RDD元素保存到本地文件和HDFS文件中，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的saveAsTextFile算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84saveAsTextFile%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的saveAsTextFile算子的使用（Java代码）/</id>
    <published>2018-09-12T06:02:01.000Z</published>
    <updated>2018-09-12T07:11:14.260Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对saveAsTextFile算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/75-1.png" width="100%" height="100%"><br>将数据集的元素以text file的形式保存到本地文件系统、HDFS文件系统或者其它支持的文件系统，对于每个元素，Spark会调用toString方法，将它转换为文件中的文本<br><a id="more"></a><br>例如：将RDD元素保存到本地文件中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">public class SaveToLocalFile &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;SaveToLocalFile&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">numbers.saveAsTextFile(&quot;C://Users//asus/Desktop//number&quot;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，就可以在本地保存的路径中看到一个名为number的文件夹，文件夹中内容如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/75-2.png" width="100%" height="100%"><br>在part-00000文件中就保存了具体内容，如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/75-3.png" width="60%" height="60%"><br><br><br>例如：将RDD元素保存到HDFS文件中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">public class SaveToHDFS &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;SaveToHDFS&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">numbers.saveAsTextFile(&quot;hdfs://ggstu:9000/number&quot;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>代码完成后，将代码打包成jar包，上传到Linux上。接着启动hadoop集群和spark集群，最后执行脚本文件。<br>以上步骤在<a href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Java%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/" target="_blank">使用Java开发Spark的wordcount程序</a>这篇文章中介绍过了。<br>执行完成，在hdfs的保存路径可以看到保存的number目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2018-09-12 15:05 /number</span><br></pre></td></tr></table></figure></p><p>在part-00000文件中就保存了具体内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# hdfs dfs -ls /number</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 root supergroup          0 2018-09-12 15:05 /number/_SUCCESS</span><br><span class="line">-rw-r--r--   3 root supergroup         10 2018-09-12 15:05 /number/part-00000</span><br><span class="line">[root@ggstu ~]# hdfs dfs -cat /number/part-00000</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td></tr></table></figure></p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对saveAsTextFile算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/75-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;将数据集的元素以text file的形式保存到本地文件系统、HDFS文件系统或者其它支持的文件系统，对于每个元素，Spark会调用toString方法，将它转换为文件中的文本&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的take算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84take%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的take算子的使用（Scala代码）/</id>
    <published>2018-09-12T05:31:03.000Z</published>
    <updated>2018-09-12T05:53:37.078Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84take%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中action的take算子的使用（Java代码）</a>这篇文章中用Java代码实现了take算子获取集合中的前3个元素，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object TakeTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;TakeTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val nameArray = Array(&quot;Tom&quot;,&quot;Bob&quot;,&quot;Alice&quot;,&quot;Jack&quot;,&quot;Jerry&quot;)</span><br><span class="line">    </span><br><span class="line">    val names = sc.parallelize(nameArray)</span><br><span class="line">    </span><br><span class="line">    val top3Name = names.take(3)</span><br><span class="line">    </span><br><span class="line">    for(name &lt;- top3Name)&#123;</span><br><span class="line">      println(name)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用take算子获取集合中的前3个元素<br><img src="http://pd8lpasbc.bkt.clouddn.com/74-1.png" width="40%" height="40%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84take%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中action的take算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了take算子获取集合中的前3个元素，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的take算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84take%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的take算子的使用（Java代码）/</id>
    <published>2018-09-12T05:11:36.000Z</published>
    <updated>2018-09-12T05:24:11.954Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对take算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/73-1.png" width="90%" height="90%"><br>返回一个由数据集的前n个元素组成的数组<br><a id="more"></a><br>例如：获取集合中的前3个元素<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">public class TakeTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;TakeTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;String&gt; nameList = Arrays.asList(&quot;Tom&quot;,&quot;Bob&quot;,&quot;Alice&quot;,&quot;Jack&quot;,&quot;Jerry&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; names = sc.parallelize(nameList);</span><br><span class="line"></span><br><span class="line">List&lt;String&gt; top3Names = names.take(3);</span><br><span class="line"></span><br><span class="line">for(String name : top3Names) &#123;</span><br><span class="line">System.out.println(name);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用take算子获取集合中的前3个元素<br><img src="http://pd8lpasbc.bkt.clouddn.com/73-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对take算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/73-1.png&quot; width=&quot;90%&quot; height=&quot;90%&quot;&gt;&lt;br&gt;返回一个由数据集的前n个元素组成的数组&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的count算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84count%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的count算子的使用（Scala代码）/</id>
    <published>2018-09-12T04:45:10.000Z</published>
    <updated>2018-09-12T05:06:15.549Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84count%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中action的count算子的使用（Java代码）</a>这篇文章中用Java代码实现了count算子获取集合元素总数，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object CountTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;CountTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5,6,7,8,9,10)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    val count = numbers.count()</span><br><span class="line">    </span><br><span class="line">    println(&quot;There are &quot; + count + &quot; numbers.&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用count算子获取集合元素总数<br><img src="http://pd8lpasbc.bkt.clouddn.com/72-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84count%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中action的count算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了count算子获取集合元素总数，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的count算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84count%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的count算子的使用（Java代码）/</id>
    <published>2018-09-12T04:24:15.000Z</published>
    <updated>2018-09-12T04:38:58.256Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对count算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/71-1.png" width="90%" height="90%"><br>返回RDD的元素个数<br><a id="more"></a><br>例如：获取集合元素总数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">public class CountTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;CountTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5,6,7,8,9,10);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">long count = numbers.count();</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;There are &quot; + count + &quot; numbers.&quot;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用count算子获取集合元素总数<br><img src="http://pd8lpasbc.bkt.clouddn.com/71-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对count算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/71-1.png&quot; width=&quot;90%&quot; height=&quot;90%&quot;&gt;&lt;br&gt;返回RDD的元素个数&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的collect算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84collect%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的collect算子的使用（Scala代码）/</id>
    <published>2018-09-12T01:19:32.000Z</published>
    <updated>2018-09-12T11:05:50.019Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84collect%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中action的collect算子的使用（Java代码）</a>这篇文章中用Java代码实现了collect算子打印集合中的元素，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object CollectTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;CollectTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    val numList = numbers.collect()</span><br><span class="line"></span><br><span class="line">    for(num &lt;- numList)&#123;</span><br><span class="line">      println(num)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用collect算子打印集合中的元素<br><img src="http://pd8lpasbc.bkt.clouddn.com/70-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84collect%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中action的collect算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了collect算子打印集合中的元素，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的collect算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84collect%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的collect算子的使用（Java代码）/</id>
    <published>2018-09-12T00:32:16.000Z</published>
    <updated>2018-09-12T00:59:12.152Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对collect算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/69-1.png" width="100%" height="100%"><br>在驱动程序中，以数组的形式返回数据集的所有元素<br><a id="more"></a><br>例如：打印集合中的元素<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">public class CollectTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;CollectTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numList = numbers.collect();</span><br><span class="line"></span><br><span class="line">System.out.println(numList);</span><br><span class="line">for(Integer num : numList) &#123;</span><br><span class="line">System.out.println(num);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用collect算子打印出集合中的元素<br><img src="http://pd8lpasbc.bkt.clouddn.com/69-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对collect算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/69-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在驱动程序中，以数组的形式返回数据集的所有元素&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的reduce算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84reduce%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的reduce算子的使用（Scala代码）/</id>
    <published>2018-09-11T23:57:59.000Z</published>
    <updated>2018-09-12T00:25:13.616Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/11/Spark%E4%B8%ADaction%E7%9A%84reduce%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中action的reduce算子的使用（Java代码）</a>这篇文章中用Java代码实现了reduce算子累加1到100，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">object ReduceTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;ReduceTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    var numberArray = ArrayBuffer[Int]()</span><br><span class="line">    for(i &lt;- 1 to 100)&#123;</span><br><span class="line">      numberArray += i </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    val sum = numbers.reduce(_ + _)</span><br><span class="line">    </span><br><span class="line">    println(&quot;1+2+3+...+100 = &quot; + sum)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用reduce算子累加1到100<br><img src="http://pd8lpasbc.bkt.clouddn.com/68-1.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/11/Spark%E4%B8%ADaction%E7%9A%84reduce%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中action的reduce算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了reduce算子累加1到100，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的reduce算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/11/Spark%E4%B8%ADaction%E7%9A%84reduce%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/11/Spark中action的reduce算子的使用（Java代码）/</id>
    <published>2018-09-11T15:02:10.000Z</published>
    <updated>2018-09-11T15:19:28.592Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对reduce算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/67-1.png" width="100%" height="100%"><br>通过func函数聚集RDD中的所有元素，这个函数必须是可交换且可并联的<br><a id="more"></a><br>例如：累加1到100<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line"></span><br><span class="line">public class ReduceTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;ReduceTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = new ArrayList&lt;Integer&gt;();</span><br><span class="line">for(int i=1; i&lt;=100; i++) &#123;</span><br><span class="line">numberList.add(i);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">int sum = numbers.reduce(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer num1, Integer num2) throws Exception &#123;</span><br><span class="line">return num1 + num2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;1+2+3+...+100=&quot; + sum);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用reduce算子累加1到100<br><img src="http://pd8lpasbc.bkt.clouddn.com/67-2.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对reduce算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/67-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;通过func函数聚集RDD中的所有元素，这个函数必须是可交换且可并联的&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的cogroup算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84cogroup%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/10/Spark中transformation的cogroup算子的使用（Scala代码）/</id>
    <published>2018-09-10T03:02:43.000Z</published>
    <updated>2018-09-10T03:32:10.908Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84cogroup%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中transformation的cogroup算子的使用（Java代码）</a>这篇文章中用Java代码实现了cogroup算子打印出每个学生姓名对应的多门成绩，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object CogroupTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;CogroupTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf);</span><br><span class="line">    </span><br><span class="line">    val nameList = Array(</span><br><span class="line">      Tuple2(1, &quot;Tom&quot;),</span><br><span class="line">      Tuple2(2, &quot;Bob&quot;),</span><br><span class="line">      Tuple2(3, &quot;Alice&quot;)</span><br><span class="line">    )</span><br><span class="line">    val scoreList = Array(</span><br><span class="line">      Tuple2(1, 80),</span><br><span class="line">      Tuple2(1, 81),</span><br><span class="line">      Tuple2(1, 82),</span><br><span class="line">      Tuple2(2, 90),</span><br><span class="line">      Tuple2(2, 92),</span><br><span class="line">      Tuple2(2, 94),</span><br><span class="line">      Tuple2(3, 60),</span><br><span class="line">      Tuple2(3, 70),</span><br><span class="line">      Tuple2(3, 80)    </span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    val names = sc.parallelize(nameList)</span><br><span class="line">    val scores = sc.parallelize(scoreList)</span><br><span class="line">    </span><br><span class="line">    val studentScore = names.cogroup(scores)</span><br><span class="line">    </span><br><span class="line">    studentScore.foreach&#123;t =&gt; </span><br><span class="line">      println(&quot;ID:&quot;+t._1+ &quot;\t&quot; +&quot;Name:&quot;+t._2._1.mkString(&quot;&quot;)+ &quot;\t&quot; +&quot;Score:&quot;+t._2._2.mkString(&quot;,&quot;))</span><br><span class="line">      println(&quot;*****************************************&quot;)  </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用cogroup打印出每个学生姓名对应的多门成绩<br><img src="http://pd8lpasbc.bkt.clouddn.com/66-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84cogroup%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中transformation的cogroup算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了cogroup算子打印出每个学生姓名对应的多门成绩，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中transformation的cogroup算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/10/Spark%E4%B8%ADtransformation%E7%9A%84cogroup%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/10/Spark中transformation的cogroup算子的使用（Java代码）/</id>
    <published>2018-09-10T02:13:04.000Z</published>
    <updated>2018-09-10T02:55:31.515Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对cogroup算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/65-1.png" width="100%" height="100%"><br>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable&lt;V>,Iterable&lt;W>))类型的RDD<br><a id="more"></a><br>例如：已知两个序列，(学号,姓名)，(学号,成绩)，这里同一个学生有多门成绩，打印出每个学生姓名对应的成绩<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class CogroupTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;CogroupTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;Integer, String&gt;&gt; nameList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(1, &quot;Tom&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(2, &quot;Bob&quot;),</span><br><span class="line">new Tuple2&lt;Integer, String&gt;(3, &quot;Alice&quot;)</span><br><span class="line">);</span><br><span class="line">List&lt;Tuple2&lt;Integer, Integer&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(1, 80),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(1, 81),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(1, 82),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(2, 90),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(2, 92),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(2, 94),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(3, 60),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(3, 70),</span><br><span class="line">new Tuple2&lt;Integer, Integer&gt;(3, 80)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; names = sc.parallelizePairs(nameList);</span><br><span class="line">JavaPairRDD&lt;Integer, Integer&gt; scores = sc.parallelizePairs(scoreList);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, Tuple2&lt;Iterable&lt;String&gt;, Iterable&lt;Integer&gt;&gt;&gt; studentScore = names.cogroup(scores);</span><br><span class="line"></span><br><span class="line">studentScore.foreach(new VoidFunction&lt;Tuple2&lt;Integer,Tuple2&lt;Iterable&lt;String&gt;,Iterable&lt;Integer&gt;&gt;&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;Integer, Tuple2&lt;Iterable&lt;String&gt;, Iterable&lt;Integer&gt;&gt;&gt; t) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;ID:&quot;+t._1+ &quot;\t&quot; +&quot;Name:&quot;+t._2._1+ &quot;\t&quot; +&quot;Score:&quot;+t._2._2);</span><br><span class="line">System.out.println(&quot;*****************************************&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用cogroup打印出每个学生姓名对应的多门成绩<br><img src="http://pd8lpasbc.bkt.clouddn.com/65-2.png" width="60%" height="60%"><br>同样的例子，如果使用join算子进行操作，得到如下结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/65-3.png" width="60%" height="60%"><br>这里就能明显的看出join算子和cogroup算子的区别<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对cogroup算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/65-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable&amp;lt;V&gt;,Iterable&amp;lt;W&gt;))类型的RDD&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
</feed>
