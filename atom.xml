<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>GGSTU</title>
  
  <subtitle>Good Good Study</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.ggstu.com/"/>
  <updated>2018-09-20T11:12:44.430Z</updated>
  <id>https://www.ggstu.com/</id>
  
  <author>
    <name>Wu Zhenyong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark Shuffle的原理分析</title>
    <link href="https://www.ggstu.com/2018/09/20/Spark-Shuffle%E7%9A%84%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/"/>
    <id>https://www.ggstu.com/2018/09/20/Spark-Shuffle的原理分析/</id>
    <published>2018-09-20T00:12:45.000Z</published>
    <updated>2018-09-20T11:12:44.430Z</updated>
    
    <content type="html"><![CDATA[<p>Shuffle，翻译成中文就是洗牌。因为具有某种共同特征的一类数据需要最终汇聚到一个计算节点上进行计算，所以需要Shuffle。</p><p><font size="4"><b>Spark Shuffle的历史演进：</b></font><br>在Spark 1.1以前的版本采用的是Hash Based Shuffle的实现方式。到1.1版本时参考Hadoop MapReduce的实现引入了Sort Based Shuffle。在1.4版本时引入了Tungsten-Sort Based Shuffle。在1.6中将Tungsten统一到了Sort Based Shuffle中。到2.0版本，Hash Based Shuffle被弃用，所有的Shuffle方式全部统一到Sort Based Shuffle来实现。<br><a id="more"></a><br>下图是Spark Shuffle的版本演进：<br><img src="http://pd8lpasbc.bkt.clouddn.com/92-5.png" width="100%" height="100%"><br><br></p><p><font size="4"><b>Hash Based Shuffle v1</b></font><br>早期的Hash Based Shuffle，每一个Map会根据Reduce的数量创建出相应的临时文件bucket，bucket的数量是M*R个，其中M是Map的个数，R是Reduce的个数。这样就会产生大量的小文件，对文件系统压力过大，而且不利于IO吞吐量。<br>例如：有3个Map Task，3个Reduce Task，就会产生9个小文件。<br><img src="http://pd8lpasbc.bkt.clouddn.com/92-7.png" width="100%" height="100%"><br><br></p><p><font size="4"><b>Hash Based Shuffle v2</b></font><br>后来引入了Consolidation机制，使用这种机制，在同一个core上先后运行多个Map Task的输出。<br>产生的文件数为：core的数量*Reduce的数量。</p><p>如果没有使用Consolidation机制，有4个Map Task，3个Reduce Task，这样只会产生12个小文件。<br>如果使用了Consolidation机制，那么这4个Map Task会分两批运行在2个core上，这样只会产生6个小文件。<br><img src="http://pd8lpasbc.bkt.clouddn.com/92-3.png" width="100%" height="100%"><br>这样做，表面上是减少了文件数，但是如果下游partition数量很大，输出大量的文件(cores*R)，性能会降低。而且大量的文件写入，使文件系统开始变为随机写，性能比顺序写要低。缓存空间占用比较大。<br><br></p><p><font size="4"><b>Sort Based Shuffle</b></font><br>针对上述Hash Based Shuffle的弊端，在spark 1.1引入了Sort Based Shuffle。它参考了Hadoop MapReduce中shuffle的实现，对记录进行排序来shuffle。如下图所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/92-8.png" width="90%" height="90%"><br>map端的任务会按照key对应的partition id进行排序，属于同一个partition的key不会进行排序。将排序好的数据写在同一个文件中，该文件中的记录是按照partition id排序一个一个分区的顺序排列。Map Task运行期间会顺序写每个partition的数据，并通过一个索引文件记录每个partition的大小和偏移量。<br>reduce端拉取数据做合并时不再采用HashMap，而是采用ExternalAppendOnlyMap，该数据结构在做合并时，如果内存不足，会写到磁盘上。<br><br></p><p><font size="4"><b>Tungsten-Sort Based Shuffle</b></font><br>从spark 1.4开始，spark开始了钨丝计划(Tungsten)，目的是优化内存和CPU的使用，进而提升spark的性能。从spark-1.6开始，将Tungsten-sort并入Sort Based Shuffle。<br>有关Tungsten的说明可以查看如下这篇文章：<br><a href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html" target="_blank">Project Tungsten: Bringing Apache Spark Closer to Bare Metal</a><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Shuffle，翻译成中文就是洗牌。因为具有某种共同特征的一类数据需要最终汇聚到一个计算节点上进行计算，所以需要Shuffle。&lt;/p&gt;
&lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;Spark Shuffle的历史演进：&lt;/b&gt;&lt;/font&gt;&lt;br&gt;在Spark 1.1以前的版本采用的是Hash Based Shuffle的实现方式。到1.1版本时参考Hadoop MapReduce的实现引入了Sort Based Shuffle。在1.4版本时引入了Tungsten-Sort Based Shuffle。在1.6中将Tungsten统一到了Sort Based Shuffle中。到2.0版本，Hash Based Shuffle被弃用，所有的Shuffle方式全部统一到Sort Based Shuffle来实现。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中RDD的检查点(Checkpoint)机制</title>
    <link href="https://www.ggstu.com/2018/09/17/Spark%E4%B8%ADRDD%E7%9A%84%E6%A3%80%E6%9F%A5%E7%82%B9-Checkpoint-%E6%9C%BA%E5%88%B6/"/>
    <id>https://www.ggstu.com/2018/09/17/Spark中RDD的检查点-Checkpoint-机制/</id>
    <published>2018-09-17T11:34:02.000Z</published>
    <updated>2018-09-17T12:40:38.678Z</updated>
    
    <content type="html"><![CDATA[<p>RDD的检查点机制的本质是将RDD写入磁盘作为检查点，是为了避免缓存丢失重新计算带来的开销，或者lineage(血统)过长而计算时间过长造成容错成本过高。这样就不如在中间阶段做检查点容错，如果之后有结点出现问题而丢失分区，那么可以从做检查点的RDD开始重做lineage，进而可以减少开销。</p><p>设置checkpoint的目录，可以是本地目录，也可以是HDFS上的目录。一般是在具有容错能力，高可靠的文件系统上（比如HDFS、S3等）设置一个检查点路径，用于保存检查点数据。<br><a id="more"></a><br>例如：设置检查点目录为本地目录<br>我在桌面创建了个名为checkpoint的目录作为检查点目录</p><p>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">public class CheckpointTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;CheckpointTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">//设置检查点目录</span><br><span class="line">sc.setCheckpointDir(&quot;C://Users//asus//Desktop//checkpoint&quot;);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">//设置rdd的检查点</span><br><span class="line">numbers.checkpoint();</span><br><span class="line"></span><br><span class="line">//一旦触发action操作，就会在检查点目录下生成检查点</span><br><span class="line">numbers.count();</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行完成后，在检查点目录checkpoint下会生成一个文件夹，文件夹下的内容如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/91-1.png" width="100%" height="100%"><br><br><br>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object CheckpointTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;CheckpointTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    sc.setCheckpointDir(&quot;C://Users//asus//Desktop//checkpoint&quot;)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    numbers.checkpoint</span><br><span class="line">    </span><br><span class="line">    numbers.count</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，同样得到如上结果</p><p>若要将检查点目录设置成HDFS上的目录，将上面代码的本地目录改成HDFS的目录即可。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RDD的检查点机制的本质是将RDD写入磁盘作为检查点，是为了避免缓存丢失重新计算带来的开销，或者lineage(血统)过长而计算时间过长造成容错成本过高。这样就不如在中间阶段做检查点容错，如果之后有结点出现问题而丢失分区，那么可以从做检查点的RDD开始重做lineage，进而可以减少开销。&lt;/p&gt;
&lt;p&gt;设置checkpoint的目录，可以是本地目录，也可以是HDFS上的目录。一般是在具有容错能力，高可靠的文件系统上（比如HDFS、S3等）设置一个检查点路径，用于保存检查点数据。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中RDD的依赖关系以及stage划分</title>
    <link href="https://www.ggstu.com/2018/09/17/Spark%E4%B8%ADRDD%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB%E4%BB%A5%E5%8F%8Astage%E5%88%92%E5%88%86/"/>
    <id>https://www.ggstu.com/2018/09/17/Spark中RDD的依赖关系以及stage划分/</id>
    <published>2018-09-17T07:33:28.000Z</published>
    <updated>2018-09-17T08:33:42.313Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>RDD的依赖关系</b></font><br>RDD和它依赖的父RDD的关系有两种不同的类型，即窄依赖(Narrow Dependency)和宽依赖(Wide Dependency)<br><a id="more"></a><br><img src="http://pd8lpasbc.bkt.clouddn.com/90-1.png" width="80%" height="80%"><br><b>窄依赖：</b>指的是子RDD中的每一个partition仅仅依赖于父RDD中的一个partition。例如map、filter、union等都会产生窄依赖。<br>如果子RDD执行的时候某个分区执行失败（数据丢失），只需要重新执行父RDD对应的分区即可进行数据恢复。<br><b>宽依赖：</b>指的是子RDD中的每一个partition都依赖所有父RDD的所有partition。例如groupByKey、reduceByKey、sortByKey等都会产生宽依赖。<br>如果子RDD执行的时候某个分区执行失败（数据丢失），需要将父RDD的所有分区全部进行执行才可以进行数据恢复。<br><br></p><p><font size="4"><b>Spark任务中的stage</b></font><br>DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的stage。</p><p>对于窄依赖，partition的转换处理在stage中完成计算。<br>对于宽依赖，由于有shuffle的存在，只能在父RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分stage的依据。<br><img src="http://pd8lpasbc.bkt.clouddn.com/90-2.png" width="70%" height="70%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;RDD的依赖关系&lt;/b&gt;&lt;/font&gt;&lt;br&gt;RDD和它依赖的父RDD的关系有两种不同的类型，即窄依赖(Narrow Dependency)和宽依赖(Wide Dependency)&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中实现分组取Top N（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/17/Spark%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%88%86%E7%BB%84%E5%8F%96Top-N%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/17/Spark中实现分组取Top-N（Scala代码）/</id>
    <published>2018-09-17T05:28:28.000Z</published>
    <updated>2018-09-17T06:33:50.314Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/16/Spark%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%88%86%E7%BB%84%E5%8F%96Top-N%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中实现分组取Top N（Java代码）</a>这篇文章中用Java代码找出了每个省份对应的海拔最高的三个站点，实现了分组取Top N的功能，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object GroupTop3 &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;GroupTop3&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//meteorological_station.csv&quot;)</span><br><span class="line">    </span><br><span class="line">    val pairs = lines.map(line =&gt; (line.split(&quot;,&quot;)(0), line.split(&quot;,&quot;)(2)+&quot;,&quot;+line.split(&quot;,&quot;)(5)))</span><br><span class="line">  </span><br><span class="line">    val groupedPairs = pairs.groupByKey()</span><br><span class="line">    </span><br><span class="line">    val groupTop3 = groupedPairs.map&#123;t =&gt;</span><br><span class="line">      val top3 = new Array[String](3)</span><br><span class="line">      val top3Height = new Array[Double](3)</span><br><span class="line">      val province = t._1</span><br><span class="line">      val cityAndHeights = t._2.iterator</span><br><span class="line">      while(cityAndHeights.hasNext)&#123;</span><br><span class="line">        val cityAndHeight = cityAndHeights.next()</span><br><span class="line">        val city = cityAndHeight.split(&quot;,&quot;)(0)</span><br><span class="line">        val height = cityAndHeight.split(&quot;,&quot;)(1).toDouble</span><br><span class="line">        var flag = true</span><br><span class="line">        for(i &lt;- 0 until 3 if flag)&#123;</span><br><span class="line">          if(top3(i) == null)&#123;</span><br><span class="line">            top3Height(i) = height</span><br><span class="line">            top3(i) = city + &quot; &quot; + height</span><br><span class="line">            flag = false</span><br><span class="line">          &#125;else if(height &gt; top3Height(i))&#123;</span><br><span class="line">            for(j &lt;- (i+1 to 2).reverse)&#123;</span><br><span class="line">              top3Height(j) = top3Height(j-1)</span><br><span class="line">              top3(j) = top3(j-1)</span><br><span class="line">            &#125;</span><br><span class="line">            top3Height(i) = height</span><br><span class="line">            top3(i) = city + &quot; &quot; + height</span><br><span class="line">            flag = false</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      (province, top3)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    groupTop3.foreach&#123;t =&gt; </span><br><span class="line">      println(&quot;Province: &quot; + t._1)</span><br><span class="line">      for(i &lt;- 0 to 2)&#123;</span><br><span class="line">        print(t._2.array(i))</span><br><span class="line">        if(i != 2)&#123;</span><br><span class="line">          print(&quot;,&quot;)</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">          println</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      println(&quot;**********************************&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，找出了每个省份对应的海报最高的三个站点，实现了分组取Top n的功能<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">Province: 新疆</span><br><span class="line">吐尔尕特 3504.4,塔什库尔干 3090.1,巴音布鲁克 2458.0</span><br><span class="line">**********************************</span><br><span class="line">Province: 辽宁</span><br><span class="line">建平县 462.2,建昌 367.0,新宾 328.4</span><br><span class="line">**********************************</span><br><span class="line">Province: 内蒙古</span><br><span class="line">希拉穆仁气侯站 1602.3,阿拉善左旗 1561.4,阿右旗 1510.1</span><br><span class="line">**********************************</span><br><span class="line">Province: 浙江</span><br><span class="line">淳安 171.4,云和 163.0,石浦 128.4</span><br><span class="line">**********************************</span><br><span class="line">Province: 青海</span><br><span class="line">班玛 13530.0,河南 13500.0,刚察 13301.5</span><br><span class="line">**********************************</span><br><span class="line">Province: 陕西</span><br><span class="line">华山 2064.9,太白 1543.6,定边 1360.3</span><br><span class="line">**********************************</span><br><span class="line">Province: 福建</span><br><span class="line">九仙山 1653.5,屏南 869.5,寿宁 815.9</span><br><span class="line">**********************************</span><br><span class="line">Province: 山西</span><br><span class="line">五台山 2208.3,五寨 1401.0,右玉 1345.8</span><br><span class="line">**********************************</span><br><span class="line">Province: 甘肃</span><br><span class="line">玛曲 3471.4,乌鞘岭 3045.1,合作 2910.0</span><br><span class="line">**********************************</span><br><span class="line">Province: 河北</span><br><span class="line">张北 1393.3,蔚县 909.5,围场 892.7</span><br><span class="line">**********************************</span><br><span class="line">Province: 宁夏</span><br><span class="line">六盘山 2841.2,西吉 1916.5,海原 1854.2</span><br><span class="line">**********************************</span><br><span class="line">Province: 云南</span><br><span class="line">香格里拉 3341.5,德钦 3319.0,丽江 2380.9</span><br><span class="line">**********************************</span><br><span class="line">Province: 广西</span><br><span class="line">那坡 794.1,靖西 739.9,凤山 509.4</span><br><span class="line">**********************************</span><br><span class="line">Province: 安徽</span><br><span class="line">黄山 1840.4,屯溪 142.7,祁门 142.0</span><br><span class="line">**********************************</span><br><span class="line">Province: 湖北</span><br><span class="line">利川 1074.1,五峰 619.9,建始 609.2</span><br><span class="line">**********************************</span><br><span class="line">Province: 重庆</span><br><span class="line">酉阳 826.5,黔江 786.9,綦江 474.7</span><br><span class="line">**********************************</span><br><span class="line">Province: 广东</span><br><span class="line">连平 215.2,新丰 199.3,龙川 179.6</span><br><span class="line">**********************************</span><br><span class="line">Province: 江西</span><br><span class="line">庐山 1164.5,井冈山 843.0,寻乌 303.9</span><br><span class="line">**********************************</span><br><span class="line">Province: 山东</span><br><span class="line">泰山 1533.7,沂源 305.1,济南 170.3</span><br><span class="line">**********************************</span><br><span class="line">Province: 吉林</span><br><span class="line">长白 775.0,东岗 774.2,二道 721.4</span><br><span class="line">**********************************</span><br><span class="line">Province: 江苏</span><br><span class="line">徐州 41.2,盱眙 40.8,南京 35.2</span><br><span class="line">**********************************</span><br><span class="line">Province: 北京</span><br><span class="line">延庆 487.9,密云 71.8,北京 31.3</span><br><span class="line">**********************************</span><br><span class="line">Province: 海南</span><br><span class="line">三亚 419.4,琼中 250.9,儋州 169.0</span><br><span class="line">**********************************</span><br><span class="line">Province: 贵州</span><br><span class="line">威宁 2237.5,水城 1815.9,盘县 1800.0</span><br><span class="line">**********************************</span><br><span class="line">Province: 河南</span><br><span class="line">嵩山 1178.4,栾川 742.4,卢氏 658.5</span><br><span class="line">**********************************</span><br><span class="line">Province: 上海</span><br><span class="line">宝山 5.5,null,null</span><br><span class="line">**********************************</span><br><span class="line">Province: 四川</span><br><span class="line">石渠 14200.0,理塘 3948.9,色达 3893.9</span><br><span class="line">**********************************</span><br><span class="line">Province: 天津</span><br><span class="line">宝坻 5.1,塘沽 4.8,天津 3.5</span><br><span class="line">**********************************</span><br><span class="line">Province: 黑龙江</span><br><span class="line">绥芬河 567.8,呼中 514.5,新林 501.5</span><br><span class="line">**********************************</span><br><span class="line">Province: 西藏</span><br><span class="line">安多 14800.0,班戈 14700.0,申扎 14672.0</span><br><span class="line">**********************************</span><br><span class="line">Province: 湖南</span><br><span class="line">南岳 1265.9,桂东 835.9,城步 477.7</span><br><span class="line">**********************************</span><br></pre></td></tr></table></figure></p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/16/Spark%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%88%86%E7%BB%84%E5%8F%96Top-N%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中实现分组取Top N（Java代码）&lt;/a&gt;这篇文章中用Java代码找出了每个省份对应的海拔最高的三个站点，实现了分组取Top N的功能，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中实现分组取Top N（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/16/Spark%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%88%86%E7%BB%84%E5%8F%96Top-N%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/16/Spark中实现分组取Top-N（Java代码）/</id>
    <published>2018-09-16T10:40:00.000Z</published>
    <updated>2018-09-17T01:32:32.024Z</updated>
    
    <content type="html"><![CDATA[<p>分组取Top N，就是按照某一列进行分组，然后分别取每组中的前N个数据。<br>我这里下载了一个数据集，数据集文件名为meteorological_station.csv，是2016年的中国气象站站点数据。一共有六列，分别对应省份、区站号、站点、维度、经度、观测场海拔高度。<br><a id="more"></a><br>测试数据如下所示，一共有800多条数据，这里列举其中一部分：<br>安徽,58436,宁国,30.37,118.59,87.3<br>安徽,58437,黄山,30.08,118.09,1840.4<br>安徽,58520,祁门,29.51,117.43,142<br>安徽,58531,屯溪,29.43,118.17,142.7<br>北京,54406,延庆,40.27,115.58,487.9<br>北京,54416,密云,40.23,116.52,71.8<br>北京,54511,北京,39.48,116.28,31.3<br>福建,58725,邵武,27.2,117.28,218<br>福建,58730,武夷山,27.46,118.02,222.1<br>福建,58731,浦城,27.55,118.32,276.9<br>福建,58734,建阳,27.2,118.07,196.9<br>福建,58737,建瓯,27.03,118.19,154.9</p><p><b>需求：</b>按照省份进行分组，找出每个省份对应的海拔最高的三个站点。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class GroupTop3 &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;GroupTop3&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">//读取meteorological_station.csv，创建RDD</span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users/asus//Desktop//meteorological_station.csv&quot;);</span><br><span class="line"></span><br><span class="line">//将省份作为key，站点+海拔作为value，映射成key-value的形式</span><br><span class="line">JavaPairRDD&lt;String, String&gt; pairs = lines.mapToPair(new PairFunction&lt;String, String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, String&gt; call(String line) throws Exception &#123;</span><br><span class="line">String[] lineSplited = line.split(&quot;,&quot;);</span><br><span class="line">return new Tuple2&lt;String, String&gt;(lineSplited[0], lineSplited[2]+&quot;,&quot;+lineSplited[5]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">//按照省份进行分组</span><br><span class="line">JavaPairRDD&lt;String, Iterable&lt;String&gt;&gt; groupedPairs = pairs.groupByKey();</span><br><span class="line"></span><br><span class="line">//对海拔高度进行判断，取出最高的三个高度</span><br><span class="line">JavaPairRDD&lt;String, Iterable&lt;String&gt;&gt; groupTop3 = groupedPairs.mapToPair(new PairFunction&lt;Tuple2&lt;String,Iterable&lt;String&gt;&gt;, String, Iterable&lt;String&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, Iterable&lt;String&gt;&gt; call(Tuple2&lt;String, Iterable&lt;String&gt;&gt; t) throws Exception &#123;</span><br><span class="line">String[] top3 = new String[3];</span><br><span class="line">Double[] top3Height = new Double[3];</span><br><span class="line">String province = t._1;</span><br><span class="line">Iterator&lt;String&gt; cityAndHeights = t._2.iterator();</span><br><span class="line">while(cityAndHeights.hasNext()) &#123;</span><br><span class="line">String cityAndHeight = cityAndHeights.next();</span><br><span class="line">String city = cityAndHeight.split(&quot;,&quot;)[0];</span><br><span class="line">Double height = Double.valueOf(cityAndHeight.split(&quot;,&quot;)[1]);</span><br><span class="line">for(int i=0; i&lt;3; i++) &#123;</span><br><span class="line">if(top3[i] == null) &#123;</span><br><span class="line">top3Height[i] = height;</span><br><span class="line">top3[i] = city + &quot; &quot; + height;</span><br><span class="line">break;</span><br><span class="line">&#125;else if(height &gt; top3Height[i]) &#123;</span><br><span class="line">for(int j=2; j&gt;i; j--) &#123;</span><br><span class="line">top3Height[j] = top3Height[j-1];</span><br><span class="line">top3[j] = top3[j-1];</span><br><span class="line">&#125;</span><br><span class="line">top3Height[i] = height;</span><br><span class="line">top3[i] = city + &quot; &quot; + height;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">return new Tuple2&lt;String, Iterable&lt;String&gt;&gt;(province, Arrays.asList(top3));</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">groupTop3.foreach(new VoidFunction&lt;Tuple2&lt;String,Iterable&lt;String&gt;&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;String, Iterable&lt;String&gt;&gt; t) throws Exception &#123;</span><br><span class="line">System.out.println(&quot;Province: &quot; + t._1);</span><br><span class="line">System.out.println(t._2);</span><br><span class="line">System.out.println(&quot;*************************************&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后，得到如下结果，即找出每个省份对应的海拔最高的三个站点，实现分组取Top N的功能。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">Province: 新疆</span><br><span class="line">[吐尔尕特 3504.4, 塔什库尔干 3090.1, 巴音布鲁克 2458.0]</span><br><span class="line">*************************************</span><br><span class="line">Province: 辽宁</span><br><span class="line">[建平县 462.2, 建昌 367.0, 新宾 328.4]</span><br><span class="line">*************************************</span><br><span class="line">Province: 内蒙古</span><br><span class="line">[希拉穆仁气侯站 1602.3, 阿拉善左旗 1561.4, 阿右旗 1510.1]</span><br><span class="line">*************************************</span><br><span class="line">Province: 浙江</span><br><span class="line">[淳安 171.4, 云和 163.0, 石浦 128.4]</span><br><span class="line">*************************************</span><br><span class="line">Province: 青海</span><br><span class="line">[班玛 13530.0, 河南 13500.0, 刚察 13301.5]</span><br><span class="line">*************************************</span><br><span class="line">Province: 陕西</span><br><span class="line">[华山 2064.9, 太白 1543.6, 定边 1360.3]</span><br><span class="line">*************************************</span><br><span class="line">Province: 福建</span><br><span class="line">[九仙山 1653.5, 屏南 869.5, 寿宁 815.9]</span><br><span class="line">*************************************</span><br><span class="line">Province: 山西</span><br><span class="line">[五台山 2208.3, 五寨 1401.0, 右玉 1345.8]</span><br><span class="line">*************************************</span><br><span class="line">Province: 甘肃</span><br><span class="line">[玛曲 3471.4, 乌鞘岭 3045.1, 合作 2910.0]</span><br><span class="line">*************************************</span><br><span class="line">Province: 河北</span><br><span class="line">[张北 1393.3, 蔚县 909.5, 围场 892.7]</span><br><span class="line">*************************************</span><br><span class="line">Province: 宁夏</span><br><span class="line">[六盘山 2841.2, 西吉 1916.5, 海原 1854.2]</span><br><span class="line">*************************************</span><br><span class="line">Province: 云南</span><br><span class="line">[香格里拉 3341.5, 德钦 3319.0, 丽江 2380.9]</span><br><span class="line">*************************************</span><br><span class="line">Province: 广西</span><br><span class="line">[那坡 794.1, 靖西 739.9, 凤山 509.4]</span><br><span class="line">*************************************</span><br><span class="line">Province: 安徽</span><br><span class="line">[黄山 1840.4, 屯溪 142.7, 祁门 142.0]</span><br><span class="line">*************************************</span><br><span class="line">Province: 湖北</span><br><span class="line">[利川 1074.1, 五峰 619.9, 建始 609.2]</span><br><span class="line">*************************************</span><br><span class="line">Province: 重庆</span><br><span class="line">[酉阳 826.5, 黔江 786.9, 綦江 474.7]</span><br><span class="line">*************************************</span><br><span class="line">Province: 广东</span><br><span class="line">[连平 215.2, 新丰 199.3, 龙川 179.6]</span><br><span class="line">*************************************</span><br><span class="line">Province: 江西</span><br><span class="line">[庐山 1164.5, 井冈山 843.0, 寻乌 303.9]</span><br><span class="line">*************************************</span><br><span class="line">Province: 山东</span><br><span class="line">[泰山 1533.7, 沂源 305.1, 济南 170.3]</span><br><span class="line">*************************************</span><br><span class="line">Province: 吉林</span><br><span class="line">[长白 775.0, 东岗 774.2, 二道 721.4]</span><br><span class="line">*************************************</span><br><span class="line">Province: 江苏</span><br><span class="line">[徐州 41.2, 盱眙 40.8, 南京 35.2]</span><br><span class="line">*************************************</span><br><span class="line">Province: 北京</span><br><span class="line">[延庆 487.9, 密云 71.8, 北京 31.3]</span><br><span class="line">*************************************</span><br><span class="line">Province: 海南</span><br><span class="line">[三亚 419.4, 琼中 250.9, 儋州 169.0]</span><br><span class="line">*************************************</span><br><span class="line">Province: 贵州</span><br><span class="line">[威宁 2237.5, 水城 1815.9, 盘县 1800.0]</span><br><span class="line">*************************************</span><br><span class="line">Province: 河南</span><br><span class="line">[嵩山 1178.4, 栾川 742.4, 卢氏 658.5]</span><br><span class="line">*************************************</span><br><span class="line">Province: 上海</span><br><span class="line">[宝山 5.5, null, null]</span><br><span class="line">*************************************</span><br><span class="line">Province: 四川</span><br><span class="line">[石渠 14200.0, 理塘 3948.9, 色达 3893.9]</span><br><span class="line">*************************************</span><br><span class="line">Province: 天津</span><br><span class="line">[宝坻 5.1, 塘沽 4.8, 天津 3.5]</span><br><span class="line">*************************************</span><br><span class="line">Province: 黑龙江</span><br><span class="line">[绥芬河 567.8, 呼中 514.5, 新林 501.5]</span><br><span class="line">*************************************</span><br><span class="line">Province: 西藏</span><br><span class="line">[安多 14800.0, 班戈 14700.0, 申扎 14672.0]</span><br><span class="line">*************************************</span><br><span class="line">Province: 湖南</span><br><span class="line">[南岳 1265.9, 桂东 835.9, 城步 477.7]</span><br><span class="line">*************************************</span><br></pre></td></tr></table></figure></p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分组取Top N，就是按照某一列进行分组，然后分别取每组中的前N个数据。&lt;br&gt;我这里下载了一个数据集，数据集文件名为meteorological_station.csv，是2016年的中国气象站站点数据。一共有六列，分别对应省份、区站号、站点、维度、经度、观测场海拔高度。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>使用Scala实现Spark的二次排序</title>
    <link href="https://www.ggstu.com/2018/09/16/%E4%BD%BF%E7%94%A8Scala%E5%AE%9E%E7%8E%B0Spark%E7%9A%84%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F/"/>
    <id>https://www.ggstu.com/2018/09/16/使用Scala实现Spark的二次排序/</id>
    <published>2018-09-16T08:51:03.000Z</published>
    <updated>2018-09-16T09:15:38.263Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/16/%E4%BD%BF%E7%94%A8Java%E5%AE%9E%E7%8E%B0Spark%E7%9A%84%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F/" target="_blank">使用Java实现Spark的二次排序</a>这篇文章中用Java代码实现了用户对电影评分数据的二次排序，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br>首先实现自定义的key，实现Ordered接口和Serializable接口，并在key中实现自己对多个列的排序算法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class SecondarySortKey(val first:Int, val second:Int) extends Ordered[SecondarySortKey] with Serializable &#123;</span><br><span class="line">  def compare(that:SecondarySortKey):Int = &#123;</span><br><span class="line">    if(this.first-that.first != 0)&#123;</span><br><span class="line">      this.first - that.first</span><br><span class="line">    &#125;else&#123;</span><br><span class="line">      this.second - that.second</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>主方法：将包含文本的RDD映射成key为自定义key，value为文本的JavaPairRDD。接着使用sortByKey算子使用自定义的key进行排序。最后再次映射，删除掉自定义的key，只保留文本行。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object SecondarySort &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;SecondarySort&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//movies.csv&quot;)</span><br><span class="line">    </span><br><span class="line">    val pairs = lines.map&#123;line =&gt;</span><br><span class="line">      (new SecondarySortKey(line.split(&quot;,&quot;)(0).toInt, line.split(&quot;,&quot;)(1).toInt), line)  </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    val sortedPairs = pairs.sortByKey()</span><br><span class="line">    </span><br><span class="line">    val sortedLines = sortedPairs.map(pair =&gt; pair._2)</span><br><span class="line">    </span><br><span class="line">    sortedLines.foreach(line =&gt; println(line))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，截取一部分数据，如下所示，可以看到首先按照第一个字段用户id进行排序，若用户id相同，按照第二个字段电影id进行排序。即，使用Scala实现了用户对电影评分数据的二次排序。<br><img src="http://pd8lpasbc.bkt.clouddn.com/87-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/16/%E4%BD%BF%E7%94%A8Java%E5%AE%9E%E7%8E%B0Spark%E7%9A%84%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F/&quot; target=&quot;_blank&quot;&gt;使用Java实现Spark的二次排序&lt;/a&gt;这篇文章中用Java代码实现了用户对电影评分数据的二次排序，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>使用Java实现Spark的二次排序</title>
    <link href="https://www.ggstu.com/2018/09/16/%E4%BD%BF%E7%94%A8Java%E5%AE%9E%E7%8E%B0Spark%E7%9A%84%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F/"/>
    <id>https://www.ggstu.com/2018/09/16/使用Java实现Spark的二次排序/</id>
    <published>2018-09-16T03:33:52.000Z</published>
    <updated>2018-09-16T07:48:48.476Z</updated>
    
    <content type="html"><![CDATA[<p>二次排序就是首先按照第一个字段进行排序，然后再对第一个字段相同的行按照第二个字段排序，并且不能破坏第一次排序的结果。<br>我这里下载了一个数据集，数据集文件名为movies.csv，是用户对电影的评分数据。一共有四列，分别是user_id(用户id)、item_id(电影id)、rating(评分)、timestamp(评分时间)。<br><a id="more"></a><br>测试数据如下所示，一共有10万条数据，这里只列举其中一部分：<br>186,302,3,891717742<br>22,377,1,878887116<br>244,51,2,880606923<br>166,346,1,886397596<br>298,474,4,884182806<br>115,265,2,881171488<br>253,465,5,891628467<br>305,451,3,886324817<br>6,86,3,883603013<br>62,257,2,879372434<br>286,1014,5,879781125<br>200,222,5,876042340<br>210,40,3,891035994<br>224,29,3,888104457<br>303,785,3,879485318<br>122,387,5,879270459<br>194,274,2,879539794<br>291,1042,4,874834944</p><p><b>需求：</b>使用二次排序，按照用户id升序排序，若用户id相同，即一个用户对多个电影评分，则按照电影id升序排序。</p><p>首先实现自定义的key，实现Ordered接口和Serializable接口，并在key中实现自己对多个列的排序算法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">import java.io.Serializable;</span><br><span class="line"></span><br><span class="line">import scala.math.Ordered;</span><br><span class="line"></span><br><span class="line">public class SecondarySortKey implements Ordered&lt;SecondarySortKey&gt;,Serializable &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line">private int first;</span><br><span class="line">private int second;</span><br><span class="line"></span><br><span class="line">public SecondarySortKey(int first, int second) &#123;</span><br><span class="line">super();</span><br><span class="line">this.first = first;</span><br><span class="line">this.second = second;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public int getFirst() &#123;</span><br><span class="line">return first;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void setFirst(int first) &#123;</span><br><span class="line">this.first = first;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public int getSecond() &#123;</span><br><span class="line">return second;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void setSecond(int second) &#123;</span><br><span class="line">this.second = second;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public boolean $greater(SecondarySortKey other) &#123;</span><br><span class="line">if(this.first &gt; other.getFirst()) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else if(this.first==other.getFirst() &amp;&amp; this.second&gt;other.getSecond()) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return false;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public boolean $greater$eq(SecondarySortKey other) &#123;</span><br><span class="line">if(this.$greater(other)) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else if(this.first==other.getFirst() &amp;&amp; this.second==other.getSecond()) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return false;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public boolean $less(SecondarySortKey other) &#123;</span><br><span class="line">if(this.first &lt; other.getFirst()) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else if(this.first==other.getFirst() &amp;&amp; this.second&lt;other.getSecond()) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return false;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public boolean $less$eq(SecondarySortKey other) &#123;</span><br><span class="line">if(this.$less(other)) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else if(this.first==other.getFirst() &amp;&amp; this.second==other.getSecond()) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return false;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public int compare(SecondarySortKey other) &#123;</span><br><span class="line">if(this.first-other.getFirst() != 0) &#123;</span><br><span class="line">return this.first - other.getFirst();</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return this.second - other.getSecond();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public int compareTo(SecondarySortKey other) &#123;</span><br><span class="line">if(this.first-other.getFirst() != 0) &#123;</span><br><span class="line">return this.first - other.getFirst();</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return this.second - other.getSecond();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>主方法：将包含文本的RDD映射成key为自定义key，value为文本的JavaPairRDD。接着使用sortByKey算子使用自定义的key进行排序。最后再次映射，删除掉自定义的key，只保留文本行。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class SecondarySort &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;SecondarySort&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus//Desktop//movies.csv&quot;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;SecondarySortKey, String&gt; pairs = lines.mapToPair(new PairFunction&lt;String, SecondarySortKey, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;SecondarySortKey, String&gt; call(String line) throws Exception &#123;</span><br><span class="line">String[] lineSplited = line.split(&quot;,&quot;);</span><br><span class="line">SecondarySortKey key = new SecondarySortKey(Integer.valueOf(lineSplited[0]), Integer.valueOf(lineSplited[1]));</span><br><span class="line">return new Tuple2&lt;SecondarySortKey, String&gt;(key, line);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;SecondarySortKey, String&gt; sortedPairs = pairs.sortByKey();</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; sortedLines = sortedPairs.map(new Function&lt;Tuple2&lt;SecondarySortKey,String&gt;, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public String call(Tuple2&lt;SecondarySortKey, String&gt; line) throws Exception &#123;</span><br><span class="line">return line._2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sortedLines.foreach(new VoidFunction&lt;String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(String line) throws Exception &#123;</span><br><span class="line">System.out.println(line);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>由于数据过多，我就截取一部分输出数据，如下所示，可以看到首先按照第一个字段用户id进行排序，若用户id相同，按照第二个字段电影id进行排序<br><img src="http://pd8lpasbc.bkt.clouddn.com/86-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;二次排序就是首先按照第一个字段进行排序，然后再对第一个字段相同的行按照第二个字段排序，并且不能破坏第一次排序的结果。&lt;br&gt;我这里下载了一个数据集，数据集文件名为movies.csv，是用户对电影的评分数据。一共有四列，分别是user_id(用户id)、item_id(电影id)、rating(评分)、timestamp(评分时间)。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark基于排序机制的wordcount程序（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/16/Spark%E5%9F%BA%E4%BA%8E%E6%8E%92%E5%BA%8F%E6%9C%BA%E5%88%B6%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/16/Spark基于排序机制的wordcount程序（Scala代码）/</id>
    <published>2018-09-16T03:00:33.000Z</published>
    <updated>2018-09-16T03:22:14.657Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/16/Spark%E5%9F%BA%E4%BA%8E%E6%8E%92%E5%BA%8F%E6%9C%BA%E5%88%B6%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark基于排序机制的wordcount程序（Java代码）</a>这篇文章中用Java代码实现了wordcount的排序，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object SortWordCount &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;SortWordCount&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    val words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    </span><br><span class="line">    val pairs = words.map(word =&gt; (word,1))</span><br><span class="line">    </span><br><span class="line">    val wordCount = pairs.reduceByKey(_ + _)</span><br><span class="line">    </span><br><span class="line">    val countWord = wordCount.map(t =&gt; (t._2,t._1))</span><br><span class="line">    </span><br><span class="line">    val sortCountWord = countWord.sortByKey(false)</span><br><span class="line">    </span><br><span class="line">    val sortWordCount = sortCountWord.map(t =&gt; (t._2,t._1))</span><br><span class="line">    </span><br><span class="line">    sortWordCount.foreach(t =&gt; println(t._1 + &quot;: &quot; + t._2))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，实现了基于排序机制的wordcount程序<br><img src="http://pd8lpasbc.bkt.clouddn.com/85-1.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/16/Spark%E5%9F%BA%E4%BA%8E%E6%8E%92%E5%BA%8F%E6%9C%BA%E5%88%B6%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark基于排序机制的wordcount程序（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了wordcount的排序，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark基于排序机制的wordcount程序（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/16/Spark%E5%9F%BA%E4%BA%8E%E6%8E%92%E5%BA%8F%E6%9C%BA%E5%88%B6%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/16/Spark基于排序机制的wordcount程序（Java代码）/</id>
    <published>2018-09-16T00:41:39.000Z</published>
    <updated>2018-09-16T01:30:25.551Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Java%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/" target="_blank">使用Java开发Spark的wordcount程序</a>这篇文章中开发了Java版本的wordcount程序，得到了如下所示的结果。可以发现，并没有按照单词的总数进行排序，而是乱序的。<br><img src="http://pd8lpasbc.bkt.clouddn.com/84-1.png" width="70%" height="70%"><br><a id="more"></a><br>下面就来使用Java开发基于排序机制的Spark的wordcount程序<br>按照单词出现总数降序排序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class SortWordCount &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;SortWordCount&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus/Desktop//test.txt&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; pairs = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;String, Integer&gt;(word, 1);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; wordCount = pairs.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer v1, Integer v2) throws Exception &#123;</span><br><span class="line">return v1 + v2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; countWord = wordCount.mapToPair(new PairFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;Integer, String&gt; call(Tuple2&lt;String, Integer&gt; t) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;Integer, String&gt;(t._2, t._1);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;Integer, String&gt; sortCountWord = countWord.sortByKey(false);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; sortWordCount = sortCountWord.mapToPair(new PairFunction&lt;Tuple2&lt;Integer,String&gt;, String, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Integer, String&gt; t) throws Exception &#123;</span><br><span class="line">return new Tuple2&lt;String, Integer&gt;(t._2, t._1);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sortWordCount.foreach(new VoidFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Tuple2&lt;String, Integer&gt; t) throws Exception &#123;</span><br><span class="line">System.out.println(t._1 + &quot;: &quot; + t._2);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后，得到如下结果，即按照单词出现总数降序排序<br><img src="http://pd8lpasbc.bkt.clouddn.com/84-2.png" width="60%" height="60%"></p><p>这个程序的关键步骤就是在使用reduceByKey统计出每个单词和其出现的总次数后，进行了key-value的转换，此时的key是单词总数，value是单词。然后使用sortByKey(false)按照key降序排序。排序完成后，再次进行key-value转换，转换为原来的key是单词，value是单词总数的情况。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Java%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/&quot; target=&quot;_blank&quot;&gt;使用Java开发Spark的wordcount程序&lt;/a&gt;这篇文章中开发了Java版本的wordcount程序，得到了如下所示的结果。可以发现，并没有按照单词的总数进行排序，而是乱序的。&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/84-1.png&quot; width=&quot;70%&quot; height=&quot;70%&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中的共享变量（Shared Variables）</title>
    <link href="https://www.ggstu.com/2018/09/15/Spark%E4%B8%AD%E7%9A%84%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F%EF%BC%88Shared-Variables%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/15/Spark中的共享变量（Shared-Variables）/</id>
    <published>2018-09-15T06:57:41.000Z</published>
    <updated>2018-09-15T14:56:30.315Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>共享变量工作原理：</b></font><br>默认情况下，如果在一个算子的函数中使用到了某个外部的变量，那么这个变量的值会被拷贝到每个task中。此时每个task操作的是拷贝的那份变量副本，而不是原变量。因此，这种方式无法共享此变量。</p><p>Spark为此提供了两种共享变量，一种是Broadcast Variables(广播变量)，另一种是Accumulators(累加器)。Broadcast Variables会将使用到的变量，仅仅为每个节点(机器)拷贝一份，因此可以优化性能，减少网络传输以及内存消耗。Accumulators则可以让多个task共同操作一份变量，主要可以进行累加操作。<br><a id="more"></a><br><b>Broadcast Variables：</b><br>广播变量可以在每台机器上只保留一个变量，并且这个变量是只读的，而不会为每个task都拷贝一份副本。因此其可以减少变量到各个节点的网络传输消耗，以及在各个节点上的内存消耗。此外，spark内部也使用了高效的广播算法来分发广播变量，以减少通信成本。</p><p>可以通过调用SparkContext的broadcast()方法，来针对某个变量创建广播变量。然后在算子的函数内，使用到广播变量时，每个节点只会拷贝一份副本。每个节点可以使用广播变量的value()方法获取值。</p><p>例如：将集合中的每个元素乘以10，即把10当成广播变量<br>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.Function;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line">import org.apache.spark.broadcast.Broadcast;</span><br><span class="line"></span><br><span class="line">public class BroadcastTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;BroadcastTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">int factor = 10;</span><br><span class="line"></span><br><span class="line">Broadcast&lt;Integer&gt; factorBroadcast  = sc.broadcast(factor);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; result = numbers.map(new Function&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Integer call(Integer num) throws Exception &#123;</span><br><span class="line">int factor = factorBroadcast.value();</span><br><span class="line">return num * factor;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">result.foreach(new VoidFunction&lt;Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Integer res) throws Exception &#123;</span><br><span class="line">System.out.println(res);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE后，运行结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/83-1.png" width="60%" height="60%"></p><p>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object BroadcastTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;BroadcastTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"> </span><br><span class="line">    val factor = 10</span><br><span class="line">    </span><br><span class="line">    val factorBroadcast = sc.broadcast(factor)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    val result = numbers.map(num =&gt; num*factorBroadcast.value)</span><br><span class="line"> </span><br><span class="line">    result.foreach(res =&gt; println(res))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE上运行，同样得到如上结果。<br><br><br><b>Accumulators:</b><br>Spark提供的Accumulator，主要用于多个节点对一个变量进行共享性的操作。Accumulator只提供了累加的功能，可以多个task对一个变量并行操作。<br>task只能对Accumulator进行累加操作，不能读取它的值。只有Driver程序可以读取Accumulator的值。</p><p>例如：累加1到5<br>Java代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line">import org.apache.spark.util.LongAccumulator;</span><br><span class="line"></span><br><span class="line">public class AccumulatorTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;AccumulatorTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">LongAccumulator sum  = sc.sc().longAccumulator();</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">numbers.foreach(new VoidFunction&lt;Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Integer num) throws Exception &#123;</span><br><span class="line">sum.add(num);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">System.out.println(&quot;1+2+3+4+5 = &quot; + sum.value());</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE后，运行结果如下<br><img src="http://pd8lpasbc.bkt.clouddn.com/83-2.png" width="60%" height="60%"></p><p>Scala代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object AccumulatorTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;AccumulatorTest&quot;)</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line">      </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val sum = sc.longAccumulator(&quot;My Accumulator&quot;)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    numbers.foreach(num =&gt; sum.add(num))</span><br><span class="line">    </span><br><span class="line">    println(&quot;1+2+3+4+5 = &quot; + sum.value)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE上运行，同样得到如上结果。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;共享变量工作原理：&lt;/b&gt;&lt;/font&gt;&lt;br&gt;默认情况下，如果在一个算子的函数中使用到了某个外部的变量，那么这个变量的值会被拷贝到每个task中。此时每个task操作的是拷贝的那份变量副本，而不是原变量。因此，这种方式无法共享此变量。&lt;/p&gt;
&lt;p&gt;Spark为此提供了两种共享变量，一种是Broadcast Variables(广播变量)，另一种是Accumulators(累加器)。Broadcast Variables会将使用到的变量，仅仅为每个节点(机器)拷贝一份，因此可以优化性能，减少网络传输以及内存消耗。Accumulators则可以让多个task共同操作一份变量，主要可以进行累加操作。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中RDD的持久化</title>
    <link href="https://www.ggstu.com/2018/09/14/Spark%E4%B8%ADRDD%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96/"/>
    <id>https://www.ggstu.com/2018/09/14/Spark中RDD的持久化/</id>
    <published>2018-09-14T06:20:41.000Z</published>
    <updated>2018-09-14T15:38:02.773Z</updated>
    
    <content type="html"><![CDATA[<p><font size="4"><b>RDD持久化原理</b></font><br>Spark非常重要的一个功能特性就是可以将RDD持久化到内存中。当对RDD执行持久化操作时，每个节点都会将自己操作的RDD的partition持久化到内存中，并且在之后对该RDD的反复使用中，可以直接使用内存缓存的partition。这样的话，若对于一个RDD反复执行多个操作，就只要对RDD计算一次即可，后面直接使用该RDD，而不需要多次计算该RDD。<br><a id="more"></a><br>要持久化一个RDD，只要调用其cache()或者persist()方法即可。在该RDD第一次被计算出来时，就会直接缓存每个节点。而且Spark的持久化机制还是自动容错的，如果持久化的RDD的任何partition丢失了，那么Spark会自动通过其源RDD，使用transformation操作重新计算该partition。</p><p>通过查看Spark源码可以发现，cache()的底层其实调用的persist()的无参版本，亦即调用persist(StorageLevel.MEMORY_ONLY)将数据持久化到内存中。<br><img src="http://pd8lpasbc.bkt.clouddn.com/82-1.png" width="100%" height="100%"></p><p>Spark自己会在shuffle操作时，进行数据持久化，比如写入磁盘，主要是为了在结点失败时，避免需要重新计算整个过程。</p><p>cache()或persist()的使用必须在transformation或者textFile等创建了一个RDD后直接调用cache()或persist()，单独用一条语句执行cache()或persist()方法会报错。如果需要从内存中清除缓存，那么可以使用unpersist()方法。<br><br><br>例如：使用持久化统计文件中单词总数<br>我在桌面创建了个文件，名为test.txt，文件中的内容是以空格隔开的单词<br><b>Java代码：</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"></span><br><span class="line">public class PersistTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;PersistTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;).cache();</span><br><span class="line"></span><br><span class="line">long beginTime = System.currentTimeMillis();</span><br><span class="line">JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">long count = words.count();</span><br><span class="line">System.out.println(&quot;Count: &quot; + count);</span><br><span class="line">long endTime = System.currentTimeMillis();</span><br><span class="line">System.out.println(&quot;Cost: &quot; + (endTime-beginTime) + &quot; milliseconds.&quot;);</span><br><span class="line"></span><br><span class="line">beginTime = System.currentTimeMillis();</span><br><span class="line">words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span><br><span class="line">return Arrays.asList(line.split(&quot; &quot;)).iterator();</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">count = words.count();</span><br><span class="line">System.out.println(&quot;Count: &quot; + count);</span><br><span class="line">endTime = System.currentTimeMillis();</span><br><span class="line">System.out.println(&quot;Cost: &quot; + (endTime-beginTime) + &quot; milliseconds.&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE运行后得到如下结果<br><img src="http://pd8lpasbc.bkt.clouddn.com/82-3.png" width="60%" height="60%"><br><img src="http://pd8lpasbc.bkt.clouddn.com/82-4.png" width="60%" height="60%"><br>如果没有使用持久化，即没有在代码中添加.cache()，结果如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/82-5.png" width="60%" height="60%"><br><img src="http://pd8lpasbc.bkt.clouddn.com/82-6.png" width="60%" height="60%"><br>可以发现使用持久化后，第二次执行同样操作，其执行的时间缩短了。<br><br><br><b>Scala代码：</b><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object PersistTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;PersistTest&quot;)</span><br><span class="line">      .setMaster(&quot;local&quot;)</span><br><span class="line">      </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val lines = sc.textFile(&quot;C://Users//asus//Desktop//test.txt&quot;).cache()</span><br><span class="line">    </span><br><span class="line">    var beginTime = System.currentTimeMillis()</span><br><span class="line">    var words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    var count = words.count()</span><br><span class="line">    println(&quot;Count: &quot; + count)</span><br><span class="line">    var endTime = System.currentTimeMillis()</span><br><span class="line">    println(&quot;Cost: &quot; + (endTime-beginTime) + &quot; milliseconds.&quot;)</span><br><span class="line">    </span><br><span class="line">    beginTime = System.currentTimeMillis()</span><br><span class="line">    words = lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">    count = words.count()</span><br><span class="line">    println(&quot;Count: &quot; + count)</span><br><span class="line">    endTime = System.currentTimeMillis()</span><br><span class="line">    println(&quot;Cost: &quot; + (endTime-beginTime) + &quot; milliseconds.&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE运行后，同样得到如上结果<br><br></p><p><font size="4"><b>RDD持久化策略</b></font><br>RDD持久化可以手动选择不同的策略。比如可以将RDD持久化到内存中、持久化到磁盘上、使用序列化的方式持久化、多持久化的数据进行多路复用。只要在调用persist()时传入对应的StorageLevel即可。</p><p><b>StorageLevel（持久化级别）：</b><br>DISK_ONLY：使用非序列化Java对象的方式持久化，完全存储到磁盘上。</p><p>MEMORY_ONLY：以非序列化的Java对象的方式持久化在JVM内存中。如果内存无法完全存储RDD所有的partition，那些没有持久化的partition就会在下一次需要使用它的时候，重新被计算。</p><p>MEMORY_ONLY_SER：同MEMORY_ONLY，但是会使用Java序列化方式，将Java对象序列化后进行持久化。可以减少内存开销，但是需要进行反序列化，因此会加大CPU开销。</p><p>MEMORY_AND_DISK：同上，但是当某些partition无法存储在内存中时，会持久化到磁盘中。下次需要使用这些partition时，需要从磁盘上读取。</p><p>MEMORY_AND_DISK_SER：同MEMORY_AND_DISK。但是使用序列化方式持久化Java对象。</p><p>DISK_ONLY_2、MEMORY_ONLY_2、MEMORY_ONLY_SER_2、<br>MEMORY_AND_DISK_2、MEMORY_AND_DISK_SER_2：如果是尾部加了2的持久化级别，表示会将持久化数据复用一份保存到其它节点，从而在数据丢失时不需要再次计算，只需要使用备份数据即可。</p><p>查看Spark的文档，可以看到这些持久化级别，如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/82-2.png" width="100%" height="100%"><br><br></p><p><font size="4"><b>如何选择RDD持久化策略？</b></font><br>Spark提供的多种持久化级别，主要是为了在CPU和内存消耗之间进行取舍。下面是一些通用的持久化级别的选择建议：<br>1、优先使用MEMORY_ONLY，如果可以缓存所有数据，就使用这种策略。因为内存速度最快，而且没有序列化，不需要消耗CPU进行反序列化操作。<br>2、如果MEMORY_ONLY策略，无法存储的下所有数据的话，那么使用MEMORY_ONLY_SER，将数据进行序列化进行存储，但是要消耗CPU进行反序列化。<br>3、如果需要进行快速的失败恢复，那么就选择带后缀为_2的策略，进行数据的备份，这样在失败时，就不需要重新计算了。<br>4、尽量不使用DISK相关的策略，因为从磁盘读取数据的时间可能要比重新计算一次的时间长。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;4&quot;&gt;&lt;b&gt;RDD持久化原理&lt;/b&gt;&lt;/font&gt;&lt;br&gt;Spark非常重要的一个功能特性就是可以将RDD持久化到内存中。当对RDD执行持久化操作时，每个节点都会将自己操作的RDD的partition持久化到内存中，并且在之后对该RDD的反复使用中，可以直接使用内存缓存的partition。这样的话，若对于一个RDD反复执行多个操作，就只要对RDD计算一次即可，后面直接使用该RDD，而不需要多次计算该RDD。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark的体系架构</title>
    <link href="https://www.ggstu.com/2018/09/13/Spark%E7%9A%84%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/"/>
    <id>https://www.ggstu.com/2018/09/13/Spark的体系架构/</id>
    <published>2018-09-13T14:47:28.000Z</published>
    <updated>2018-09-14T04:13:38.960Z</updated>
    
    <content type="html"><![CDATA[<p>Spark应用程序作为集群上的独立进程集运行，由SparkContext主程序中的对象（称为驱动程序）协调。</p><p>具体来说，要在集群上运行，SparkContext可以连接到几种类型的集群管理器（Spark的standalone集群管理器，Mesos或YARN），它们跨应用程序分配资源。连接后，Spark会在集群中的节点上获取执行程序，这些节点是应用程序运行计算和存储数据的进程。接下来，它将应用程序的代码发送给执行程序。最后SparkContext将任务发送给执行程序来运行。<br><a id="more"></a><br>在Spark的官方文档中，Spark的架构图如下所示，我在图上标记了数字，表示执行的顺序<br><img src="http://pd8lpasbc.bkt.clouddn.com/81-2.png" width="100%" height="100%"></p><p><font size="4"><b>各个组件介绍：</b></font><br><b>Driver：</b><br>1、编写的Spark程序就在Driver上，由spark-submit执行；<br>2、运行应用程序的main方法，并创建SparkContext的进程。<br><b>Master(Cluster Manager)：</b><br>1、负责全局的资源管理和分配，包括内存、CPU、网络等等；<br>2、接收客户端Driver提交的任务请求，会发送请求给Worker，进行资源分配；<br>3、支持的类型：Standalone、Apache Mesos、Hadoop YARN。<br><b>Worker：</b><br>每个子节点上的资源管理者，可以由多个Executor组成。<br><b>Executor：</b><br>Worker上应用程序启动的进程，Executor运行任务，并将数据保存在内存或磁盘中。<br><b>Task：</b><br>发送给Executor上的的任务运行的单位。<br><br></p><p><font size="4"><b>执行步骤：</b></font><br>第一步：Driver进程启动，提交任务请求到Master上；<br>第二步：Master接收到客户端Driver提交的任务请求，会将请求发送给Worker，进行资源分配；Worker接收到Master的请求后，为Spark应用程序启动Executor；<br>第三步：Executor启动后，会向Driver进行反注册，这样Driver就知道哪些Executor是为它服务的；<br>第四步：Driver会根据RDD定义的操作，提交task到Executor上；Executor接收到task后，会启动多个线程来执行task。<br><br></p><p><font size="4"><b>监控</b></font><br>每个Driver Program都有一个Web UI，通常在端口4040上显示有关运行任务，执行程序和存储使用情况的信息。只需访问http://&lt;driver-node>:4040，Web浏览器即可访问此UI。<br><br></p><p><font size="4"><b>关于这种架构有几点注意事项：</b></font><br>1、每个应用程序都有自己的执行程序的进程，这些进程在整个应用程序的持续时间内保持不变并在多个线程中运行任务。这样可以在调度方（每个驱动程序调度自己的任务）和执行方（在不同JVM中运行的不同应用程序中的任务）之间隔离应用程序。但是，这也意味着只有将Spark应用程序写入外部存储系统的情况下才能共享数据。</p><p>2、Spark与底层集群管理器无关。只要它可以获取执行程序进程，并且这些进程相互通信，即使在Mesos、YARN等集群管理器上运行它也相对容易。</p><p>3、驱动程序必须在其生命周期内监听并接收来自其执行程序的传入连接。因此，驱动程序必须是Worker节点网络可寻址的。</p><p>4、因为驱动程序在集群上调度任务，所以它应该靠近Worker节点运行，最好是在同一局域网上。如果要远程向集群发送请求，最好让驱动程序打开RPC并让它从附近提交操作，而不是远离Worker节点运行驱动程序。<br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spark应用程序作为集群上的独立进程集运行，由SparkContext主程序中的对象（称为驱动程序）协调。&lt;/p&gt;
&lt;p&gt;具体来说，要在集群上运行，SparkContext可以连接到几种类型的集群管理器（Spark的standalone集群管理器，Mesos或YARN），它们跨应用程序分配资源。连接后，Spark会在集群中的节点上获取执行程序，这些节点是应用程序运行计算和存储数据的进程。接下来，它将应用程序的代码发送给执行程序。最后SparkContext将任务发送给执行程序来运行。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的foreach算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84foreach%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/13/Spark中action的foreach算子的使用（Scala代码）/</id>
    <published>2018-09-13T12:38:43.000Z</published>
    <updated>2018-09-13T13:07:50.316Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84foreach%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中action的foreach算子的使用（Java代码）</a>这篇文章中用Java代码实现了foreach算子遍历集合中的每个元素，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object ForeachTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;ForeachTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    numbers.foreach(num =&gt; println(num))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用foreach算子遍历集合中的每个元素<br><img src="http://pd8lpasbc.bkt.clouddn.com/80-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84foreach%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中action的foreach算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了foreach算子遍历集合中的每个元素，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的foreach算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84foreach%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/13/Spark中action的foreach算子的使用（Java代码）/</id>
    <published>2018-09-13T05:32:19.000Z</published>
    <updated>2018-09-13T06:14:26.125Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对foreach算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/79-1.png" width="100%" height="100%"><br>在数据集的每个元素上，运行func函数进行更新<br><a id="more"></a><br>例如：遍历集合中的每个元素<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line">import org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line">public class ForeachTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;ForeachTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">numbers.foreach(new VoidFunction&lt;Integer&gt;() &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void call(Integer num) throws Exception &#123;</span><br><span class="line">System.out.println(num);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用foreach算子遍历集合中的每个元素<br><img src="http://pd8lpasbc.bkt.clouddn.com/79-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对foreach算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/79-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;在数据集的每个元素上，运行func函数进行更新&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的countByKey算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84countByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/13/Spark中action的countByKey算子的使用（Scala代码）/</id>
    <published>2018-09-13T00:51:34.000Z</published>
    <updated>2018-09-13T01:16:24.720Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84countByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中action的countByKey算子的使用（Java代码）</a>这篇文章中用Java代码实现了countByKey算子统计出每个学生有几门课程，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object CountByKeyTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;CountByKeyTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val courseArray = Array(</span><br><span class="line">      Tuple2(&quot;Tom&quot;, &quot;Physics&quot;),</span><br><span class="line">      Tuple2(&quot;Tom&quot;, &quot;Maths&quot;),</span><br><span class="line">      Tuple2(&quot;Bob&quot;, &quot;English&quot;),</span><br><span class="line">      Tuple2(&quot;Bob&quot;, &quot;Science&quot;),</span><br><span class="line">      Tuple2(&quot;Bob&quot;, &quot;Chemistry&quot;)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    val courses = sc.parallelize(courseArray)</span><br><span class="line">    </span><br><span class="line">    val courseCounts = courses.countByKey()</span><br><span class="line">    </span><br><span class="line">    println(courseCounts)</span><br><span class="line">    for(count &lt;- courseCounts)&#123;</span><br><span class="line">      println(&quot;Name: &quot; + count._1 + &quot;\t&quot; + &quot;Course: &quot; + count._2)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用countByKey算子统计出每个学生有几门课程<br><img src="http://pd8lpasbc.bkt.clouddn.com/78-1.png" width="60%" height="60%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84countByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中action的countByKey算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了countByKey算子统计出每个学生有几门课程，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的countByKey算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/13/Spark%E4%B8%ADaction%E7%9A%84countByKey%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/13/Spark中action的countByKey算子的使用（Java代码）/</id>
    <published>2018-09-13T00:09:19.000Z</published>
    <updated>2018-09-13T00:42:00.445Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对countByKey算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/77-1.png" width="100%" height="100%"><br>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数<br><a id="more"></a><br>例如：统计每个学生有几门课程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Map;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class CountByKeyTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;CountByKeyTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;String, String&gt;&gt; courseList = Arrays.asList(</span><br><span class="line">new Tuple2&lt;String, String&gt;(&quot;Tom&quot;, &quot;Physics&quot;),</span><br><span class="line">new Tuple2&lt;String, String&gt;(&quot;Tom&quot;, &quot;Maths&quot;),</span><br><span class="line">new Tuple2&lt;String, String&gt;(&quot;Bob&quot;, &quot;English&quot;),</span><br><span class="line">new Tuple2&lt;String, String&gt;(&quot;Bob&quot;, &quot;Science&quot;),</span><br><span class="line">new Tuple2&lt;String, String&gt;(&quot;Bob&quot;, &quot;Chemistry&quot;)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, String&gt; courses = sc.parallelizePairs(courseList);</span><br><span class="line"></span><br><span class="line">Map&lt;String, Long&gt; courseCounts = courses.countByKey();</span><br><span class="line"></span><br><span class="line">System.out.println(courseCounts);</span><br><span class="line">for(Map.Entry&lt;String, Long&gt; count : courseCounts.entrySet()) &#123;</span><br><span class="line">System.out.println(&quot;Name: &quot; + count.getKey() + &quot;\t&quot; + &quot;Course: &quot; + count.getValue());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用countByKey算子统计每个学生有几门课程<br><img src="http://pd8lpasbc.bkt.clouddn.com/77-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对countByKey算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/77-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的saveAsTextFile算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84saveAsTextFile%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的saveAsTextFile算子的使用（Scala代码）/</id>
    <published>2018-09-12T11:03:54.000Z</published>
    <updated>2018-09-12T12:00:44.323Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84saveAsTextFile%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中action的saveAsTextFile算子的使用（Java代码）</a>这篇文章中用Java代码实现了saveAsTextFile算子将RDD元素保存到本地文件和HDFS文件中，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br>将RDD元素保存到本地文件中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object SaveToLocalFile &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;SaveToLocalFile&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    numbers.saveAsTextFile(&quot;C://Users//asus/Desktop/number&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，就可以在本地保存的路径中看到一个名为number的文件夹，文件夹中内容如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/76-1.png" width="100%" height="100%"><br>在part-00000文件中就保存了具体内容，如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/76-2.png" width="60%" height="60%"><br><br><br>将RDD元素保存到HDFS文件中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object SaveToHDFS &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;SaveToHDFS&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val numberArray = Array(1,2,3,4,5)</span><br><span class="line">    </span><br><span class="line">    val numbers = sc.parallelize(numberArray)</span><br><span class="line">    </span><br><span class="line">    numbers.saveAsTextFile(&quot;hdfs://ggstu:9000/number&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>代码完成后，将代码打包成jar包，上传到Linux上。接着启动hadoop集群和spark集群，最后执行脚本文件。<br>以上步骤在<a href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Scala%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/" target="_blank">使用Scala开发Spark的wordcount程序</a>这篇文章中介绍过了。<br>执行完成，在hdfs的保存路径可以看到保存的number目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2018-09-12 19:57 /number</span><br></pre></td></tr></table></figure></p><p>在part-00000文件中就保存了具体内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# hdfs dfs -ls /number</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 root supergroup          0 2018-09-12 19:57 /number/_SUCCESS</span><br><span class="line">-rw-r--r--   3 root supergroup         10 2018-09-12 19:57 /number/part-00000</span><br><span class="line">[root@ggstu ~]# hdfs dfs -cat /number/part-00000</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td></tr></table></figure></p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84saveAsTextFile%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中action的saveAsTextFile算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了saveAsTextFile算子将RDD元素保存到本地文件和HDFS文件中，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的saveAsTextFile算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84saveAsTextFile%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的saveAsTextFile算子的使用（Java代码）/</id>
    <published>2018-09-12T06:02:01.000Z</published>
    <updated>2018-09-12T07:11:14.260Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对saveAsTextFile算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/75-1.png" width="100%" height="100%"><br>将数据集的元素以text file的形式保存到本地文件系统、HDFS文件系统或者其它支持的文件系统，对于每个元素，Spark会调用toString方法，将它转换为文件中的文本<br><a id="more"></a><br>例如：将RDD元素保存到本地文件中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">public class SaveToLocalFile &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;SaveToLocalFile&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">numbers.saveAsTextFile(&quot;C://Users//asus/Desktop//number&quot;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，就可以在本地保存的路径中看到一个名为number的文件夹，文件夹中内容如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/75-2.png" width="100%" height="100%"><br>在part-00000文件中就保存了具体内容，如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/75-3.png" width="60%" height="60%"><br><br><br>例如：将RDD元素保存到HDFS文件中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">public class SaveToHDFS &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;SaveToHDFS&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;Integer&gt; numberList = Arrays.asList(1,2,3,4,5);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Integer&gt; numbers = sc.parallelize(numberList);</span><br><span class="line"></span><br><span class="line">numbers.saveAsTextFile(&quot;hdfs://ggstu:9000/number&quot;);</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>代码完成后，将代码打包成jar包，上传到Linux上。接着启动hadoop集群和spark集群，最后执行脚本文件。<br>以上步骤在<a href="https://www.ggstu.com/2018/09/02/%E4%BD%BF%E7%94%A8Java%E5%BC%80%E5%8F%91Spark%E7%9A%84wordcount%E7%A8%8B%E5%BA%8F/" target="_blank">使用Java开发Spark的wordcount程序</a>这篇文章中介绍过了。<br>执行完成，在hdfs的保存路径可以看到保存的number目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2018-09-12 15:05 /number</span><br></pre></td></tr></table></figure></p><p>在part-00000文件中就保存了具体内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@ggstu ~]# hdfs dfs -ls /number</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 root supergroup          0 2018-09-12 15:05 /number/_SUCCESS</span><br><span class="line">-rw-r--r--   3 root supergroup         10 2018-09-12 15:05 /number/part-00000</span><br><span class="line">[root@ggstu ~]# hdfs dfs -cat /number/part-00000</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td></tr></table></figure></p><p><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对saveAsTextFile算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/75-1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;br&gt;将数据集的元素以text file的形式保存到本地文件系统、HDFS文件系统或者其它支持的文件系统，对于每个元素，Spark会调用toString方法，将它转换为文件中的文本&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的take算子的使用（Scala代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84take%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Scala%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的take算子的使用（Scala代码）/</id>
    <published>2018-09-12T05:31:03.000Z</published>
    <updated>2018-09-12T05:53:37.078Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84take%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/" target="_blank">Spark中action的take算子的使用（Java代码）</a>这篇文章中用Java代码实现了take算子获取集合中的前3个元素，这里使用同样的例子，用Scala来开发。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line"></span><br><span class="line">object TakeTest &#123;</span><br><span class="line">  def main(args:Array[String])&#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setAppName(&quot;TakeTest&quot;)</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    </span><br><span class="line">    val nameArray = Array(&quot;Tom&quot;,&quot;Bob&quot;,&quot;Alice&quot;,&quot;Jack&quot;,&quot;Jerry&quot;)</span><br><span class="line">    </span><br><span class="line">    val names = sc.parallelize(nameArray)</span><br><span class="line">    </span><br><span class="line">    val top3Name = names.take(3)</span><br><span class="line">    </span><br><span class="line">    for(name &lt;- top3Name)&#123;</span><br><span class="line">      println(name)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地Scala IDE执行后，得到如下结果，即，使用take算子获取集合中的前3个元素<br><img src="http://pd8lpasbc.bkt.clouddn.com/74-1.png" width="40%" height="40%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84take%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/&quot; target=&quot;_blank&quot;&gt;Spark中action的take算子的使用（Java代码）&lt;/a&gt;这篇文章中用Java代码实现了take算子获取集合中的前3个元素，这里使用同样的例子，用Scala来开发。&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Scala" scheme="https://www.ggstu.com/tags/Scala/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中action的take算子的使用（Java代码）</title>
    <link href="https://www.ggstu.com/2018/09/12/Spark%E4%B8%ADaction%E7%9A%84take%E7%AE%97%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88Java%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>https://www.ggstu.com/2018/09/12/Spark中action的take算子的使用（Java代码）/</id>
    <published>2018-09-12T05:11:36.000Z</published>
    <updated>2018-09-12T05:24:11.954Z</updated>
    
    <content type="html"><![CDATA[<p>在spark官方文档中，对take算子的描述如下所示<br><img src="http://pd8lpasbc.bkt.clouddn.com/73-1.png" width="90%" height="90%"><br>返回一个由数据集的前n个元素组成的数组<br><a id="more"></a><br>例如：获取集合中的前3个元素<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line">public class TakeTest &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;TakeTest&quot;)</span><br><span class="line">.setMaster(&quot;local&quot;);</span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">List&lt;String&gt; nameList = Arrays.asList(&quot;Tom&quot;,&quot;Bob&quot;,&quot;Alice&quot;,&quot;Jack&quot;,&quot;Jerry&quot;);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; names = sc.parallelize(nameList);</span><br><span class="line"></span><br><span class="line">List&lt;String&gt; top3Names = names.take(3);</span><br><span class="line"></span><br><span class="line">for(String name : top3Names) &#123;</span><br><span class="line">System.out.println(name);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在本地IDE执行后，得到如下结果，即，使用take算子获取集合中的前3个元素<br><img src="http://pd8lpasbc.bkt.clouddn.com/73-2.png" width="50%" height="50%"><br><br><br>本文均为<font color="#f00">原创</font>，如需转载请注明链接<a href="http://www.ggstu.com">http://www.ggstu.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在spark官方文档中，对take算子的描述如下所示&lt;br&gt;&lt;img src=&quot;http://pd8lpasbc.bkt.clouddn.com/73-1.png&quot; width=&quot;90%&quot; height=&quot;90%&quot;&gt;&lt;br&gt;返回一个由数据集的前n个元素组成的数组&lt;br&gt;
    
    </summary>
    
      <category term="技术文章" scheme="https://www.ggstu.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/"/>
    
    
      <category term="Java" scheme="https://www.ggstu.com/tags/Java/"/>
    
      <category term="Spark" scheme="https://www.ggstu.com/tags/Spark/"/>
    
  </entry>
  
</feed>
